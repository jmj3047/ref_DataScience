{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part4_11_GAN_Fashion_MNIST_tensorflow.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNiPW4uSb6VWremqUn8savp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import sys\n","sys.path.append('/content/drive/MyDrive/#fastcampus')\n","drive_project_root = '/content/drive/MyDrive/#fastcampus'\n","!pip install -r '/content/drive/MyDrive/#fastcampus/requirements.txt'"],"metadata":{"id":"bfadOzkbuIB4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datetime import datetime\n","\n","import numpy as np\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from omegaconf import OmegaConf, DictConfig    # DictConfig is for time checking\n","\n","import hydra\n","from hydra.core.config_store import ConfigStore\n","\n","import tensorflow as tf\n","import tensorflow_probability as tfp  # 직접 구현해도 되지만 요즘은 이런 게 잘 되어 있으니 편리하게 여기서 가져다 사용하는 편\n","import tensorflow_addons as tfa\n","\n","import wandb"],"metadata":{"id":"wmIGtqQXpfJr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from config_utils_tf import flatten_dict, register_config, get_optimizer_element, get_callbacks"],"metadata":{"id":"T8mcCpw73M-i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## make model"],"metadata":{"id":"L7L9eTkdq1qI"}},{"cell_type":"markdown","source":["VAE\n","- generative model은 학습이 느린 편\n","- GPU 3개 썼는데 2주가 넘어야 학습되기도 함"],"metadata":{"id":"4Z0d6uTbhVEC"}},{"cell_type":"code","source":["class VAE(tf.keras.Model):\n","    # convolutional variational autoencoder 모델을 사용할 예정\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__()\n","        self.cfg = cfg\n","        self.latent_dim = cfg.model.latent_dim  # auto encoder에서 잠재변수 차원은 중요 -> 따로 정의\n","        self.encoder = tf.keras.Sequential(\n","            [\n","             # 일단 2개 정도만 쌓을 예정\n","             # 너무 많이 쌓으면 하루가 지나도 결과가 안 나옴;;\n","             # Conv2D : 자체 activation 제공하고 있기 때문에 따로 쓸 필요 없음\n","             tf.keras.layers.Conv2D(**cfg.model.enc.conv1),\n","             tf.keras.layers.Conv2D(**cfg.model.enc.conv2),\n","             tf.keras.layers.Flatten(),\n","             tf.keras.layers.Dense(cfg.model.enc.out_fc.units),\n","            ],\n","            name=\"encoder\",\n","        ) \n","        self.decoder = tf.keras.Sequential(\n","            [\n","             # encoder의 Dense layer를 convolutional 2d transpose에 맞는 형태로 바꾸기\n","             tf.keras.layers.Dense(\n","                 units=cfg.model.dec.in_fc.units,\n","                 activation=tf.nn.relu\n","             ),\n","             tf.keras.layers.Reshape(\n","                 target_shape=tuple(cfg.model.dec.reshape_shape)\n","             ),\n","             # Transpose convolution 사용 (= deep convolution)\n","             tf.keras.layers.Conv2DTranspose(**cfg.model.dec.tr_conv1),\n","             tf.keras.layers.Conv2DTranspose(**cfg.model.dec.tr_conv2),\n","             tf.keras.layers.Conv2DTranspose(**cfg.model.dec.tr_conv3),\n","            ],\n","            name=\"decoder\",\n","        )\n","\n","        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n","        self.recon_loss_tracker = tf.keras.metrics.Mean(name=\"recon_loss\")\n","        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n","\n","    # compile된 metric이 아닌 customize해서 사용하기 위함\n","    @property\n","    def metrics(self):\n","        return [\n","            self.total_loss_tracker,\n","            self.recon_loss_tracker,\n","            self.kl_loss_tracker,\n","        ]\n","\n","    # call, train_step, test_step은 tensorflow2(keras)에서 기본적으로 제공하는 것\n","    # 따라서 compile이 자동으로 됨\n","    # but, 그 외 'sample' 같은 건 안 됨\n","    # -> @tf.function 이라는 decorator 붙여서 compile 되도록 써야 함\n","    # encode, reparameterize, decode 등에 @tf.function 안 다는 이유는\n","    # call에서 불러서 사용할 함수이기 때문 (call에서 사용되는 경우 자동 compile됨)\n","\n","    # sample : 임의의 값을 생성하는 generator 모델의 목적\n","    @tf.function\n","    def sample(self, epsilon=None, sample_size=100):\n","        if epsilon is None:\n","            epsilon = tf.random.normal(shape=(sample_size, self.latent_dim))\n","        return self.decode(epsilon)\n","\n","    def encode(self, x, training=False):\n","        # tf.split : encoder에서 받아 온 module을 2개로 쪼갬\n","        mu, logvar = tf.split(\n","            self.encoder(x, training=training),\n","            num_or_size_splits=2,\n","            axis=1\n","        )\n","        return mu, logvar\n","    \n","    # logvar : sigmoid\n","    def reparameterize(self, mu, logvar):\n","        # epsilon의 random 값을 가지고 gaussian (normal) distribution에서 임의로 가져옴\n","        epsilon = tf.random.normal(shape=mu.shape)\n","        return mu + epsilon * tf.exp(logvar * .5)   # z\n","\n","    def decode(self, z, training=False):\n","        return tf.sigmoid(self.decoder(z, training=training))\n","\n","    def call(self, input, training=False):\n","        mu, logvar = self.encode(input, training=training)\n","        z = self.reparameterize(mu, logvar)\n","        output = self.decode(z, training=training)\n","        return output, z, mu, logvar\n","\n","    def train_step(self, data):\n","        images, _ = data\n","        # images = [batch * 28 * 28]\n","        # 지금 convolutional layer를 쓰고 있기 때문에 images = [batch * 28 * 28 * 1]이 되어야 함\n","        images = tf.cast(tf.expand_dims(images, -1), tf.float32)\n","\n","        with tf.GradientTape() as tape:            \n","            outputs, z, z_mu, z_logvar = self(images, training=True)\n","\n","            # calculate loss\n","            # VAE의 loss는 recons loss 와 kld loss의 합으로 이루어짐\n","\n","            # reconstruction loss\n","            # reduce mean & reduce sum : image 별로, batch 별로 따로 계산하기 위함\n","            recon_loss = tf.reduce_mean(\n","                tf.reduce_sum(\n","                    tf.keras.losses.mae(images, outputs),\n","                    axis=(1, 2)  # image size : width, height\n","                )\n","            )\n","\n","            # kld_loss\n","            kl_loss = -0.5 + (1 + z_logvar - tf.square(z_mu) - tf.exp(z_logvar))\n","            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n","\n","            # total_loss\n","            # 원하면 kl_loss에 대해 가중치 줘도 됨\n","            total_loss = recon_loss + kl_loss\n","        \n","        # compute gradients\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(total_loss, trainable_vars)\n","\n","        # update weights\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","\n","        # update the metrics\n","        # 내가 원하는 게 tensorflow에서 제공되지 않는 metrics이기 때문에 customize 하여 metrics 사용\n","        self.total_loss_tracker.update_state(total_loss)\n","        self.recon_loss_tracker.update_state(recon_loss)\n","        self.kl_loss_tracker.update_state(kl_loss)\n","\n","        # tensorboard image update\n","        # 이 이미지를 wandb랑 sync 하는 코드를 항상 넣어놓았으니\n","        # 이 이미지 업데이트 되면 wandb에서도 찍힘\n","        # max_outputs=5 : batch가 32개 들어간 다음에 맨 앞에 있는 5개만 사용하겠다 -> 너무 많아지면 느려지니까\n","        tf.summary.image(\"train_source_img\", images, max_outputs=5)  # tensorflow log에 저장\n","        tf.summary.image(\"train_recon_img\", outputs, max_outputs=5)\n","\n","        # return a dict mapping metrics names to current values\n","        # metric에 total loss가 이미 있기 때문에 그대로 사용\n","        logs = {m.name: m.result() for m in self.metrics}\n","        return logs\n","    \n","    def test_step(self, data):\n","        images, _ = data\n","        # images = [batch * 28 * 28]\n","        # 지금 convolutional layer를 쓰고 있기 때문에 images = [batch * 28 * 28 * 1]이 되어야 함\n","        # images = tf.expand_dims(images, -1).astype(tf.float32)  # -> 오류 발생함 : 아래와 같이 바꾸기\n","        images = tf.cast(tf.expand_dims(images, -1), tf.float32)\n","       \n","        outputs, z, z_mu, z_logvar = self(images, training=False)\n","\n","        # calculate loss\n","        # VAE의 loss는 recons loss 와 kld loss의 합으로 이루어짐\n","\n","        # reconstruction loss\n","        # reduce mean & reduce sum : image 별로, batch 별로 따로 계산하기 위함\n","        recon_loss = tf.reduce_mean(\n","            tf.reduce_sum(\n","                tf.keras.losses.mae(images, outputs),\n","                axis=(1, 2)  # image size : width, height\n","            )\n","        )\n","\n","        # kld_loss\n","        kl_loss = -0.5 + (1 + z_logvar - tf.square(z_mu) - tf.exp(z_logvar))\n","        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n","\n","        # total_loss\n","        # 원하면 kl_loss에 대해 가중치 줘도 됨\n","        total_loss = recon_loss + kl_loss\n","\n","        # update the metrics\n","        # 내가 원하는 게 tensorflow에서 제공되지 않는 metrics이기 때문에 customize 하여 metrics 사용\n","        self.total_loss_tracker.update_state(total_loss)\n","        self.recon_loss_tracker.update_state(recon_loss)\n","        self.kl_loss_tracker.update_state(kl_loss)\n","\n","        # tensorboard image update\n","        # 이 이미지를 wandb랑 sync 하는 코드를 항상 넣어놓았으니\n","        # 이 이미지 업데이트 되면 wandb에서도 찍힘\n","        # max_outputs=5 : batch가 32개 들어간 다음에 맨 앞에 있는 5개만 사용하겠다 -> 너무 많아지면 느려지니까\n","        tf.summary.image(\"val_source_img\", images, max_outputs=5)  # tensorflow log에 저장\n","        tf.summary.image(\"val_recon_img\", outputs, max_outputs=5)\n","\n","        # return a dict mapping metrics names to current values\n","        # metric에 total loss가 이미 있기 때문에 그대로 사용\n","        logs = {m.name: m.result() for m in self.metrics}\n","        return logs"],"metadata":{"id":"wn2XnFMFq1tW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GAN(tf.keras.Model):\n","    # convolutional variational autoencoder 모델을 사용할 예정\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__()\n","        self.cfg = cfg\n","        self.latent_dim = cfg.model.latent_dim  # auto encoder에서 잠재변수 차원은 중요 -> 따로 정의\n","        # GAN에서는 encoder와 비슷한 게 discriminator (image를 받아서 classification하니까)\n","        self.discriminator = tf.keras.Sequential(\n","            [\n","             # 일단 2개 정도만 쌓을 예정\n","             # 너무 많이 쌓으면 하루가 지나도 결과가 안 나옴;;\n","             # Conv2D : 자체 activation 제공하고 있기 때문에 따로 쓸 필요 없음\n","             tf.keras.layers.Conv2D(**cfg.model.dis.conv1),\n","             tf.keras.layers.Conv2D(**cfg.model.dis.conv2),\n","             tf.keras.layers.Flatten(),\n","             tf.keras.layers.Dense(1),  # 실제 이미지인지 fake인지 구분\n","            ],\n","            name=\"discriminator\",\n","        ) \n","        self.generator = tf.keras.Sequential(\n","            [\n","             # encoder의 Dense layer를 convolutional 2d transpose에 맞는 형태로 바꾸기\n","             tf.keras.layers.Dense(\n","                 units=cfg.model.gen.in_fc.units,\n","                 activation=tf.nn.relu\n","             ),\n","             tf.keras.layers.Reshape(\n","                 target_shape=tuple(cfg.model.gen.reshape_shape)\n","             ),\n","             # Transpose convolution 사용 (= deep convolution)\n","             tf.keras.layers.Conv2DTranspose(**cfg.model.gen.tr_conv1),\n","             tf.keras.layers.Conv2DTranspose(**cfg.model.gen.tr_conv2),\n","             tf.keras.layers.Conv2DTranspose(**cfg.model.gen.tr_conv3),\n","            ],\n","            name=\"generator\",\n","        )\n","\n","        self.generator_loss_tracker = tf.keras.metrics.Mean(name=\"generator_loss\")\n","        self.discriminator_loss_tracker = tf.keras.metrics.Mean(name=\"discriminator_loss\")\n","        # loss를 generator, discriminator 따로 정의를 하고\n","        # GradientTape를 따로 붙여야 하기 때문에 compile 못 씀\n","        # -> 따라서 범용으로 쓸 loss 식 정의\n","        self.cross_entropy = tf.keras.losses.BinaryCrossentropy(\n","            from_logits=True, reduction=tf.keras.losses.Reduction.SUM\n","        )\n","\n","    # compile된 metric이 아닌 customize해서 사용하기 위함\n","    @property\n","    def metrics(self):\n","        return [\n","            self.generator_loss_tracker,\n","            self.discriminator_loss_tracker,\n","        ]\n","    \n","    # keras에서 제공하는 compile은 optimizer가 한 개라고 가정하는데\n","    # GAN은 discriminator와 generator 각각에 대한 optimizer 총 2개가 있어 compile에 대해 따로 함수 생성 필요\n","    def compile(self, d_optimizer, g_optimizer, **kwargs):\n","        super().compile(**kwargs)  # 이렇게 하면 하위 호환성을 최대한 만족시키면서 원하는 것 정의 가능\n","        self.d_optimizer = d_optimizer\n","        self.g_optimizer = g_optimizer\n","\n","    @tf.function\n","    def get_generator_loss(self, fake_outputs):\n","        return self.cross_entropy(\n","            tf.ones_like(fake_outputs),  # label\n","            fake_outputs\n","        ) / fake_outputs.shape[0]   # batch size에 맞도록\n","    \n","    @tf.function\n","    def get_discriminator_loss(self, real_outputs, fake_outputs):\n","        real_loss = self.cross_entropy(\n","            tf.ones_like(real_outputs),\n","            real_outputs\n","        )\n","        fake_loss = self.cross_entropy(\n","            tf.zeros_like(fake_outputs),\n","            fake_outputs\n","        )\n","        total_d_loss = (real_loss + fake_loss) / (real_outputs.shape[0] + fake_outputs.shape[0])\n","        return total_d_loss\n","\n","    # call, train_step, test_step은 tensorflow2(keras)에서 기본적으로 제공하는 것\n","    # 따라서 compile이 자동으로 됨\n","    # but, 그 외 'sample' 같은 건 안 됨\n","    # -> @tf.function 이라는 decorator 붙여서 compile 되도록 써야 함\n","    # encode, reparameterize, decode 등에 @tf.function 안 다는 이유는\n","    # call에서 불러서 사용할 함수이기 때문 (call에서 사용되는 경우 자동 compile됨)\n","\n","    # sample : 임의의 값을 생성하는 generator 모델의 목적\n","    @tf.function\n","    def sample(self, sample_size=100):\n","        # generator에 맞게 shape 지정\n","        return tf.random.normal(shape=(sample_size, 1, 1, self.latent_dim))\n","\n","    # GAN은 input을 받지 않고 무조건 sampling을 통해 학습함\n","    def call(self, _, training=False):\n","        noise = self.sample(sample_size=self.cfg.train.train_batch_size)\n","        generated_images = self.generator(noise, training=training)\n","        discriminator_results = self.discriminator(generated_images, training=training)\n","        return generated_images, discriminator_results\n","\n","    def train_step(self, data):\n","        images, _ = data\n","        # images = [batch * 28 * 28]\n","        # 지금 convolutional layer를 쓰고 있기 때문에 images = [batch * 28 * 28 * 1]이 되어야 함\n","        images = tf.cast(tf.expand_dims(images, -1), tf.float32)\n","\n","        with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:            \n","            outputs, fake_output = self(images, training=True)  # images 대신 None 넣어도 됨\n","            real_output = self.discriminator(images, training=True)\n","\n","            # Calculate the loss for each item in the batch\n","            g_loss = self.get_generator_loss(fake_output)\n","            d_loss = self.get_discriminator_loss(real_output, fake_output)\n","        \n","        # compute gradients\n","        g_gradients = g_tape.gradient(g_loss, self.generator.trainable_variables)\n","        d_gradients = d_tape.gradient(d_loss, self.discriminator.trainable_variables)\n","\n","        # update weights\n","        self.g_optimizer.apply_gradients(zip(g_gradients, self.generator.trainable_variables))\n","        self.d_optimizer.apply_gradients(zip(d_gradients, self.discriminator.trainable_variables))\n","\n","        # update the metrics\n","        self.generator_loss_tracker.update_state(g_loss)\n","        self.discriminator_loss_tracker.update_state(d_loss)\n","\n","        # tensorboard image update\n","        # 이 이미지를 wandb랑 sync 하는 코드를 항상 넣어놓았으니\n","        # 이 이미지 업데이트 되면 wandb에서도 찍힘\n","        # max_outputs=5 : batch가 32개 들어간 다음에 맨 앞에 있는 5개만 사용하겠다 -> 너무 많아지면 느려지니까\n","        tf.summary.image(\"train_real_img\", images, max_outputs=5)  # tensorflow log에 저장\n","        tf.summary.image(\"train_generated_img\", outputs, max_outputs=5)  # fake image\n","\n","        # return a dict mapping metrics names to current values\n","        # metric에 total loss가 이미 있기 때문에 그대로 사용\n","        logs = {m.name: m.result() for m in self.metrics}\n","        return logs\n","    \n","    def test_step(self, data):\n","        images, _ = data\n","        # images = [batch * 28 * 28]\n","        # 지금 convolutional layer를 쓰고 있기 때문에 images = [batch * 28 * 28 * 1]이 되어야 함\n","        # images = tf.expand_dims(images, -1).astype(tf.float32)  # -> 오류 발생함 : 아래와 같이 바꾸기\n","        images = tf.cast(tf.expand_dims(images, -1), tf.float32)\n","       \n","        outputs, fake_output = self(images, training=True)  # images 대신 None 넣어도 됨\n","        real_output = self.discriminator(images, training=True)\n","\n","        # Calculate the loss for each item in the batch\n","        g_loss = self.get_generator_loss(fake_output)\n","        d_loss = self.get_discriminator_loss(real_output, fake_output)\n","\n","                # update the metrics\n","        self.generator_loss_tracker.update_state(g_loss)\n","        self.discriminator_loss_tracker.update_state(d_loss)\n","\n","        # tensorboard image update\n","        # 이 이미지를 wandb랑 sync 하는 코드를 항상 넣어놓았으니\n","        # 이 이미지 업데이트 되면 wandb에서도 찍힘\n","        # max_outputs=5 : batch가 32개 들어간 다음에 맨 앞에 있는 5개만 사용하겠다 -> 너무 많아지면 느려지니까\n","        tf.summary.image(\"val_real_img\", images, max_outputs=5)  # tensorflow log에 저장\n","        tf.summary.image(\"val_generated_img\", outputs, max_outputs=5)  # fake image\n","\n","        # return a dict mapping metrics names to current values\n","        # metric에 total loss가 이미 있기 때문에 그대로 사용\n","        logs = {m.name: m.result() for m in self.metrics}\n","        return logs"],"metadata":{"id":"EQV4o5LQ2h4F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Configuration 정의"],"metadata":{"id":"uDuzP0sY4b0_"}},{"cell_type":"markdown","source":["### data configuration"],"metadata":{"id":"8r1ww_KV6p-r"}},{"cell_type":"code","source":["data_fashion_mnist_cfg: dict = {\n","    \"n_class\": 10,\n","    \"train_val_split\": [0.9, 0.1],\n","    \"train_val_shuffle\": True,\n","    \"train_val_shuffle_buffer_size\": 1024,\n","    \"test_shuffle\": False,\n","    \"tset_shuffle_buffer_size\": 1024,\n","}"],"metadata":{"id":"NGVZjvXZ6qAO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### model configuration"],"metadata":{"id":"LId0Hw-W6qJr"}},{"cell_type":"code","source":["model_mnist_vae_cfg: dict = {\n","    \"name\": \"VAE\",\n","    \"data_normalize\": True,\n","    \"latent_dim\": 2,   # 잘 floting 하려면 latent_dim이 2이면 좋음\n","    \"enc\": {\n","        \"conv1\": {\n","            \"filters\": 32,\n","            \"kernel_size\": 3,\n","            \"strides\": [2, 2],\n","            \"activation\": \"relu\",\n","        },\n","        \"conv2\": {\n","            \"filters\": 64,\n","            \"kernel_size\": 3,\n","            \"strides\": [2, 2],\n","            \"activation\": \"relu\",\n","\n","        },\n","        \"out_fc\": {\n","            \"units\": 4,  # latent_dim의 두 배를 해야 split에 대한 대응 가능 (mu, log_var)\n","        },\n","    },\n","    \"dec\": {\n","        \"in_fc\": {\n","            # enc에서 받아와야 함\n","            # test 돌려서 잘 되는 것 찾기 -> 지금은 7*7*32가 잘 됨\n","            \"units\": 7*7*32,\n","        },\n","        \"reshape_shape\": [7, 7, 32],\n","        \"tr_conv1\": {\n","            \"filters\": 64,\n","            \"kernel_size\": 3,\n","            \"strides\": [2, 2],\n","            \"padding\": \"same\",\n","            \"activation\": \"relu\",\n","        },\n","        \"tr_conv2\": {\n","            \"filters\": 32,\n","            \"kernel_size\": 3,\n","            \"strides\": [2, 2],\n","            \"padding\": \"same\",\n","            \"activation\": \"relu\",\n","        },\n","        \"tr_conv3\": {\n","            \"filters\": 1,\n","            \"kernel_size\": 3,\n","            \"strides\": [1, 1],\n","            \"padding\": \"same\",\n","        },\n","    }\n","}\n","\n","model_mnist_gan_cfg: dict = {\n","    \"name\": \"GAN\",\n","    \"data_normalize\": True,\n","    \"latent_dim\": 64,   # GAN은 완전 noise에서 가져오고 어떤 도움도 안 받기 때문에 latent 차원이 좀 큰 게 좋음\n","    # discriminator\n","    \"dis\": {\n","        \"conv1\": {\n","            \"filters\": 32,\n","            \"kernel_size\": 3,\n","            \"strides\": [2, 2],\n","            \"activation\": \"relu\",\n","            \"padding\": \"same\",   # default : valid\n","        },\n","        \"conv2\": {\n","            \"filters\": 64,\n","            \"kernel_size\": 3,\n","            \"strides\": [2, 2],\n","            \"activation\": \"relu\",\n","            \"padding\": \"same\",\n","        },\n","    },\n","    # generator\n","    \"gen\": {\n","        \"in_fc\": {\n","            \"units\": 7*7*32,\n","        },\n","        \"reshape_shape\": [7, 7, 32],\n","        \"tr_conv1\": {\n","            \"filters\": 64,\n","            \"kernel_size\": 3,\n","            \"strides\": [2, 2],\n","            \"padding\": \"same\",\n","            \"use_bias\": False, # generator 부분을 좀 빡세게 하기 위해서 bias를 아예 안 써보려고 함\n","            \"activation\": \"relu\",\n","        },\n","        \"tr_conv2\": {\n","            \"filters\": 32,\n","            \"kernel_size\": 3,\n","            \"strides\": [2, 2],\n","            \"padding\": \"same\",\n","            \"activation\": \"relu\",\n","        },\n","        \"tr_conv3\": {\n","            \"filters\": 1,\n","            \"kernel_size\": 3,\n","            \"strides\": [1, 1],\n","            \"padding\": \"same\",\n","            \"activation\": \"sigmoid\",\n","        },\n","    }\n","}"],"metadata":{"id":"nALBOsFe3voa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### optimizer configuration"],"metadata":{"id":"DW0a1cVelHKl"}},{"cell_type":"code","source":["adam_warmup_lr_sch_opt_cfg = {\n","    \"optimizer\": {\n","        \"name\": \"Adam\",\n","        \"other_kwargs\": {},\n","    },\n","    \"lr_scheduler\": {\n","        \"name\": \"LinearWarmupLRSchedule\",\n","        \"kwargs\": {\n","            \"lr_peak\": 1e-3,\n","            \"warmup_end_steps\": 1500,\n","        }\n","    }\n","}\n","\n","# RAdam은 scheduler 필요 없었음\n","radam_no_lr_sch_opt_cfg = {\n","    \"optimizer\": {\n","        \"name\": \"RectifiedAdam\",\n","        \"learning_rate\": 1e-3,\n","        \"other_kwargs\": {},\n","    },\n","    \"lr_scheduler\": None\n","}\n","\n","gan_opt_cfg = {\n","    # discriminator와 generator optimizer를 서로 다른 거 쓰는 게 좋음\n","    # + 둘 다 너무 좋은 거 (rectified adam) 쓰면 성능 오히려 안 좋을 수 있음\n","    \"discriminator\": {\n","        \"optimizer\": {\n","            \"name\": \"RMSprop\",\n","            \"learning_rate\": 0.005,\n","            \"other_kwargs\": {}\n","        },\n","        \"lr_scheduler\": None,\n","    },\n","    \"generator\": {\n","        \"optimizer\": {\n","            \"name\": \"Adam\",\n","            \"learning_rate\": 0.001,\n","            \"other_kwargs\": {\n","                \"beta_1\": 0.5\n","            },\n","        },\n","        \"lr_scheduler\": None,\n","    }\n","}\n","\n","# train_cfg\n","train_cfg: dict = {\n","    \"train_batch_size\": 256,\n","    \"val_batch_size\": 32,\n","    \"test_batch_size\": 32,\n","    \"max_epochs\": 50,\n","    \"distribute_strategy\": \"MirroredStrategy\",   # colab(notebook)이 아니고 다른 server에서 하면 다른 strategy 필요\n","}\n","\n","_merged_cfg_presets = {\n","    \"vae_fashion_mnist_radam\": {\n","        \"data\": data_fashion_mnist_cfg,\n","        \"model\": model_mnist_vae_cfg,\n","        \"opt\": radam_no_lr_sch_opt_cfg,\n","        \"train\": train_cfg\n","    },\n","    \"gan_fashion_mnist\": {\n","        \"data\": data_fashion_mnist_cfg,\n","        \"model\": model_mnist_gan_cfg,\n","        \"opt\": gan_opt_cfg,\n","        \"train\": train_cfg\n","    }\n","}\n","\n","### hydra composition ###\n","# clear hydra instance -> Jupyter 환경에서 할 때는 일단 instance clear 하기\n","hydra.core.global_hydra.GlobalHydra.instance().clear()\n","\n","# register preset configs\n","register_config(_merged_cfg_presets)\n","\n","# initialization\n","hydra.initialize(config_path=None)    # yaml을 쓰고 있고 외부에서 하면 config_path 지정해야 함\n","\n","# using_config_key = \"cnn_fashion_mnist_radam\"\n","using_config_key = \"gan_fashion_mnist\"\n","cfg = hydra.compose(using_config_key)\n","\n","# define & override log _cfg\n","model_name = cfg.model.name\n","run_dirname = \"fastcampus_generative_model_tutorials_tf\"\n","run_name = f\"{datetime.now().isoformat(timespec='seconds')}-{using_config_key}-{model_name}\"\n","log_dir = os.path.join(drive_project_root, \"runs\", run_dirname, run_name)\n","\n","log_cfg = {\n","    \"run_name\": run_name,\n","    \"callbacks\": {\n","        \"TensorBoard\": {\n","            \"log_dir\": log_dir,\n","            \"update_freq\": 10,\n","        },\n","        # generative model은 classification이 상대적으로 불안정함\n","        # 어느 정도로 setting해야 적당한지 감이 잘 안 오기 때문에 이 부분 없앰\n","        # \"EarlyStopping\": {\n","        #     \"patience\": 3,\n","        #     \"verbose\": True,\n","        # }\n","    },\n","    \"wandb\": {\n","        \"project\": \"fastcampus_generative_model_tutorials_tf\",\n","        \"name\": run_name,\n","        \"tags\": [\"fastcampus_generative_model_tutorials_tf\"],\n","        \"reinit\": True,\n","        \"sync_tensorboard\": True,\n","    }\n","}\n","\n","# unlock struct of config & set log config\n","OmegaConf.set_struct(cfg, False)\n","cfg.log = log_cfg\n","\n","# relock config\n","OmegaConf.set_struct(cfg, True)\n","print(OmegaConf.to_yaml(cfg))\n","\n","# save yaml\n","# with open(os.path.join(log_dir, \"config.yaml\")) as f:\n","# with open(\"config.yaml\", \"w\") as f:\n","#     OmegaConf.save(cfg, f)\n","\n","# This would open the file we saved above\n","# and tell you the result of the model and its configs (weights, ...)\n","# You can check it whenever you want\n","# OmegaConf.load()"],"metadata":{"id":"StvxdanulHNa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_distribute_strategy(strategy_name: str, **kwargs):\n","    return getattr(tf.distribute, strategy_name)(**kwargs)\n","\n","distribute_strategy = get_distribute_strategy(cfg.train.distribute_strategy)"],"metadata":{"id":"hlbgAzj0lHQs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with distribute_strategy.scope():\n","    # 데이터 셋 정의 \n","    fashion_mnist = tf.keras.datasets.fashion_mnist\n","    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n","    \n","    # normalization\n","    if cfg.model.data_normalize:\n","        x_train = x_train / 255.0\n","        x_test = x_test / 255.0\n","\n","    # train/val splits\n","    assert sum(cfg.data.train_val_split) == 1.0\n","    train_size = int(len(x_train) * cfg.data.train_val_split[0])\n","    val_size = len(x_train) - train_size\n","\n","    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n","    if cfg.data.train_val_shuffle:\n","        dataset = dataset.shuffle(\n","            buffer_size=cfg.data.train_val_shuffle_buffer_size,\n","        )\n","    if cfg.data.test_shuffle:\n","        test_dataset = test_dataset.shuffle(\n","            buffer_size=cfg.data.test_shuffle_buffer_size,\n","        )\n","\n","    train_dataset = dataset.take(train_size)\n","    val_dataset = dataset.skip(train_size)\n","    print(len(train_dataset), len(val_dataset), len(dataset), len(test_dataset))\n","    \n","    # dataloader 정의\n","    train_batch_size = cfg.train.train_batch_size\n","    val_batch_size = cfg.train.val_batch_size\n","    test_batch_size = cfg.train.test_batch_size\n","\n","    train_dataloader = train_dataset.batch(train_batch_size, drop_remainder=True)\n","    val_dataloader = val_dataset.batch(val_batch_size, drop_remainder=True)\n","    test_dataloader = test_dataset.batch(test_batch_size, drop_remainder=True)\n","\n","sample_example = next(iter(train_dataloader))\n","# print(sample_example)"],"metadata":{"id":"ysJadQhpjkdD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## define model\n","\n","LinearWarmupLRScheduler 하는 이유\n","- SGD는 다른 optimizer 대비 learning rate 값에 매우 민감\n","  - learning rate를 잘 setting 해야 성능이 좋게 나옴 (Adam보다 더 좋게 나오기도 함)\n","- 따라서 optimizer와 함께 learning rate도 tuning 하는 게 원래는 좋음\n","- 그러나 학습 속도가 너무 느려지는 단점\n","\n","warmup을 하기 어려운 상황이면?\n","- Rectified Adam으로 먼저 테스트 해 보고, optimizer는 조절해도 거의 결과 비슷하게 나오니, 모델링 부분을 업데이트 해 보기\n","- Rectified Adam에도 tuning 할 수 있는 요소 많음\n","  - https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/RectifiedAdam"],"metadata":{"id":"04O0P4wLPmC2"}},{"cell_type":"code","source":["# 모델 정의\n","def get_model(cfg: DictConfig):\n","    if cfg.model.name == 'VAE':\n","        model = VAE(cfg)\n","    elif cfg.model.name == \"GAN\":\n","        model = GAN(cfg)\n","    else:\n","        raise NotImplementedError()\n","    \n","    return model\n","    \n","with distribute_strategy.scope():\n","    model = get_model(cfg)\n","\n","    # define optimizer & scheduler\n","    if cfg.model.name == \"GAN\":  # compile 방식이 다르기 때문에 GAN과 그외 방식 구분 필요\n","        d_optimizer, d_scheduler = get_optimizer_element(\n","            cfg.opt.discriminator.optimizer, cfg.opt.discriminator.lr_scheduler\n","        )\n","        g_optimizer, g_scheduler = get_optimizer_element(\n","            cfg.opt.generator.optimizer, cfg.opt.generator.lr_scheduler\n","        )\n","        model.compile(\n","            d_optimizer=d_optimizer,\n","            g_optimizer=g_optimizer,\n","            # run_eagerly = True,\n","        )\n","    else:\n","        optimizer, scheduler = get_optimizer_element(\n","            cfg.opt.optimizer, cfg.opt.lr_scheduler\n","        )\n","\n","        # model.compile(optimizer = optimizer, run_eagerly=True)  # run_eagerly=True : for debugging\n","        model.compile(optimizer = optimizer)  # -> 끄면 학습 빨라짐\n","\n","        # model build\n","        # 이 부분 생략해도 되지만 build를 해 놓으면 나중에 debugging하기 좋음 -> 권장\n","        # batch 1 : 임의로 설정\n","        # model.build((1, 28*28*1))\n","        model.build((train_batch_size, 28, 28, 1))      # This build code is for 'CNN'\n","\n","        # 만약 build 안 하고 summary 하면 build, fit을 하거나 input shape를 넣으라고 경고 뜸\n","        # fit은 학습이기 때문에 무거운 감이 있고 빠르게 하기 위해 build 선호\n","        model.summary()"],"metadata":{"id":"-gL7tOWBiJJa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## get callbacks"],"metadata":{"id":"rapd69GTRAuU"}},{"cell_type":"code","source":["callbacks = get_callbacks(cfg.log)"],"metadata":{"id":"lSbWZOHRSsGW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## wandb setup\n","\n","- https://docs.wandb.ai/guides/integrations/tensorflow\n","- sync_tensorboard=True : tensorflow에 적혀있는 걸 wandb에 업로드"],"metadata":{"id":"W3kBknhvHV3r"}},{"cell_type":"code","source":["# flatten_dict(cfg)   # 전부 flatten 하게 바꿔주는 함수 -> nested 구조를 모두 under bar 형태로 바꿈"],"metadata":{"id":"SpD4-l8og5RW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wandb.init(\n","    config= flatten_dict(cfg),\n","    **cfg.log.wandb\n",")"],"metadata":{"id":"WxPeYZkVHWBV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tensorboard load하기 : load extension\n","%load_ext tensorboard\n","\n","# 경로 지정 : terminal 문법이기 때문에 #을 # 그대로 인지하려면 앞에 '\\' 써줘야 함\n","%tensorboard --logdir /content/drive/MyDrive/\\#fastcampus/runs/fastcampus_generative_model_tutorials_tf\n","\n","model.fit(\n","    train_dataloader,\n","    validation_data=val_dataloader,\n","    epochs=cfg.train.max_epochs,\n","    callbacks=callbacks,\n",")"],"metadata":{"id":"HubFa3A2Szm9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## model testing"],"metadata":{"id":"F9Tx9Zu5Tg5Y"}},{"cell_type":"code","source":["# model.evaluate(test_dataloader)"],"metadata":{"id":"83w4bBslWGNw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_latent_img(model, n, single_img_size=28):\n","    # plot n * n images decoded from the latent_space\n","    \n","    norm = tfp.distributions.Normal(0, 1)\n","    grid_x = norm.quantile(np.linspace(0.05, 0.95, n))  # 0.05 ~ 0.95 중 n개 이미지 가져오기\n","    grid_y = norm.quantile(np.linspace(0.05, 0.95, n))\n","    width = single_img_size * n\n","    height = width\n","    image = np.zeros((height, width))\n","    # print(image.shape)\n","\n","    for i, yi in enumerate(grid_y):\n","        for j, xi in enumerate(grid_x):\n","            z = np.array([[xi, yi]])\n","            x_decoded = model.sample(z)\n","            digit = tf.reshape(x_decoded[0], (single_img_size, single_img_size))\n","            # print(digit.shape)\n","            image[\n","                  i * single_img_size: (i + 1) * single_img_size,\n","                  j * single_img_size: (j + 1) * single_img_size \n","            ] = digit.numpy()\n","    return image\n","\n","latent_img = get_latent_img(model, n=10)\n","plt.figure(figsize=(10, 10))\n","plt.imshow(latent_img, cmap=\"Greys_r\")\n","plt.axis(\"Off\")\n","plt.show()"],"metadata":{"id":"2pWod1MIZILf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"efuZl9Odmiwv"},"execution_count":null,"outputs":[]}]}