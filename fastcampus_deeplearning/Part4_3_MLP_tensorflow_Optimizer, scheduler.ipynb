{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02.MLP_tensorflow_Optimizer, scheduler.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["8gmIaEsDowHC","2lfmx_jAp5ac"],"authorship_tag":"ABX9TyMWE3EbzJ4FMtaTxzbfA1Xr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Tensorflow\n","- `Tensorflow`가 예전에는 쓰기 어려운 모델이었음 (코딩할 줄 아는 사람들만 사용)\n","- 그래서 `pytorch`가 많이 쓰이다 보니, `Tensorflow`에서도 쉽게 사용할 수 있는 `Keras` 만듦\n","- `Tensorflow 2.0`에서는 `keras`와 합쳐진 `tf.keras.Model`이나 `Sequential` 많이 사용\n","- `Tensorflow`에서 train step, test step을 사용하는 class 구조는 `pytorch lightening`과 비슷\n","  - `pytorch lightening` : `pytorch`를 더 쉽게 사용하기 위한 library\n","- Optimizer : Tensorflow addon\n","  - https://www.tensorflow.org/addons/overview?hl=ko\n","  - https://github.com/tensorflow/addons\n","    - 여러 tensorflow 개발자들이 다양한 optimizer 구현 코드 업로드"],"metadata":{"id":"8gmIaEsDowHC"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"bfadOzkbuIB4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import sys\n","sys.path.append('/content/drive/MyDrive/#fastcampus')\n","drive_project_root = '/content/drive/MyDrive/#fastcampus'\n","# !pip install -r '/content/drive/MyDrive/#fastcampus/requirements.txt'"],"metadata":{"id":"4AAPJkUPpFvb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install tensorflow_addons"],"metadata":{"id":"WNUrJmSfA8Ll"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install wandb"],"metadata":{"id":"wVE8r_8EBA9l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datetime import datetime\n","\n","import numpy as np\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","\n","import wandb"],"metadata":{"id":"wmIGtqQXpfJr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GPU 확인"],"metadata":{"id":"G8_oGfQKp2cT"}},{"cell_type":"code","source":["tf.config.list_physical_devices()"],"metadata":{"id":"NMr3t8-QpzX0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"L76O6F6_p3Ud"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["https://www.tensorflow.org/</br>\n","- https://www.tensorflow.org/overview/?hl=ko</br>\n","- 튜토리얼 : https://www.tensorflow.org/tutorials?hl=ko\n","- API > Tensorflow : 각 함수에 대한 설명\n","  - 구글에 'Tensorflow API 한글' 검색하면 번역본도 볼 수 있음\n","\n","초보자용 vs 전문가용\n","- 수업에서는 전문가용으로 할 예정\n","- 초보자용에서 사용하는 Sequential 버전(순차적으로 build 하는 방법)에는 한계가 있기 때문\n","- 실제 현업/연구에서는 Sequential 거의 안 씀"],"metadata":{"id":"eV6g35xTqKhF"}},{"cell_type":"markdown","source":["## define gpu\n","- https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy\n","- This strategy is typically used for training on one machine with multiple GPUs.\n","- 아래 코드 결과 보면 GPU 0번 잡아서 가져옴"],"metadata":{"id":"2lfmx_jAp5ac"}},{"cell_type":"code","source":["mirrored_strategy = tf.distribute.MirroredStrategy()"],"metadata":{"id":"N0sq47Enq1fU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## data, data loader 정의\n","\n","- 사실 tensorflow에서는 data loader를 정의를 안 하기도 함\n","- dataset으로 그냥 처리 가능\n","- 단, 여기서는 pytorch 방식과 비교하기 위해 사용함"],"metadata":{"id":"TQmPE2zdsfCx"}},{"cell_type":"code","source":["with mirrored_strategy.scope():\n","\n","    # dataset 정의 =====================================================================\n","    fashion_mnist = tf.keras.datasets.fashion_mnist\n","    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n","\n","    # normalization\n","    x_train = x_train / 255.0\n","    x_test = x_test / 255.0\n","\n","    # train/val splits\n","    train_size = int(len(x_train)*0.9)\n","    val_size = len(x_train) - train_size\n","\n","    # train, test dataset 정의\n","    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size=1024)\n","    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).shuffle(buffer_size=1024)\n","\n","    # train dataset을 train과 validation으로 나누기\n","    # tensorflow에서는 아래와 같이 take, skip 사용해서 데이터 많이 나눔\n","    train_dataset = dataset.take(train_size)   # train은 dataset에서 train_size만큼 take하고\n","    val_dataset = dataset.skip(train_size)     # val은 전체 dataset에서 train_size만큼 skip하고 남은 것\n","\n","    # 검증\n","    print(f'train total : {len(dataset)} (train : {len(train_dataset)}, validation : {len(val_dataset)})')\n","    print(f'test : {len(test_dataset)}')\n","\n","    # dataloader 정의 ==================================================================\n","    train_batch_size = 100\n","    val_batch_size = 10\n","    test_batch_size = 100\n","\n","    # drop_remainder=True : memory size가 안 맞으면 error 나는 것 방지 (pytorch는 이런 것 자동으로 처리함 = tensorflow와 차이점)\n","    train_dataloader = train_dataset.batch(train_batch_size, drop_remainder=True)\n","    val_dataloader = val_dataset.batch(val_batch_size, drop_remainder=True)\n","    test_dataloader = test_dataset.batch(test_batch_size, drop_remainder=True)"],"metadata":{"id":"_asj7gDkq1iE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_example = next(iter(train_dataloader))\n","print(sample_example)"],"metadata":{"id":"Zy_V0pfIu9tq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## plot figure"],"metadata":{"id":"qqD1A_X9q1lC"}},{"cell_type":"code","source":["plt.figure(figsize=(10,10))\n","for c in range(16):\n","    plt.subplot(4, 4, c+1)\n","    plt.imshow(x_train[c].reshape(28,28), cmap='gray')\n","plt.show()"],"metadata":{"id":"qZwUyCthq1oE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## make model"],"metadata":{"id":"L7L9eTkdq1qI"}},{"cell_type":"code","source":["class MLP(tf.keras.Model):\n","    def __init__(self, input_dim: int, h1_dim: int, h2_dim: int, out_dim: int):\n","        super().__init__()\n","\n","        # tensorflow는 pytorch와 다르게 flatten을 하지 않아도 되지만\n","        # pytorch와 비슷한 구조로 코딩하기 위해 여기서는 썼음\n","        self.flatten = tf.keras.layers.Flatten()\n","\n","        # tf nn module vs keras module\n","        # 1) nn module : tensorflow 1.0에서 사용, 기능이 조금 더 많음\n","        # 2) keras : tensorflow 2.0에서 사용\n","        self.linear1 = tf.keras.layers.Dense(input_dim=input_dim, units=h1_dim)\n","        # self.linear2 = tf.keras.layers.Dense(input_dim=h1_dim, units=h2_dim)\n","        # -> 이렇게 써도 되지만, pytorh보다 keras는 flexibility가 있어서 input_dim 생략해도 알아서 인지함\n","        self.linear2 = tf.keras.layers.Dense(units=h2_dim)\n","        self.linear3 = tf.keras.layers.Dense(units=out_dim)\n","        self.relu = tf.nn.relu\n","    \n","    # tensorflow에서는 'training=Fasle' 구문 꼭 넣기를 권장함\n","    # 나중에 regularization에서 drop out 할 때 이 부분을 조절할 수 있어야 함\n","    # - 학습일 때는 켜고, evaluation 때는 끄고\n","    def call(self, input, training=False):\n","        x = self.flatten(input)\n","        x = self.relu(self.linear1(x))\n","        x = self.relu(self.linear2(x))\n","        out = self.linear3(x)\n","        out = tf.nn.softmax(out)  # output을 확률값으로 바꿈\n","        return out\n","    \n","    # GradientTape() 구현\n","    # 따로 구현해도 되지만 class 안에 이렇게 넣어주면 나중에 더 코드가 깔끔해짐\n","    def train_step(self, data):\n","        # pass\n","        images, labels = data\n","        \n","        with tf.GradientTape() as tape:\n","            outputs = self(images, training=True)\n","            preds = tf.argmax(outputs, 1)\n","\n","            # 위에서 out이 softmax 안 거친 경우, 여기 넣을 때 softmax 처리 해줘야 함\n","            loss = self.compiled_loss(\n","                labels, outputs\n","            )\n","\n","        # compute gradients\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","\n","        # update weights\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","        \n","        # update the metrics\n","        self.compiled_metrics.update_state(labels, preds)\n","\n","        # return a dict mapping metrics names to current values\n","        logs = {m.name: m.result() for m in self.metrics}\n","        logs.update({\"loss\": loss})\n","        return logs\n","    \n","    def test_step(self, data):\n","        # pass\n","        images, labels = data\n","        outputs = self(images, training=False)\n","        preds = tf.argmax(outputs, 1)\n","        loss = self.compiled_loss(\n","            labels, outputs\n","        )\n","\n","        # update the metrics\n","        self.compiled_metrics.update_state(labels, preds)\n","\n","        # return a dict mapping metrics names to current values\n","        logs = {m.name: m.result() for m in self.metrics}\n","        logs.update({\"test_loss\": loss})\n","        return logs"],"metadata":{"id":"wn2XnFMFq1tW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dropout"],"metadata":{"id":"zYcW3GJlfKWm"}},{"cell_type":"code","source":["# tf.keras.Model 대신 MLP 써도 됨\n","# 그러면 위에서 썼던 MLP class를 가져오는 것이니\n","# MLP 모델과 달라진 __init__, call만 코드 써서 일부 수정해 주고\n","# 그대로 써도 되는 train_step, test_step는 생략 가능\n","class MLPWithDropout(tf.keras.Model):\n","    def __init__(self, input_dim: int, h1_dim: int, h2_dim: int, out_dim: int, dropout_prob: float):\n","        super().__init__()\n","        self.flatten = tf.keras.layers.Flatten()\n","        self.linear1 = tf.keras.layers.Dense(input_dim=input_dim, units=h1_dim)\n","        self.linear2 = tf.keras.layers.Dense(units=h2_dim)\n","        self.linear3 = tf.keras.layers.Dense(units=out_dim)\n","        self.dropout = tf.keras.layers.Dropout(dropout_prob)\n","        self.relu = tf.nn.relu\n","    \n","    def call(self, input, training=False):\n","        x = self.flatten(input)\n","        x = self.relu(self.linear1(x))\n","        x = self.dropout(x, training=training)\n","        x = self.relu(self.linear2(x))\n","        x = self.dropout(x, training=training)\n","        out = self.linear3(x)\n","        out = tf.nn.softmax(out)\n","        return out\n","\n","    def train_step(self, data):\n","        # pass\n","        images, labels = data\n","        \n","        with tf.GradientTape() as tape:\n","            outputs = self(images, training=True)\n","            preds = tf.argmax(outputs, 1)\n","\n","            # 위에서 out이 softmax 안 거친 경우, 여기 넣을 때 softmax 처리 해줘야 함\n","            loss = self.compiled_loss(\n","                labels, outputs\n","            )\n","\n","        # compute gradients\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","\n","        # update weights\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","        \n","        # update the metrics\n","        self.compiled_metrics.update_state(labels, preds)\n","\n","        # return a dict mapping metrics names to current values\n","        logs = {m.name: m.result() for m in self.metrics}\n","        return logs\n","    \n","    def test_step(self, data):\n","        # pass\n","        images, labels = data\n","        outputs = self(images, training=False)\n","        preds = tf.argmax(outputs, 1)\n","        loss = self.compiled_loss(\n","            labels, outputs\n","        )\n","\n","        # update the metrics\n","        self.compiled_metrics.update_state(labels, preds)\n","\n","        # return a dict mapping metrics names to current values\n","        logs = {m.name: m.result() for m in self.metrics}\n","        return logs     "],"metadata":{"id":"1AbjCxDNe9jV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Learning Rate scheduler 구현\n","- A Learning rate schedule is a predefined framework that adjusts the learning rate between epochs or iterations as the training progresses\n","- tensorflow 페이지 들어가보면 schedules에서 learning rate decay 관련 자료는 많지만 warmup은 많이 쓰임에도 불구하고 자료가 잘 없음\n","  - 당시 library가 2015년 이전에 많이 정립이 되었는데\n","  - warmup은 residual network, transformer에 사용하기 위해 2015년 이후에 발견되었기 때문\n","- 그래서 이번 연습에서는 warmup scheduler를 구현해 볼 예정\n","- warmup은 맨 처음에 0에 가까운 숫자에서 시작하고 원하는 learning rate까지 linear하게 점점 증가하는 형태\n","  - 따라서 `def __init__(self, initial_learning_rate)`로 보통은 쓰지만 여기서는 `initial_learning_rate` 부분이 필요하지 않음 "],"metadata":{"id":"S28iNbShBSu6"}},{"cell_type":"code","source":["class LinearWarmupLRScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n","\n","    def __init__(self, lr_peak: float, warmup_end_steps: int,):\n","        super().__init__()\n","        self.lr_peak = lr_peak\n","        self.warmup_end_steps = warmup_end_steps\n","    \n","    def __call__(self, step):\n","\n","        # tensor 형태로 변환 안 하면 error 나기도 해서 tensor 형태로 바꾸기\n","        step_float = tf.cast(step, tf.float32)\n","        warmup_end_step = tf.cast(self.warmup_end_steps, tf.float32)\n","        lr_peak = tf.cast(self.lr_peak, tf.float32)\n","\n","        # tensor 형태는 if 구문 못 쓰니 tf.cond 형식으로 구현해야 함\n","        # tf.cond(condition, condition이 true인 경우 값, false인 경우 값)\n","        return tf.cond(\n","            step_float < warmup_end_step,\n","            # if True : 보통 lambda 형태로 많이 구현 (callable 형태) \n","            # 만약 step이 0부터 시작하면 lr_peak 값이 0이 되는데, 그러면 문제가 생길 수 있으니 step이 0이 되지 않도록 maximum 처리\n","            lambda: lr_peak * (tf.math.maximum(step_float, 1) / warmup_end_step),   # -> 서서히 증가할 예정\n","            # else\n","            lambda: lr_peak,\n","        )"],"metadata":{"id":"vSSHNMAVBSxR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["잘 만들어졌는지 test\n","- 결과 : <tf.Tensor: shape=(), dtype=float32, numpy=0.0005>\n","  - 0.001의 절반에 해당하는 값(0.0005)이 잘 나옴"],"metadata":{"id":"j1cnIIFbFUiZ"}},{"cell_type":"code","source":["a = LinearWarmupLRScheduler(lr_peak=1e-3, warmup_end_steps=1000)\n","a(500)"],"metadata":{"id":"DAdio_g0E9kX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = LinearWarmupLRScheduler(lr_peak=1e-3, warmup_end_steps=1000)\n","a(1500)"],"metadata":{"id":"n42dzrDqF00W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0을 넣어도 maximum 처리를 해줬기 때문에 0은 아닌 값으로 결과가 잘 나옴"],"metadata":{"id":"BxHeUWMgF6XT"}},{"cell_type":"code","source":["a = LinearWarmupLRScheduler(lr_peak=1e-3, warmup_end_steps=1000)\n","a(0)"],"metadata":{"id":"EuJP3opbF4de"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## AdamP import"],"metadata":{"id":"-6ALFh-POD-X"}},{"cell_type":"markdown","source":["아래에서 AdamP 사용하려는데 error 나서 google에 AdamP 검색하고 필요한 코드 부분 그대로 가져와서 붙여 넣음\n","- https://github.com/taki0112/AdamP-Tensorflow/blob/master/adamp_tf.py\n","- class AdamP 부분 그대로 붙여 넣으면 코드 오류 남\n","  - optimizer_v2.OptimizerV2 -> tensorflow.python.keras.optimizer_v2.OptimizerV2\n","  - github 코드 확인해보면 `optimizer_v2.OptimizerV2`를 `tensorflow.python.keras`에서 import 하는 것 볼 수 있음\n","  - 그래도 오류남\n","  - 오래된 문서고 수정된 버전을 안 올려나서 그럴 수 있음\n","- 이럴 땐 공신력 있는 addon에서 rectified adam import 하는 코드 참고\n","  - https://github.com/tensorflow/addons/blob/v0.15.0/tensorflow_addons/optimizers/rectified_adam.py#L24-L328\n","  - 앞에 `@tf.keras.utils.register_keras_serializable(package=\"Addons\")` 붙일 것\n","  - 안에 `tf.keras.optimizers.Optimizer`로 바꿔볼 것\n","  - 에러 해결"],"metadata":{"id":"67IRs4yIMRgI"}},{"cell_type":"code","source":["from tensorflow.python.framework import ops\n","from tensorflow.python.keras import backend_config\n","from tensorflow.python.keras.optimizer_v2 import optimizer_v2\n","from tensorflow.python.ops import array_ops\n","from tensorflow.python.ops import control_flow_ops\n","from tensorflow.python.ops import math_ops\n","from tensorflow.python.ops import state_ops"],"metadata":{"id":"7w_ccFYsOy4c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@tf.keras.utils.register_keras_serializable(package=\"Addons\")\n","class AdamP(tf.keras.optimizers.Optimizer):\n","    # 출처 적기\n","    # Code is from https://github.com/taki0112/AdamP-Tensorflow/blob/master/adamp_tf.py with modification\n","    _HAS_AGGREGATE_GRAD = True\n","\n","    def __init__(self,\n","                 learning_rate=0.001,\n","                 beta_1=0.9,\n","                 beta_2=0.999,\n","                 epsilon=1e-8,\n","                 weight_decay=0.0,\n","                 delta=0.1, wd_ratio=0.1, nesterov=False,\n","                 name='AdamP',\n","                 **kwargs):\n","\n","        super(AdamP, self).__init__(name, **kwargs)\n","        self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n","        self._set_hyper('beta_1', beta_1)\n","        self._set_hyper('beta_2', beta_2)\n","        self._set_hyper('delta', delta)\n","        self._set_hyper('wd_ratio', wd_ratio)\n","\n","        self.epsilon = epsilon or backend_config.epsilon()\n","        self.weight_decay = weight_decay\n","        self.nesterov = nesterov\n","\n","    def _create_slots(self, var_list):\n","        # Create slots for the first and second moments.\n","        # Separate for-loops to respect the ordering of slot variables from v1.\n","        for var in var_list:\n","            self.add_slot(var, 'm')\n","        for var in var_list:\n","            self.add_slot(var, 'v')\n","        for var in var_list:\n","            self.add_slot(var, 'p')\n","\n","    def _prepare_local(self, var_device, var_dtype, apply_state):\n","        super(AdamP, self)._prepare_local(var_device, var_dtype, apply_state)\n","\n","        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n","        beta_1_t = array_ops.identity(self._get_hyper('beta_1', var_dtype))\n","        beta_2_t = array_ops.identity(self._get_hyper('beta_2', var_dtype))\n","        beta_1_power = math_ops.pow(beta_1_t, local_step)\n","        beta_2_power = math_ops.pow(beta_2_t, local_step)\n","\n","        lr = apply_state[(var_device, var_dtype)]['lr_t']\n","        bias_correction1 = 1 - beta_1_power\n","        bias_correction2 = 1 - beta_2_power\n","\n","        delta = array_ops.identity(self._get_hyper('delta', var_dtype))\n","        wd_ratio = array_ops.identity(self._get_hyper('wd_ratio', var_dtype))\n","\n","        apply_state[(var_device, var_dtype)].update(\n","            dict(\n","                lr=lr,\n","                epsilon=ops.convert_to_tensor_v2(self.epsilon, var_dtype),\n","                weight_decay=ops.convert_to_tensor_v2(self.weight_decay, var_dtype),\n","                beta_1_t=beta_1_t,\n","                beta_1_power=beta_1_power,\n","                one_minus_beta_1_t=1 - beta_1_t,\n","                beta_2_t=beta_2_t,\n","                beta_2_power=beta_2_power,\n","                one_minus_beta_2_t=1 - beta_2_t,\n","                bias_correction1=bias_correction1,\n","                bias_correction2=bias_correction2,\n","                delta=delta,\n","                wd_ratio=wd_ratio))\n","\n","    def set_weights(self, weights):\n","        params = self.weights\n","        # If the weights are generated by Keras V1 optimizer, it includes vhats\n","        # optimizer has 2x + 1 variables. Filter vhats out for compatibility.\n","        num_vars = int((len(params) - 1) / 2)\n","        if len(weights) == 3 * num_vars + 1:\n","            weights = weights[:len(params)]\n","        super(AdamP, self).set_weights(weights)\n","\n","    def _resource_apply_dense(self, grad, var, apply_state=None):\n","        var_device, var_dtype = var.device, var.dtype.base_dtype\n","        coefficients = ((apply_state or {}).get((var_device, var_dtype))\n","                        or self._fallback_apply_state(var_device, var_dtype))\n","\n","        # m_t = beta1 * m + (1 - beta1) * g_t\n","        m = self.get_slot(var, 'm')\n","        m_scaled_g_values = grad * coefficients['one_minus_beta_1_t']\n","        m_t = state_ops.assign(m, m * coefficients['beta_1_t'] + m_scaled_g_values, use_locking=self._use_locking)\n","\n","        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n","        v = self.get_slot(var, 'v')\n","        v_scaled_g_values = (grad * grad) * coefficients['one_minus_beta_2_t']\n","        v_t = state_ops.assign(v, v * coefficients['beta_2_t'] + v_scaled_g_values, use_locking=self._use_locking)\n","\n","        denorm = (math_ops.sqrt(v_t) / math_ops.sqrt(coefficients['bias_correction2'])) + coefficients['epsilon']\n","        step_size = coefficients['lr'] / coefficients['bias_correction1']\n","\n","        if self.nesterov:\n","            perturb = (coefficients['beta_1_t'] * m_t + coefficients['one_minus_beta_1_t'] * grad) / denorm\n","        else:\n","            perturb = m_t / denorm\n","\n","        # Projection\n","        wd_ratio = 1\n","        if len(var.shape) > 1:\n","            perturb, wd_ratio = self._projection(var, grad, perturb, coefficients['delta'], coefficients['wd_ratio'], coefficients['epsilon'])\n","\n","        # Weight decay\n","\n","        if self.weight_decay > 0:\n","            var = state_ops.assign(var, var * (1 - coefficients['lr'] * coefficients['weight_decay'] * wd_ratio), use_locking=self._use_locking)\n","\n","        var_update = state_ops.assign_sub(var, step_size * perturb, use_locking=self._use_locking)\n","\n","        return control_flow_ops.group(*[var_update, m_t, v_t])\n","\n","\n","    def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n","\n","        var_device, var_dtype = var.device, var.dtype.base_dtype\n","        coefficients = ((apply_state or {}).get((var_device, var_dtype))\n","                        or self._fallback_apply_state(var_device, var_dtype))\n","        \"\"\"\n","        Adam\n","        \"\"\"\n","        # m_t = beta1 * m + (1 - beta1) * g_t\n","        m = self.get_slot(var, 'm')\n","        m_scaled_g_values = grad * coefficients['one_minus_beta_1_t']\n","        m_t = state_ops.assign(m, m * coefficients['beta_1_t'],\n","                               use_locking=self._use_locking)\n","        with ops.control_dependencies([m_t]):\n","            m_t = self._resource_scatter_add(m, indices, m_scaled_g_values)\n","\n","        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n","        v = self.get_slot(var, 'v')\n","        v_scaled_g_values = (grad * grad) * coefficients['one_minus_beta_2_t']\n","        v_t = state_ops.assign(v, v * coefficients['beta_2_t'],\n","                               use_locking=self._use_locking)\n","        with ops.control_dependencies([v_t]):\n","            v_t = self._resource_scatter_add(v, indices, v_scaled_g_values)\n","\n","        denorm = (math_ops.sqrt(v_t) / math_ops.sqrt(coefficients['bias_correction2'])) + coefficients['epsilon']\n","        step_size = coefficients['lr'] / coefficients['bias_correction1']\n","\n","        if self.nesterov:\n","            p_scaled_g_values = grad * coefficients['one_minus_beta_1_t']\n","            perturb = m_t * coefficients['beta_1_t']\n","            perturb = self._resource_scatter_add(perturb, indices, p_scaled_g_values) / denorm\n","\n","        else:\n","            perturb = m_t / denorm\n","\n","        # Projection\n","        wd_ratio = 1\n","        if len(var.shape) > 1:\n","            perturb, wd_ratio = self._projection(var, grad, perturb, coefficients['delta'], coefficients['wd_ratio'], coefficients['epsilon'])\n","\n","        # Weight decay\n","        if self.weight_decay > 0:\n","            var = state_ops.assign(var, var * (1 - coefficients['lr'] * coefficients['weight_decay'] * wd_ratio), use_locking=self._use_locking)\n","\n","        var_update = state_ops.assign_sub(var, step_size * perturb, use_locking=self._use_locking)\n","\n","        return control_flow_ops.group(*[var_update, m_t, v_t])\n","\n","    def _channel_view(self, x):\n","        return array_ops.reshape(x, shape=[x.shape[0], -1])\n","\n","    def _layer_view(self, x):\n","        return array_ops.reshape(x, shape=[1, -1])\n","\n","    def _cosine_similarity(self, x, y, eps, view_func):\n","        x = view_func(x)\n","        y = view_func(y)\n","\n","        x_norm = math_ops.euclidean_norm(x, axis=-1) + eps\n","        y_norm = math_ops.euclidean_norm(y, axis=-1) + eps\n","        dot = math_ops.reduce_sum(x * y, axis=-1)\n","\n","        return math_ops.abs(dot) / x_norm / y_norm\n","\n","    def _projection(self, var, grad, perturb, delta, wd_ratio, eps):\n","        # channel_view\n","        cosine_sim = self._cosine_similarity(grad, var, eps, self._channel_view)\n","        cosine_max = math_ops.reduce_max(cosine_sim)\n","        compare_val = delta / math_ops.sqrt(math_ops.cast(self._channel_view(var).shape[-1], dtype=delta.dtype))\n","\n","        perturb, wd = control_flow_ops.cond(pred=cosine_max < compare_val,\n","                                            true_fn=lambda : self.channel_true_fn(var, perturb, wd_ratio, eps),\n","                                            false_fn=lambda : self.channel_false_fn(var, grad, perturb, delta, wd_ratio, eps))\n","\n","        return perturb, wd\n","\n","    def channel_true_fn(self, var, perturb, wd_ratio, eps):\n","        expand_size = [-1] + [1] * (len(var.shape) - 1)\n","        var_n = var / (array_ops.reshape(math_ops.euclidean_norm(self._channel_view(var), axis=-1), shape=expand_size) + eps)\n","        perturb -= var_n * array_ops.reshape(math_ops.reduce_sum(self._channel_view(var_n * perturb), axis=-1), shape=expand_size)\n","        wd = wd_ratio\n","\n","        return perturb, wd\n","\n","    def channel_false_fn(self, var, grad, perturb, delta, wd_ratio, eps):\n","        cosine_sim = self._cosine_similarity(grad, var, eps, self._layer_view)\n","        cosine_max = math_ops.reduce_max(cosine_sim)\n","        compare_val = delta / math_ops.sqrt(math_ops.cast(self._layer_view(var).shape[-1], dtype=delta.dtype))\n","\n","        perturb, wd = control_flow_ops.cond(cosine_max < compare_val,\n","                                              true_fn=lambda : self.layer_true_fn(var, perturb, wd_ratio, eps),\n","                                              false_fn=lambda : self.identity_fn(perturb))\n","\n","        return perturb, wd\n","\n","    def layer_true_fn(self, var, perturb, wd_ratio, eps):\n","        expand_size = [-1] + [1] * (len(var.shape) - 1)\n","        var_n = var / (array_ops.reshape(math_ops.euclidean_norm(self._layer_view(var), axis=-1), shape=expand_size) + eps)\n","        perturb -= var_n * array_ops.reshape(math_ops.reduce_sum(self._layer_view(var_n * perturb), axis=-1), shape=expand_size)\n","        wd = wd_ratio\n","\n","        return perturb, wd\n","\n","    def identity_fn(self, perturb):\n","        wd = 1.0\n","\n","        return perturb, wd\n","\n","    def get_config(self):\n","        config = super(AdamP, self).get_config()\n","        config.update({\n","            'learning_rate': self._serialize_hyperparameter('learning_rate'),\n","            'beta_1': self._serialize_hyperparameter('beta_1'),\n","            'beta_2': self._serialize_hyperparameter('beta_2'),\n","            'delta': self._serialize_hyperparameter('delta'),\n","            'wd_ratio': self._serialize_hyperparameter('wd_ratio'),\n","            'epsilon': self.epsilon,\n","            'weight_decay': self.weight_decay,\n","            'nesterov': self.nesterov\n","        })\n","        return "],"metadata":{"id":"cWYaynSAMRii"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## define model\n","\n","LinearWarmupLRScheduler 하는 이유\n","- SGD는 다른 optimizer 대비 learning rate 값에 매우 민감\n","  - learning rate를 잘 setting 해야 성능이 좋게 나옴 (Adam보다 더 좋게 나오기도 함)\n","- 따라서 optimizer와 함께 learning rate도 tuning 하는 게 원래는 좋음\n","- 그러나 학습 속도가 너무 느려지는 단점\n","\n","warmup을 하기 어려운 상황이면?\n","- Rectified Adam으로 먼저 테스트 해 보고, optimizer는 조절해도 거의 결과 비슷하게 나오니, 모델링 부분을 업데이트 해 보기\n","- Rectified Adam에도 tuning 할 수 있는 요소 많음\n","  - https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/RectifiedAdam"],"metadata":{"id":"04O0P4wLPmC2"}},{"cell_type":"code","source":["n_class = 10\n","max_epoch = 50\n","\n","with mirrored_strategy.scope():\n","    # channel : rgb가 없고 gray니까 1\n","    # model = MLP(28*28*1, 128, 64, n_class)  # *args\n","    model = MLPWithDropout(28*28*1, 128, 64, n_class, dropout_prob=0.3)\n","    model_name = type(model).__name__       # MLP\n","\n","    # define loss\n","    loss_function = tf.losses.SparseCategoricalCrossentropy()\n","\n","    # define optimizer\n","    lr = 1e-3\n","    # scheduler = None\n","    scheduler = LinearWarmupLRScheduler(lr, 1500)  # learning rate 추가, warmup이 있으니 속도가 느리지만 안정적인 결과 \n","    scheduler_name = type(scheduler).__name__ if scheduler is not None else \"no_scheduler\"\n","\n","    if scheduler is None:\n","        scheduler = lr\n","\n","    # optimizer = tf.optimizers.Adam(learning_rate=scheduler)\n","    optimizer = tf.optimizers.SGD(learning_rate=scheduler)\n","    # optimizer = tfa.optimizers.AdamW(learning_rate=scheduler)\n","    # optimizer = tfa.optimizers.RectifiedAdam(learning_rate=scheduler)\n","    # optimizer = AdamP(learning_rate=scheduler)\n","    optimizer_name = type(optimizer).__name__\n","\n","    model.compile(\n","        loss = loss_function,\n","        optimizer = optimizer,\n","        metrics = [tf.keras.metrics.Accuracy()],\n","    )\n","\n","    # model build\n","    # 이 부분 생략해도 되지만 build를 해 놓으면 나중에 debugging하기 좋음 -> 권장\n","    # batch 1 : 임의로 설정\n","    model.build((1, 28*28*1))\n","\n","# 만약 build 안 하고 summary 하면 build, fit을 하거나 input shape를 넣으라고 경고 뜸\n","# fit은 학습이기 때문에 무거운 감이 있고 빠르게 하기 위해 build 선호\n","model.summary()"],"metadata":{"id":"P1kMS7uKq1wU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## define logging & callbacks"],"metadata":{"id":"rapd69GTRAuU"}},{"cell_type":"code","source":["log_interval = 100\n","run_name = f'{datetime.now()}-{model.name}-{optimizer_name}-optim-{lr}-lr-with-{scheduler_name}'\n","\n","run_dirname = 'dnn-tutorial-fashion-mnist-runs-tf'\n","\n","# 경로에 'run'이라는 폴더를 만들고, run_dirname에 run_name 생성\n","log_dir = os.path.join(drive_project_root, \"runs\", run_dirname, run_name)"],"metadata":{"id":"qc5A36plRsaE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tb_callback = tf.keras.callbacks.TensorBoard(\n","    log_dir, update_freq=log_interval\n",")"],"metadata":{"id":"lSbWZOHRSsGW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## wandb setup\n","\n","- https://docs.wandb.ai/guides/integrations/tensorflow\n","- sync_tensorboard=True : tensorflow에 적혀있는 걸 wandb에 업로드"],"metadata":{"id":"W3kBknhvHV3r"}},{"cell_type":"code","source":["project_name = 'fastcampus_fashion_mnist_tutorials_tf'\n","run_tags = {project_name}   # project가 많으면 이렇게 tag명으로 관리\n","\n","wandb.init(\n","    project=project_name, \n","    name=run_name, \n","    tags=run_tags, \n","    config={\n","        \"lr\": lr,\n","        \"model_name\": model_name,\n","        \"optimizer_name\": optimizer_name,\n","        \"scheduler_name\": scheduler_name,\n","    },\n","    reinit=True,\n","    sync_tensorboard=True\n","    )"],"metadata":{"id":"WxPeYZkVHWBV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["early stopping\n","- https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n","  - view source code 클릭해서 참고하고 customize (세부 조정) 가능\n","- patience=5 : improve가 되지 않아도 5 step동안 기다리겠다는 의미"],"metadata":{"id":"i45VAnN3iO8h"}},{"cell_type":"code","source":["early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=5, verbose=True)"],"metadata":{"id":"Qu43PLE5iJ7G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["경로 잘 찾고 있는지 확인"],"metadata":{"id":"YGTecT2yVqp8"}},{"cell_type":"code","source":["! ls /content/drive/MyDrive/\\#fastcampus/runs/"],"metadata":{"id":"1u3ORwmUVijX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tensorboard load하기 : load extension\n","%load_ext tensorboard\n","\n","# 경로 지정 : terminal 문법이기 때문에 #을 # 그대로 인지하려면 앞에 '\\' 써줘야 함\n","%tensorboard --logdir /content/drive/MyDrive/\\#fastcampus/runs/\n","\n","model.fit(\n","    train_dataloader,\n","    validation_data=val_dataloader,\n","    epochs=max_epoch,\n","    # callbacks=[tb_callback] # callback은 여러 개를 넣을 수 있기 때문에 list 형태로 지정함\n","    callbacks=[tb_callback, early_stop_callback]\n",")"],"metadata":{"id":"HubFa3A2Szm9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## model testing"],"metadata":{"id":"F9Tx9Zu5Tg5Y"}},{"cell_type":"code","source":["model.evaluate(test_dataloader)"],"metadata":{"id":"83w4bBslWGNw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["auc curve"],"metadata":{"id":"XnbF1YQ_WM1p"}},{"cell_type":"code","source":["test_labels_list = []\n","test_preds_list = []\n","test_outputs_list = []\n","\n","for i, (test_images, test_labels) in enumerate(tqdm(test_dataloader, position=0, leave=True, desc='testing')):\n","    with mirrored_strategy.scope():\n","        test_outputs = model(test_images)\n","    test_preds = tf.argmax(test_outputs, 1)\n","\n","    final_outs = test_outputs.numpy()\n","    test_outputs_list.extend(final_outs)\n","\n","    test_preds_list.extend(test_preds.numpy())\n","    test_labels_list.extend(test_labels.numpy())\n","\n","test_preds_list = np.array(test_preds_list)\n","test_labels_list = np.array(test_labels_list)\n","\n","test_accuracy = np.mean(test_preds_list == test_labels_list)\n","print(f'\\nacc: {test_accuracy*100}%')"],"metadata":{"id":"5K99rt_2WJ1O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["roc curve"],"metadata":{"id":"Ib1UnftTXUkw"}},{"cell_type":"code","source":["from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score\n","\n","fpr = {}\n","tpr = {}\n","thresh = {}\n","n_class = 10\n","\n","for i in range(n_class):\n","    fpr[i], tpr[i], thresh[i] = roc_curve(test_labels_list, np.array(test_outputs_list)[:, i], pos_label=i)"],"metadata":{"id":"1-guDbjmXhR3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fpr"],"metadata":{"id":"ncHDtbpCX8oJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["plot"],"metadata":{"id":"a32IhqbCYOrO"}},{"cell_type":"code","source":["for i in range(n_class):\n","    plt.plot(fpr[i], tpr[i], linestyle=\"--\", label=f\"Class {i} vs Rest\") \n","\n","plt.title(\"Multi-class ROC Curve\")\n","plt.xlabel(\"Flase Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.legend(loc=\"best\")\n","plt.show()"],"metadata":{"id":"r7tdD3yhYR9Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["auc score\n","- multi class이기 때문에 multi_class, average option 안 넣어주면 error 발생"],"metadata":{"id":"uglGAHtqY6rq"}},{"cell_type":"code","source":["auc_score = roc_auc_score(test_labels_list, test_outputs_list, multi_class=\"ovo\", average=\"macro\")"],"metadata":{"id":"vfISE8sBYoeY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'auc score : {auc_score*100}')"],"metadata":{"id":"Z01rKFr4Y2Uv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"2pWod1MIZILf"},"execution_count":null,"outputs":[]}]}