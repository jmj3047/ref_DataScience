{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9115d97",
   "metadata": {},
   "source": [
    "# Action Recognition Model\n",
    "- video 분류 문제 : input 짧은 영상 -> label 분류\n",
    "- 영상 format은 크게 중요하지 않음 (codec이 있으면 python으로 읽는 건 다 똑같음) \n",
    "- UCF dataset\n",
    "  - https://www.crcv.ucf.edu/data/UCF101.php\n",
    "  - UCF101, 50, 11 : 숫자는 class의 개수라고 생각하면 됨 ex) 101가지 분류\n",
    "    - 숫자가 클 수록 task가 어려워짐\n",
    "  - 제일 작은 dataset 사용해 볼 것임 ([UCF 11](https://www.crcv.ucf.edu/data/UCF_YouTube_Action.php))\n",
    "    - 29.97 fps : 1초에 총 30여 개의 사진이 있는 것 = 코드로 4초짜리 영상을 읽으면 대략 120개의 사진을 읽어야 하는 것\n",
    "    - 매우 짧은 영상(4초내외)이고 한 영상에 한 가지 동작만 있음 (걷다가 자전거 타다가 등 하지 않음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f4dee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f1573b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911fab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from glob import glob\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a09d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'  # GPU device number 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f8c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob('UCF11_updated_mpg/*/*/*.mpg')\n",
    "len(file_paths)  # 영상 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16494ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_paths[0])\n",
    "cap = cv2.VideoCapture(file_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ebbb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "while True:\n",
    "    ret, frame = cap.read()  # generator처럼 read를 할 때마다 한 frame 씩 읽음\n",
    "    if not ret:   # 더 이상 읽을 frame이 없으면 return은 False가 됨\n",
    "        break\n",
    "\n",
    "    frame = cv2.resize(frame, (256, 256))\n",
    "    frame = frame[:, :, [2, 1, 0]]  # 채널 부분만 변경 필요 : BGR -> RGB\n",
    "    frames.append(frame)\n",
    "\n",
    "# ★ 반드시 영상 닫아줘야 함\n",
    "# cap 만드는 순간 이 코드부터 적을 것\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe9b427",
   "metadata": {},
   "source": [
    "잘 읽어 왔는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1efe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(frames)  # list -> array\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(10):\n",
    "    plt.subplot(10, 3, 1 + 3*i)   # 10*3 = 30개 사진 읽어보고자 함\n",
    "    plt.imshow(arr[1 + 3*i])\n",
    "    plt.subplot(10, 3, 2 + 3*i)\n",
    "    plt.imshow(arr[2 + 3*i])\n",
    "    plt.subplot(10, 3, 3 + 3*i)\n",
    "    plt.imshow(arr[3 + 3*i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f899d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1\n",
    "print(len(frames))        # 137개의 frame 읽음\n",
    "print(len(frames)/29.97)  # 한 영상 당 4초\n",
    "print('\\n')\n",
    "\n",
    "# 방법2\n",
    "for file_path in file_paths:\n",
    "    cap = cv2.VideoCapture(file_path)\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) # property frame count\n",
    "    print(file_path)\n",
    "    print(length, 'frames')\n",
    "    print(length / 29.97, 'sec')    \n",
    "    cap.release()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c306d08",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3dd610",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    columns = [\n",
    "        'file_path',  # 영상 경로\n",
    "        'frames',     # 몇 frame\n",
    "        'duration',   # 영상 길이\n",
    "        'label'       # class\n",
    "    ]\n",
    ")\n",
    "\n",
    "for file_path in file_paths:\n",
    "    label = file_path.split('/')[1]\n",
    "    \n",
    "    cap = cv2.VideoCapture(file_path)\n",
    "    frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = frames/29.97\n",
    "    \n",
    "    elem = {\n",
    "        'file_path': file_path,\n",
    "        'frames': frames,\n",
    "        'duration': duration,\n",
    "        'label': label\n",
    "    }\n",
    "    \n",
    "    df.loc[len(df)] = elem\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff47a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5495c7",
   "metadata": {},
   "source": [
    "### 1.1 같은 label에 속한 영상의 총 길이, 평균 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f9d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duration_sum_by_label = df.groupby('label').\\\n",
    "    duration.sum().rename('Sum').reset_index().\\\n",
    "    set_index('label')   # label을 index로 사용\n",
    "\n",
    "df_duration_sum_by_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade436d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duration_avg_by_label = df.groupby('label').\\\n",
    "    duration.mean().rename('Average').reset_index().\\\n",
    "    set_index('label')   # label을 index로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbc4273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index가 같아서 합치기 가능\n",
    "df_video_stats = \\\n",
    "    pd.concat([df_duration_sum_by_label, df_duration_avg_by_label], axis=1)\n",
    "\n",
    "df_video_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949bf288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y-axis 두 개 사용하기\n",
    "df_video_stats.plot.bar(secondary_y='Average')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f995a8e",
   "metadata": {},
   "source": [
    "### 1.2 label 내 영상 길이 별 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d0df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['> 10.0 Sec'] = df['duration'] >= 10.0  # 10초 이상의 영상들\n",
    "df['5.0 - 10.0 Sec'] = \\\n",
    "    (5.0 <= df['duration']) & (df['duration'] < 10.0)\n",
    "df['2.0 - 5.0 Sec'] = \\\n",
    "    (2.0 <= df['duration']) & (df['duration'] < 5.0)\n",
    "df['0.0 - 2.0 Sec'] = \\\n",
    "    (0.0 <= df['duration']) & (df['duration'] < 2.0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f088fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupby_label = df.groupby('label')\n",
    "df_groupby_stats = pd.concat([\n",
    "    df_groupby_label['0.0 - 2.0 Sec'].\n",
    "    sum().reset_index().set_index('label'),\n",
    "    df_groupby_label['2.0 - 5.0 Sec'].\n",
    "    sum().reset_index().set_index('label'),\n",
    "    df_groupby_label['5.0 - 10.0 Sec'].\n",
    "    sum().reset_index().set_index('label'),\n",
    "    df_groupby_label['> 10.0 Sec'].          # False = 0, True = 1\n",
    "    sum().reset_index().set_index('label'),  # 결국 label 별 개수 세는 것\n",
    "], axis=1)\n",
    "\n",
    "df_groupby_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a3a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupby_stats.plot.bar(stacked=True, ylabel='Number of Videos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd1b3f",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Data Loader\n",
    "\n",
    "- CNN (2D) 모델로 video classification 하기 위한 데이터 처리 방법\n",
    "- video는 frame의 연속이기 때문에 각 frame을 tree classification model로 분류하고, 그 frame들의 평균 값을 내면 video 예측 가능\n",
    "  - 이를 위해서는 video에서 각 frame을 모두 분리해서 저장 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42018e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dir = glob('UCF11_updated_mpg/*')\n",
    "label_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e201cd9",
   "metadata": {},
   "source": [
    "현재 상황\n",
    "- 11개의 class가 있음\n",
    "- class 당 25개의 영상 group이 있음\n",
    "  - 한 group 안에서 train, validation을 나누면, 결국 비슷한 영상이기 때문에 validation이 높게 측정될 수 있음\n",
    "  - 이러한 현상을 방지하기 위해 group을 미리 분류하고자 함\n",
    "    - 1\\~20 : training, 20\\~25 : validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57de24de",
   "metadata": {},
   "source": [
    "### 2.1 train, validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece5c1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(\n",
    "    columns=['file_path', 'label']\n",
    ")\n",
    "\n",
    "\n",
    "valid_df = pd.DataFrame(\n",
    "    columns=['file_path', 'label']\n",
    ")\n",
    "\n",
    "label_dirs = glob('UCF11_updated_mpg/*')\n",
    "\n",
    "for label_dir in label_dirs:\n",
    "    file_dir = glob(label_dir + '/v_*')\n",
    "    random.shuffle(file_dir)   # inplace\n",
    "    \n",
    "    for i in range(20):\n",
    "        train_dir = file_dir[i]\n",
    "        label = train_dir.split('/')[-1].split('_')[1]\n",
    "        \n",
    "        # 학습에 사용할 video를 random으로 하나 가져오기\n",
    "        file_path = random.choice(\n",
    "            glob(train_dir + '/*')\n",
    "        )\n",
    "        \n",
    "        train_df.loc[len(train_df)] = [file_path, label] # 마지막 행으로 추가\n",
    "        \n",
    "    for i in range(20, 25):\n",
    "        valid_dir = file_dir[i]\n",
    "        label = valid_dir.split('/')[-1].split('_')[1]\n",
    "        \n",
    "        # 학습에 사용할 video를 random으로 하나 가져오기\n",
    "        file_path = random.choice(\n",
    "            glob(valid_dir + '/*')\n",
    "        )\n",
    "        \n",
    "        valid_df.loc[len(valid_df)] = [file_path, label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e4836",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_df), len(valid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309266d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e554b3",
   "metadata": {},
   "source": [
    "### 2.2 영상 frame에 label 붙여서 저장하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1919c8ec",
   "metadata": {},
   "source": [
    "image 저장 directory 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cc462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('UCF11_update_png')\n",
    "os.mkdir('UCF11_update_png/train')\n",
    "os.mkdir('UCF11_update_png/valid')\n",
    "\n",
    "train_df.to_csv('UCF11_train_video.csv', index=False)\n",
    "valid_df.to_csv('UCF11_valid_video.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b5508a",
   "metadata": {},
   "source": [
    "한 영상에서 제일 앞 10개의 frame으로 image training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7453c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frame = 10\n",
    "SAVE_DIR = 'UCF11_update_png/'\n",
    "\n",
    "for i, elem in train_df.iterrows():\n",
    "    cap = cv2.VideoCapture(elem['file_path'])\n",
    "    \n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (256, 256))\n",
    "        frames.append(frame)\n",
    "        \n",
    "        if len(frames) == max_frame:\n",
    "            break\n",
    "        \n",
    "    label = elem['label']\n",
    "    \n",
    "    for j, frame in enumerate(frames):\n",
    "        file_name = f'train/{label}_{i}_{j}.png'\n",
    "        cv2.imwrite(SAVE_DIR + file_name, frame)\n",
    "    \n",
    "    cap.release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab80e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(glob(SAVE_DIR + 'train/*')))  # 2200개 저장됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad25a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frame = 10\n",
    "SAVE_DIR = 'UCF11_update_png/'\n",
    "\n",
    "for i, elem in valid_df.iterrows():\n",
    "    cap = cv2.VideoCapture(elem['file_path'])\n",
    "    \n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (256, 256))\n",
    "        frames.append(frame)\n",
    "        \n",
    "        if len(frames) == max_frame:\n",
    "            break\n",
    "        \n",
    "    label = elem['label']\n",
    "    \n",
    "    for j, frame in enumerate(frames):\n",
    "        file_name = f'valid/{label}_{i}_{j}.png'\n",
    "        cv2.imwrite(SAVE_DIR + file_name, frame)\n",
    "    \n",
    "    cap.release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e602d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(glob(SAVE_DIR + 'valid/*')))  # 550개 저장됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dd0abe",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Model training - CNN approach\n",
    "\n",
    "- 11개의 class가 있으니 multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b668e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-class를 위해 label에 고유의 int 값 부여 \n",
    "LABEL_INT_DICT = np.unique(pd.read_csv('UCF11_train_video.csv')['label'])\n",
    "LABEL_STR_DICT = {k:v for v, k in enumerate(LABEL_INT_DICT)}\n",
    "pprint(LABEL_INT_DICT)\n",
    "pprint(LABEL_STR_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f33249",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, batch_size, image_size,\n",
    "                mode='train', shuffle=True):\n",
    "        assert mode in ['train', 'valid']\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            self.transform = A.Compose([\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.ShiftScaleRotate(\n",
    "                    p=0.5,\n",
    "                    shift_limit=0.05,\n",
    "                    scale_limit=0.05,\n",
    "                    rotate_limit=15\n",
    "                )\n",
    "            ])\n",
    "        \n",
    "        self.img_paths = glob(\n",
    "            f'UCF_updated_png/{mode}/*.png'        \n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.img_paths) / self.batch_size)  # epoch 당 step 개수\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        strt = idx * self.batch_size\n",
    "        fin = (idx + 1) * self.batch_size\n",
    "        data = self.img_paths[strt:fin]\n",
    "        \n",
    "        batch_x, batch_y = self.get_data(data)\n",
    "        \n",
    "        return np.array(batch_x), np.array(batch_y)\n",
    "    \n",
    "    def get_data(self, data): # 이미지 읽기\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        \n",
    "        for img_path in data:\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, (self.image_size, self.image_size))\n",
    "            \n",
    "            if self.mode == 'train':\n",
    "                augmented = self.transform(image=img)\n",
    "                img = augmented['image']\n",
    "                \n",
    "            img = img.astype('float32')\n",
    "            img = img / 255.\n",
    "            \n",
    "            label = img_path.split('/')[-1].split('_')[0]\n",
    "            \n",
    "            # sparse를 사용하니 따로 one-hot encoding은 필요없음\n",
    "            label = LABEL_STR_DICT[label]\n",
    "            \n",
    "            batch_x.append(img)\n",
    "            batch_y.append(label)\n",
    "        \n",
    "        return batch_x, batch_y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.img_paths)\n",
    "\n",
    "train_generator = DataGenerator(\n",
    "    mode='train',\n",
    "    batch_size=128,\n",
    "    image_size=256,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_generator = DataGenerator(\n",
    "    mode='valid',\n",
    "    batch_size=128,\n",
    "    image_size=256,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eeecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지와 label이 잘 matching 되어 구현되었는지 확인\n",
    "for batch in train_generator:\n",
    "    X, y = batch\n",
    "    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i in range(9):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(X[i])\n",
    "        plt.title(LABEL_INT_DICT(y[i]))\n",
    "        plt.axis('off')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e257ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    hub.KerasLayer(\n",
    "        'http://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2', # model import\n",
    "        trainable=True  # finetuning 하기 위함\n",
    "    ),\n",
    "    keras.layers.Dense(len(LABEL_INT_DICT), activation='softmax') # 11개 class\n",
    "])\n",
    "\n",
    "model.build([None, 256, 256, 3]) # input size\n",
    "adam = keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(\n",
    "    optimizer=adam,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics='accuracy'\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbc27fa",
   "metadata": {},
   "source": [
    "- weight 파일 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7186e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('UCF11_weights')\n",
    "\n",
    "filepath = 'UCF11_weights/{epoch:02d}-{val_accuracy:.2f}.hdf5'\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(  # model 구조 전체 저장 (weight 포함)\n",
    "    filepath,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose=1,\n",
    "    save_best_only=False # 성능이 update\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    callbacks=[model_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212faac2",
   "metadata": {},
   "source": [
    "- data가 적다보니 10 epoch에서 벌써 overfitting 발생\n",
    "- 조금 더 가벼운 model 사용해도 괜찮음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0025183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = history.history\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['loss'], label='train')\n",
    "plt.plot(history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['accuracy'], label='train')\n",
    "plt.plot(history['val_acc'], label='val')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb26cb7",
   "metadata": {},
   "source": [
    "- 위 모델은 이미지 하나하나에 대한 분류 성능\n",
    "- 동영상 하나를 넣었을 때 결과가 나오는 모델이 아님\n",
    "- 각각의 frame에 대한 예측값을 가져와서 평균을 내는 작업 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693808c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'UCF_valid_video.csv'\n",
    "valid_df = pd.read_csv(csv_path)  # 동영상 Link\n",
    "\n",
    "# 예측을 잘 했는지\n",
    "correct = 0\n",
    "\n",
    "for i, elem in tqdm(valid_df.iterrows(), total=len(valid_df)):\n",
    "    cap = cv2.VideoCapture(\n",
    "        elem['file_path']\n",
    "    )\n",
    "    \n",
    "    preds = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, (256, 256))\n",
    "        frame = frame.astype('float32')\n",
    "        frame /= 255.\n",
    "        \n",
    "        pred = model.predict(frame[np.newaxis, ...])\n",
    "        preds.append(pred)\n",
    "    \n",
    "    preds = np.array(preds).mean(axis=0) # 각 column 평균 계산\n",
    "    \n",
    "    label = LABEL_INT_DICT[np.argmax(preds)]    \n",
    "    if label == elem['label']:\n",
    "        correct += 1\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "# 동영상 predictio에 대한 accuracy\n",
    "# 지금은 single frame에 대한 성능과 크게 차이 나지는 않음\n",
    "print('Accuracy: ', correct / len(valid_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d079d62",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. 예측 결과 시각화 및 문제 해결방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22db970",
   "metadata": {},
   "source": [
    "### 4.1 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e38fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "model = keras.models.load_model(\n",
    "    'UCF11_weights/09-0.87.hdf5',\n",
    "    # keras 제공 외 layer도 사용해서 model 구현했기 때문에 아래 코드 필요\n",
    "    custom_objects={'KerasLayer': hub.KerasLayer}\n",
    ")\n",
    "\n",
    "model.build([None, 256, 256, 3])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7152bca",
   "metadata": {},
   "source": [
    "- 방법\n",
    "  - 21번째 영상에 대해 각 frame에 대한 prediction 생성\n",
    "  - prediction을 frame에 덮어 씌워서 다시 동영상으로 변환\n",
    "  - 'ori.avi' 파일로 저장\n",
    "  - 해당 영상 재생해 보고 예측 잘 됐는지 확인\n",
    "- 영상 보면 상단 prediction 결과 글씨가 잠깐 바뀌는 error 발생 이유\n",
    "  - frame 한 장 한 장 별로 예측하고, 이전과 이후 상황을 고려하지 않기 때문에, 해당 frame에서 예측 값이 틀린 것\n",
    "  - 이걸 flickering effect 라고 함\n",
    "  - ★ 해결방법 : rolling average 사용하기\n",
    "- rolling average\n",
    "  - 한 frame을 예측할 때 이전 frame들에 대한 예측 값도 모두 더해서 평균을 내는 것\n",
    "  - flickering effect를 막아줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7049111",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'UCF11_valid_video.csv'\n",
    "valid_df = pd.read_csv(csv_path)\n",
    "\n",
    "# 21번째 영상 사용\n",
    "idx = 21\n",
    "elem = valid_df.iloc[idx]\n",
    "\n",
    "cap = cv2.VideoCapture(elem['file_path'])\n",
    "\n",
    "# fourcc : encoder를 정하는 것\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "# writer = cv2.VideoWriter('ori.avi', fourcc, 30, (256, 256), True) # 파일명, encoding, fps, output image\n",
    "writer = cv2.VideoWriter('roll.avi', fourcc, 30, (256, 256), True) # rolling average 사용\n",
    "\n",
    "queue = [] ### 이전까지 prediction을 모두 가지는 list\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # 기존 frame은 유지하고 모델을 위한 전처리 frame_ 생성\n",
    "    # 모델에 필요한 image와 output image가 다르기 때문 (기존 frame 필요)\n",
    "    frame_ = frame.copy()\n",
    "    frame_ = cv2.cvtColor(frame_, cv2.COLOR_BGR2RGB)\n",
    "    frame_ = cv2.resize(frame_, (256, 256))\n",
    "    frame_ = frame_.astype('float32')\n",
    "    frame_ /= 255.\n",
    "\n",
    "    pred = model.predict(frame_[np.newaxis, ...])\n",
    "    # label = LABEL_INT_DICT[np.argmax(preds)]\n",
    "\n",
    "    ### rolling average\n",
    "    queue.append(pred)\n",
    "    results = np.array(queue).mean(axis=0)    \n",
    "    label = LABEL_INT_DICT[np.argmax(results)]\n",
    "    \n",
    "    # frame에 label 덮어쓰기\n",
    "    frame = cv2.resize(frame, (256, 256))\n",
    "    cv2.putText(\n",
    "        frame,\n",
    "        label,\n",
    "        (50, 45),  # label이 덮어 씌워질 좌표\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,  # font\n",
    "        1.5,  # font size\n",
    "        (0, 255, 0),  # font color\n",
    "        5  # font 굵기\n",
    "    )\n",
    "    writer.write(frame)    \n",
    "\n",
    "cap.relaese()\n",
    "writer.release()  # writer도 반드시 release 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86442f9",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Data preparation - RNN approach\n",
    "\n",
    "- 동영상은 frame의 연속인만큼 frame 순서에도 의미가 있는데 위와 같이 예측하면 그게 반영이 안됨\n",
    "- RNN\n",
    "  - frame 간 관계 학습, 동영상의 특징이 모델에 더 잘 반영됨\n",
    "  - 보통 CNN-RNN 모델로 많이 사용함\n",
    "    - CNN : 한 동영상이 들어오면 각 frame에 대한 feature 추출\n",
    "    - CNN에서 추출된 feature를 RNN에 input\n",
    "    - 이때 feature를 미리 따로 저장해 놓으면, RNN 모델을 학습할 때 매번 CNN 모델을 사용해서 feature를 extract 하지 않아도 되니 학습 속도가 매우 빨라짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e40ec3f",
   "metadata": {},
   "source": [
    "### 5.1 feature extract & save - CNN approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8323c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.model.load_model(\n",
    "    'UCF11_weights/10-0.87.hdf5',\n",
    "    custom_objects={'KerasLayer':hub.KerasLayer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1cc9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd2c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(256, 256, 3)), # input size\n",
    "        model.layers[0] # 첫 번째 layer 결과값 가져오기 (dense layer는 사용 X) : 1280\n",
    "    ],\n",
    "    name = 'feature_extractor'\n",
    ")\n",
    "\n",
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8401485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('UCF11_updated_npy')\n",
    "os.mkdir('UCF11_updated_npy/train')\n",
    "os.mkdir('UCF11_updated_npy/valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215e550e",
   "metadata": {},
   "source": [
    "- 10개의 frame만 사용해서 학습하고자 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a59ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames = 10\n",
    "SAVE_DIR = 'UCF11_updated_npy/'\n",
    "train_df = pd.read_csv('UCF11_train_video.csv')\n",
    "valid_df = pd.read_csv('UCF11_valid_video.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4c3b30",
   "metadata": {},
   "source": [
    "- train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39b15e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, elem in tqdm(train_df.iterrows(), total=len(train_df)):\n",
    "    \n",
    "    label = elem['label']\n",
    "    \n",
    "    cap = cv2.VideoCapture(\n",
    "        elem['file_path']\n",
    "    )\n",
    "    \n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "    frame_ = frame.copy()\n",
    "    frame_ = cv2.cvtColor(frame_, cv2.COLOR_BGR2RGB)\n",
    "    frame_ = cv2.resize(frame_, (256, 256))\n",
    "    frame_ = frame_.astype('float32')\n",
    "    frame_ /= 255.\n",
    "    \n",
    "    frames.append(frame_) # 앞의 10개의 frame을 순서대로 append\n",
    "    if len(frames) == max_frames:\n",
    "        break\n",
    "        \n",
    "    cap.release()\n",
    "    \n",
    "    # frame 하나 씩을 예측하려면 시간이 많이 걸림\n",
    "    # frame을 하나의 batch로 만들어서 한 번에 예측하면 속도 훨씬 빨라짐\n",
    "    # 순서대로 append 했으니 별도 코딩 없이 그대로 RNN에 넣어주면 됨\n",
    "    frames = np.array(frames)   # batch\n",
    "    features = feature_extractor.predict(frames)  # 한 번에 예측\n",
    "    \n",
    "    file_name = SAVE_DIR + f'train/{label}_{i}.npy'\n",
    "    np.save(file_name, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1325a894",
   "metadata": {},
   "source": [
    "- validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4646c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, elem in tqdm(valid_df.iterrows(), total=len(valid_df)):\n",
    "    \n",
    "    label = elem['label']\n",
    "    \n",
    "    cap = cv2.VideoCapture(\n",
    "        elem['file_path']\n",
    "    )\n",
    "    \n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "    frame_ = frame.copy()\n",
    "    frame_ = cv2.cvtColor(frame_, cv2.COLOR_BGR2RGB)\n",
    "    frame_ = cv2.resize(frame_, (256, 256))\n",
    "    frame_ = frame_.astype('float32')\n",
    "    frame_ /= 255.\n",
    "    \n",
    "    frames.append(frame_) # 앞의 10개의 frame을 순서대로 append\n",
    "    if len(frames) == max_frames:\n",
    "        break\n",
    "        \n",
    "    cap.release()\n",
    "    \n",
    "    # frame 하나 씩을 예측하려면 시간이 많이 걸림\n",
    "    # frame을 하나의 batch로 만들어서 한 번에 예측하면 속도 훨씬 빨라짐\n",
    "    # 순서대로 append 했으니 별도 코딩 없이 그대로 RNN에 넣어주면 됨\n",
    "    frames = np.array(frames)   # batch\n",
    "    features = feature_extractor.predict(frames)  # 한 번에 예측\n",
    "    \n",
    "    file_name = SAVE_DIR + f'valid/{label}_{i}.npy'\n",
    "    np.save(file_name, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de5bc89",
   "metadata": {},
   "source": [
    "- 제대로 됐는지 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f2a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(glob(SAVE_DIR + 'train/*''))) # 220개\n",
    "print(len(glob(SAVE_DIR + 'train/*''))) # 55개"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f3d17",
   "metadata": {},
   "source": [
    "### 5.2 간단한 RNN classification model 만들어 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4abc60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURE = 10 # 연속된 이미지가 들어오는데 총 몇 개의 이미지(feature)가 한 번에 들어오는지\n",
    "NUM_FEATURES = 1280 # 위에서 CNN 모델 결과 1280개 feature 있었음\n",
    "\n",
    "def build_model():\n",
    "    inputs = keras.Input((MAX_FRAMES, NUM_FEATURES))\n",
    "    # 64 : node 개수\n",
    "    # return_sequences=False\n",
    "    #  - 10개(max_feature)가 아니라 1개 값만 나옴\n",
    "    #  - LSTM layer를 쌓을 수 없게 됨\n",
    "    x = keras.layers.LSTM(64, return_sequences=True)(inputs)\n",
    "    x = keras.layers.LSTM(64, return_sequences=False)(x)\n",
    "    x = keras.layers.Dropout(0.3)(x)\n",
    "    outputs = keras.layers.Dense(len(LABEL_INT_DICT), activation='softmax')(x) # 11개 class 예측\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.0001)\n",
    "model = build_model()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=adam,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics='accuracy'\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20db95a",
   "metadata": {},
   "source": [
    "### 5.3 Data Generator 생성\n",
    "\n",
    "- image를 load 하는 게 아니라 numpy로 된 feature를 load 하는 거라 코드 일부 수정 필요\n",
    "  - image augmentation 사용 불가 -> 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f06e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, batch_size,\n",
    "                mode='train', shuffle=True):\n",
    "        assert mode in ['train', 'valid']\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.npy_paths = glob(\n",
    "            f'UCF_updated_npy/{mode}/*.npy'        \n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.npy_paths) / self.batch_size)  # epoch 당 step 개수\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        strt = idx * self.batch_size\n",
    "        fin = (idx + 1) * self.batch_size\n",
    "        data = self.npy_paths[strt:fin]\n",
    "        \n",
    "        batch_x, batch_y = self.get_data(data)\n",
    "        \n",
    "        return np.array(batch_x), np.array(batch_y)\n",
    "    \n",
    "    def get_data(self, data): # 이미지 읽기\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        \n",
    "        for npy_path in data:\n",
    "            \n",
    "            # CNN을 거쳐 모두 전처리를 했기 때문에 RNN은 매우 간단하게 구현 가능\n",
    "            npy = np.load(npy_path)\n",
    "            \n",
    "            label = npy_path.split('/')[-1].split('_')[0]\n",
    "            \n",
    "            # sparse를 사용하니 따로 one-hot encoding은 필요없음\n",
    "            label = LABEL_STR_DICT[label]\n",
    "            \n",
    "            batch_x.append(npy)\n",
    "            batch_y.append(label)\n",
    "        \n",
    "        return batch_x, batch_y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.npy_paths)\n",
    "\n",
    "train_generator = DataGenerator(\n",
    "    mode='train',\n",
    "    batch_size=128,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_generator = DataGenerator(\n",
    "    mode='valid',\n",
    "    batch_size=128,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7421bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_generator:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c562df86",
   "metadata": {},
   "source": [
    "RNN 모델이 가벼워서 금방 돔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4a61cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    epochs=30,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7060e471",
   "metadata": {},
   "source": [
    "### 결과\n",
    "- CNN보다 성능이 훨씬 올라감\n",
    "- task가 어려울수록 CNN과 함께 RNN 사용 필수\n",
    "\n",
    "### 모델 성능을 높이려면?\n",
    "- 지금 우리는 downsampling을 했었는데 더 많은 sample 사용하기\n",
    "- 10 frame 보다 더 많은 frame 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a81bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = history.history\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history['loss'], label='train')\n",
    "plt.plot(history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history['accuracy'], label='train')\n",
    "plt.plot(history['val_acc'], label='val')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba6c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defb1079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c7e20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b76b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9199b952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
