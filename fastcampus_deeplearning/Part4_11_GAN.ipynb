{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part4_11_GAN.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"mount_file_id":"1Yp0yH2RjfScp9VM5ATexx5ttY00Ns4cs","authorship_tag":"ABX9TyMprUtzifFZiBMZKiM/UAB2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# GAN\n","- 지금까지 모델들과는 다르게 optimizer를 두 개 정의해야 함\n","  - generator, discriminator 따로 두 개가 있고 따로 학습해야 하기 때문"],"metadata":{"id":"M-WPUCURrOHF"}},{"cell_type":"markdown","source":["- 현재는 MNIST를 이용했지만, CNN 이미지 활용하면 성능 더 높을 듯\n","- 세부 parameter도 조정하며 다양한 시도 해보기"],"metadata":{"id":"U-d28-x-1XUb"}},{"cell_type":"code","source":["!pwd\n","\n","# mount\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import sys\n","\n","drive_project_root = '/content/drive/MyDrive/#fastcampus'\n","sys.path.append(drive_project_root)\n","\n","!ls"],"metadata":{"id":"nlOJHTqlrXqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["!pip install -r '/content/drive/MyDrive/#fastcampus/requirements.txt'"],"metadata":{"id":"4zXqMRJx_4sI"}},{"cell_type":"code","source":["!pip install -r '/content/drive/MyDrive/#fastcampus/requirements.txt'"],"metadata":{"id":"bhw1vAW3zuBE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install torch-optimizer"],"metadata":{"id":"htTM48WtVYBZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["wandb 오류 있을 때 : `wandb.flush`"],"metadata":{"id":"mVflxznMvrp3"}},{"cell_type":"code","source":["pip install wandb"],"metadata":{"id":"PLYu27ZkVasl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install omegaconf"],"metadata":{"id":"p_8AzO0Zkrbf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install efficientnet_pytorch"],"metadata":{"id":"YdNWgLlmYbHX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["https://pytorch.org/vision/stable/transforms.html\n","\n","★ OmegaConf\n","- https://omegaconf.readthedocs.io/en/2.1_branch/\n","- hyperparameter configuration을 관리하기 위한 open source library\n","- DictConfig : Dictionary 형태의 configuration\n","- Hydra도 omegaconf를 기반으로 만들어짐\n","  - Hydra는 무겁기 때문에 omegaconf를 먼저 거치고, hydra 사용"],"metadata":{"id":"eX4npp3cxT4p"}},{"cell_type":"markdown","source":["gpu 확인"],"metadata":{"id":"qWQbtu32WqXA"}},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","print(gpu_info)"],"metadata":{"id":"QBp8qPTCWuFN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install Hydra"],"metadata":{"id":"755H-R1HlEZA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install hydra-core"],"metadata":{"id":"r79gbQX-mGkX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip uninstall torchmetrics"],"metadata":{"id":"liRxI5U_AMeC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install torchmetrics==0.5"],"metadata":{"id":"M3q3tz-EAMmT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install pytorch-lightning"],"metadata":{"id":"oK4SoCyolwaq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 추상화를 하는 class -> 중복 코드를 없애기 위함\n","from abc import abstractmethod, ABC\n","\n","from typing import Optional, Dict, List, Union, Any, Optional, Iterable, Callable\n","from functools import partial\n","from collections import Counter, OrderedDict\n","from datetime import datetime\n","import random\n","import math\n","\n","import numpy as np\n","from tqdm import tqdm   # 진행율\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F     # relu 등 함수 모음\n","from torch.utils.data import DataLoader\n","import pytorch_lightning as pl\n","\n","# data & models\n","from torchvision.datasets import FashionMNIST\n","from torchvision import transforms    # feature engineering 전처리 작업 효율적으로 할 수 있게 도와줌\n","import torchvision.utils as vutils   # for 이미지 저장\n","\n","# For configuration\n","from omegaconf import OmegaConf, DictConfig\n","import hydra\n","from hydra.core.config_store import ConfigStore\n","\n","# For logger\n","from torch.utils.tensorboard import SummaryWriter\n","import wandb\n","os.environ[\"WANDB_START_METHOD\"] = \"thread\""],"metadata":{"id":"WC1nxqJKs72F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["강사가 미리 만들어놓은 코드가 들어있는 파일들"],"metadata":{"id":"JBbfROJWyvbG"}},{"cell_type":"code","source":["from data_utils import dataset_split\n","from config_utils import flatten_dict\n","from config_utils import register_config\n","from config_utils import configure_optimizers_from_cfg\n","from config_utils import configure_optimizers_element\n","from config_utils import get_loggers\n","from config_utils import get_callbacks"],"metadata":{"id":"2PDva3Bvw9_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Base 모델 정의\n","\n","- Generative module 정의\n","- Discriminator module 정의\n","- VanillaGAN 정의"],"metadata":{"id":"8hve9NTN_OOT"}},{"cell_type":"code","source":["# Define Model\n","class BaseGenerativeModel(pl.LightningModule):\n","    def __init__(self, cfg: DictConfig):\n","        pl.LightningModule.__init__(self)\n","        self.cfg = cfg\n","    \n","    @abstractmethod\n","    def forward(self, x):\n","        raise NotImplementedError()\n","\n","    def sample_generate(self, *args, **kwargs):\n","        raise NotImplementedError()\n","    \n","    def loss_function(self, *args, **kwargs):\n","        pass\n","\n","    def configure_optimizers(self):\n","        self._optimizers, self._schedulers = configure_optimizers_from_cfg(\n","            self.cfg, self\n","        )\n","        return self._optimizers, self._schedulers\n","\n","    def training_step(self, batch, batch_idx):\n","        pass\n","    \n","    def validation_step(self, batch, batch_idx):\n","        pass\n","\n","class Generator(nn.Module):\n","    def __init__(self, cfg: DictConfig):\n","        pl.LightningModule.__init__()\n","        self.cfg = cfg\n","        self.latent_dim = cfg.model.latent_dim\n","\n","        # mlp 사용\n","        def mlp_module(\n","            in_feat: int,   # input feature\n","            out_feat:int,   # output feature\n","            normalize: bool = True,\n","            activation: str = \"\"\n","        ):\n","            layers = [nn.Linear(in_feat, out_feat)]\n","            if normalize:\n","                layers.append(nn.BatchNorm1d(out_feat))\n","            # 최대한 pytorch에 있는 거로 class name 적음\n","            if activation == \"LeakyReLU\":\n","                layers.append(nn.LeakyReLU(inplace=True))\n","            elif activation == \"Tanh\":\n","                layers.append(nn.Tanh())\n","            elif activation == \"Sigmoid\":\n","                layers.append(nn.Sigmoid())\n","            else:\n","                raise NotImplementedError()\n","            return layers\n","        \n","        layers: List[nn.Module] = []\n","        for modules_cfg in cfg.model.generator.mlp_modules:\n","            layers.extend(mlp_module(**modules_cfg))   # dictionary 형태 받아오기\n","        self.model = nn.Sequential(*layers)\n","    \n","    def forward(self, z):\n","        img = self.model(z)\n","        img = img.view(\n","            img.size(0),\n","            self.cfg.data.C,\n","            self.cfg.data.H,\n","            self.cfg.data.W,\n","        )\n","        return img\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, cfg: DictConfig):\n","        pl.LightningModule.__init__()\n","        self.cfg = cfg\n","\n","        def mlp_module(\n","            in_feat: int,   # input feature\n","            out_feat:int,   # output feature\n","            normalize: bool = True,\n","            activation: str = \"\"\n","        ):\n","            layers = [nn.Linear(in_feat, out_feat)]\n","            if normalize:\n","                layers.append(nn.BatchNorm1d(out_feat))\n","            # 최대한 pytorch에 있는 거로 class name 적음\n","            if activation == \"LeakyReLU\":\n","                layers.append(nn.LeakyReLU(inplace=True))\n","            elif activation == \"Tanh\":\n","                layers.append(nn.Tanh())\n","            elif activation == \"Sigmoid\":\n","                layers.append(nn.Sigmoid())\n","            else:\n","                raise NotImplementedError()\n","            return layers\n","\n","        layers: List[nn.Module] = []\n","        for modules_cfg in cfg.model.discriminator.mlp_modules:\n","            layers.extend(mlp_module(**modules_cfg))   # dictionary 형태 받아오기\n","        self.model = nn.Sequential(*layers)\n","    \n","    def forward(self, z):\n","        img_flattened = img.view(img.size(0), -1)\n","        validity = self.model(img_flattened)\n","        return validity    # 내가 맞았는지 fake인지 진짜인지 예측 : validity (0, 1)\n","    \n","class VanillaGAN(BaseGenerativeModule):\n","    def __init__(self, cfg: DictConfig):\n","        super.__init__(cfg)\n","        self.latent_dim = cfg.model.latent_dim\n","\n","        self.generator = Generator(cfg)\n","        self.discriminator = Discriminator(cfg)\n","\n","    def forward(self, z):\n","        return self.generator(z)\n","\n","    # y_hat : from discriminator 가짜인지 진짜인지 판단\n","    # y : label for discriminator 실제 값\n","    def loss_function(self, y_hat, y):\n","        # adversarial loss\n","        return F.binary_cross_entropy(y_hat, y)\n","    \n","    def configure_optimizer(self):\n","        # g : generative model\n","        self._opt_g, self._scheduler_g = configure_optimizer_element(\n","            self.cfg.opt.generator.optimizer,\n","            self.cfg.opt.generator.lr_scheduler,\n","            self.generator  # generator에 대한 parameter만 update\n","        )\n","\n","        self._opt_d, self._scheduler_d = configure_optimizer_element(\n","            self.cfg.opt.discriminator.optimizer,\n","            self.cfg.opt.discriminator.lr_scheduler,\n","            self.discriminator  # generator에 대한 parameter만 update\n","        )\n","\n","        # https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.core.LightningModule.html?highlight=lightningmodule\n","        # 검색 : configure_optimizers\n","        # GAN은 지금까지 쓴 것보다 진보된 형태로 넣어야 함\n","        # dictionary/list 형태로 return\n","        optimizers = [self._opt_d, self._opt_g]\n","        schedulers = [self._scheduler_d, self._scheduler_g]\n","        schedulers = [sch for sch in schedulers if sch is not None] # None인 것 포함하면 error 발생하기 때문에 거르기\n","        return optimizers, schedulers\n","\n","    # optimizer_idx를 넣는 이유 : generative, discriminator에 따라 loss를 따로 따로 구하기 위해서\n","    def _step(self, batch, batch_idx, optimizer_idx, mode=\"train\"):\n","        assert mode in [\"train\", \"val\", \"test\"]\n","\n","        # conditional GAN이 아니니 label은 미사용\n","        imgs, _ = batch\n","\n","        # sample noise\n","        z = torch.randn(imgs.shape[0], self.cfg.model.latent_dim)\n","        z = z.type_as(imgs)\n","\n","        # ground truth_result\n","        valid = torch.ones(imgs.size(0), 1) # True image\n","        valid = valid.type_as(imgs).to(self.device)   # to(self.device) GPU에 태우기 -> 없으면 CPU 때는 문제 없지만 GPU에서 돌리면 에러 발생할 수 있음\n","\n","        # discriminator\n","        if optimizer_idx == 0:\n","\n","            # fake loss\n","            fake = torch.zeros(img_size(0), 1)\n","            fake = fake.type_as(imgs).to(self.device)\n","\n","            fake_loss = self.loss_function(\n","                self.discriminator(self(z)), fake\n","            )\n","            \n","            # true loss\n","            real_loss = self.loss_function(\n","                self.discriminator(imgs), valid\n","            )\n","\n","            # discriminator loss\n","            d_loss = (real_loss + fake_loss) / 2\n","\n","             outputs = {\n","                # 나중에 편하게 하기 위해 그냥 loss와 discriminator loss를 구분\n","                f\"{mode}_loss\": d_loss,\n","                f\"{mode}_d_loss\": d_loss,\n","                f\"{mode}_real_loss\": real_loss,\n","                f\"{mode}_fake_loss\": fake_loss,\n","            }\n","            \n","            if mode == \"train\":\n","                outputs[\"loss\"] = d_loss\n","\n","            self.log_dict(outputs)\n","\n","        # generator\n","        if optimizer_idx == 1:\n","            self.generated_imgs = self(z)  # Fake image\n","\n","            # generative loss\n","            g_loss = self.loss_function(\n","                self.discriminator(self(z)) , valid   # discriminator의 예측과 valid 비교\n","            )\n","\n","            outputs = {\n","                # 나중에 편하게 하기 위해 그냥 loss와 generative loss를 구분\n","                f\"{mode}_loss\": g_loss,\n","                f\"{mode}_g_loss\": g_loss,\n","            }\n","\n","            if mode == \"train\":\n","                outputs[\"loss\"] = g_loss   # 이렇게 해야 실제 optimizer에 반영됨\n","            else:   # valid, test\n","                sample_imgs = self.generated_imgs\n","                grid_samples = vutils.make_grid(sample_imgs)\n","                grid_imgs = vutils.make_grid(imgs)\n","\n","                # wandb 사용하는 경우\n","                self.logger.experiment[0].log(\n","                    {\n","                        # grid로 그렸던 거를 보기 좋게 한 판에 합치기\n","                        f\"{mode}_generated_images\": wandb.Image(grid_samples),\n","                        f\"{mode}_orig_images\": wandb.Image(grid_imgs),\n","                    }\n","                )\n","\n","                # tensorboard 사용하는 경우\n","                self.logger.experiment[1].add_images(\n","                    {\n","                        # grid로 그렸던 거를 보기 좋게 한 판에 합치기\n","                        f\"{mode}_generated_images\",\n","                        sample_imgs,\n","                        0,\n","                    }\n","                )\n","\n","            self.log_dict(outputs)\n","\n","        return outputs\n","    \n","    def training_step(self, batch, batch_idx, optimizer_idx):\n","        return self._step(\n","            batch, batch_idx, optimizer_idx, mode=\"train\"\n","        )\n","    \n","    def validation_step(self, batch, batch_idx):\n","        self._step(\n","            batch, batch_idx, 0, mode=\"val\"\n","        )\n","        self._step(\n","            batch, batch_idx, 1, mode=\"val\"\n","        )\n","\n","    def test_step(self, batch, batch_idx):\n","        self._step(\n","            batch, batch_idx, 0, mode=\"test\"\n","        )\n","        self._step(\n","            batch, batch_idx, 1, mode=\"test\"\n","        )\n","\n","    def _on_epoch_end(self, mode):\n","        # random noise test\n","        assert mode in [\"train\", \"val\", \"test\"]\n","        self.validation_z = torch.randn(\n","            cfg.train.test_batch_size,\n","            self.latent_dim\n","        )\n","        z = self.validation_z.type_as(self.generator.model[0].weight)\n","        \n","        # log sampled images\n","        sample_imgs = self(z)\n","\n","        grid_samples = vutils.make_grid(sample_imgs)\n","\n","        # wandb 사용하는 경우\n","        self.logger.experiment[0].log(\n","            {\n","                # grid로 그렸던 거를 보기 좋게 한 판에 합치기\n","                # 이름 위에서랑 다르게 써 줘야 함 (안 그러면 덮어쓰기 됨)\n","                f\"{mode}_generated_images_from_random_sampling\": wandb.Image(grid_samples),\n","            }\n","        )\n","    \n","    def on_train_epoch_end(self, unused: Optional=None):\n","        self._on_epoch_end(\"train\")\n","\n","    def on_validation_epoch_end(self):\n","        self._on_epoch_end(\"val\")\n","\n","    def on_test_epoch_end(self):\n","        self._on_epoch_end(\"test\")\n"],"metadata":{"id":"U6JdjBPk_ORe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class VanillaVAE(BaseGenerativeModel):\n","    def __init__(self, cfg: DictConfig):\n","        super.__init__(cfg)\n","        self.latent_dim = cfg.model.latent_dim\n","\n","        # mlp를 이용할 예정\n","        # mlp_module 부분을 꼭 mlp가 아니라 cnn 등 다른 거 사용하면서 실험해 볼 것\n","\n","        # define posterior (encoder) modules\n","        posterior_mlp_modules_list = []\n","        prev_dim = cfg.model.posterior.hidden_dims[0]\n","\n","        for h_dim in cfg.model.posterior.hidden_dims[1:]:\n","            posterior_mlp_modules_list.append(nn.Linear(prev_dim, h_dim))\n","            prev_dim = h_dim\n","\n","        # * : argument 하나씩 풀어서 입력\n","        self.posterior_mlp_modules = nn.Sequential(\n","            *posterior_mlp_modules_list\n","        )\n","\n","        # define latent encode\n","        self.posterior_mu = nn.Linear(\n","            cfg.model.posteriror.hidden_dims[-1], cfg.model.latent_dim\n","        )\n","\n","        self.posteriorlog_log_var = nn.Linear(\n","            cfg.model.posterior.hidden_dims[-1], cfg.model.latent_dim\n","        )\n","\n","        # define prior (decoder) modules\n","        prior_mlp_modules_list = []\n","        prev_dim = self.latent_dim\n","        for h_dim in cfg.model.prior.hidden_dims:\n","            prior_mlp_modules_list.append(nn.Linear(prev_dim, h_dim))\n","            prev_dim = h_dim\n","        self.prior_mlp_modules = nn.Sequential(*prior_mlp_modules_list)\n","\n","    def encode(self, input):\n","        hidden = self.posterior_mlp_modules(input)\n","        mu = self.posterior_mu(hidden)\n","        log_var = self.posterior_log_var(hidden)\n","        return mu, log_var\n","\n","    def reparameterize(self, mu, log_var):\n","        std = torch.exp(0.5 * log_var)\n","        epsilon = torch.randn_like(std)\n","        return mu + (epsilon * std)\n","\n","    def decode(self, z):\n","        result = self.prior_mlp_modules(z)\n","        return torch.sigmoid(result)\n","\n","    def forward(self, x):\n","        # 아직 코드 안 짰을 때 일단 이렇게 해 놓기\n","        # raise NotImplementedError()\n","\n","        # return reconstruction, mu, log_variance\n","        # -> 결과적으로 reconstruction, latent variable을 return 해야 함\n","\n","        # input : [batch, 1, 28, 28]\n","        input = x.view(-1, self.cfg.model.posterior.hidden_dims[0])\n","        mu, log_var = self.encode(input)\n","        z = self.reparameterize(mu, log_var)\n","        return self.decode(z), mu, log_var\n","\n","    def sample_generate(\n","        self,\n","        num_samples: int = 64,  # for training (*batch_size) or random sampling\n","        z: Optional[torch.Tensor] = None,  # for manual sample generation\n","        ):\n","            # raise NotImplementedError()\n","            if z is None:\n","                z = torch.randn(num_samples, self.latent_dim)  # num_samples : batch size\n","            else:\n","                num_samples = z.shape[0]\n","            assert z.shape[-1] == self.latent_dim\n","            z = z.to(self.device)     # GPU inference하면 어떻게 될 지 모르니 일단 device 태워 놓기\n","            samples = self.decode(z) \n","            return samples.view(num_samples, self.cfg.data.C, self.cfg.data.H, self.cfg.data.W)   # 이미 flatten 되어 있기 때문에 -1 할 필요 없음\n","    \n","    def loss_function(\n","        self, \n","        recons, \n","        real_img, \n","        mu, \n","        log_var, \n","        kld_weight, \n","        mode=\"train\"\n","        ) -> dict:  # dictionary 형태로 정의 : 여러 개 loss를 iter할 것이기 때문\n","            assert mode in [\"train\", \"val\", \"test\"]\n","            \n","            # reconstruction loss\n","            recons_loss = F.binary_cross_entropy(\n","                recons,                  # input  : mlp 모델은 항상 이 부분이 flatten 되어 있는 상태일 것임\n","                real_img.view(-1, self.cfg.model.prior.hidden_dims[-1]),    # target : 실제 정답 값 -> 따라서 얘도 flatten 해 줘야 함 (view(-1, ))\n","                reduction=\"sum\"\n","            )\n","\n","            # kld loss\n","            kld_loss = 0.5 * torch.sum(\n","                mu.pow(2) + log_var.exp() - log_var - 1\n","                )\n","            \n","            # summation\n","            loss = recons_loss + kld_weight * kld_loss\n","            loss_result = {\n","                # loss를 그냥 가져다 쓰면 너무 커서 시스템이 폭발하게 됨\n","                # 따라서 특히 generative 모델에서 VAE 생성할 때 전체 학습 데이터만큼 나눠주는 게 일반적\n","                f\"{mode}_loss\": loss / self.cfg.data[f\"num_{mode}_imgs\"],\n","                # reconstruction loss\n","                f\"{mode}_reconstruction_loss\": recons_loss / self.cfg.data[f\"num_{mode}_imgs\"],\n","                # kld loss\n","                f\"{mode}_kld_loss\": kld_loss / self.cfg.data[f\"num_{mode}_imgs\"],\n","            }\n","\n","            return loss_result\n","    \n","    def training_step(self, batch, batch_idx):\n","        real_img, labels = batch\n","        recons, mu, log_var = self.forward(real_img)\n","        loss_results = self.loss_function(\n","            recons,\n","            real_img,\n","            mu,\n","            log_var,\n","            kld_weight=self.cfg.model.kld_weight,\n","            mode=\"train\"\n","        )\n","\n","        loss_results[\"loss\"] = loss_results[\"train_loss\"]\n","        self.log_dict(loss_results)\n","        return loss_results\n","    \n","    def validation_step(self, batch, batch_idx):\n","        real_img, labels = batch\n","        recons, mu, log_var = self.forward(real_img)\n","        loss_results = self.loss_function(\n","            recons,\n","            real_img,\n","            mu,\n","            log_var,\n","            kld_weight=self.model.kld_weight,\n","            mode=\"val\"\n","        )\n","\n","        # loss_results[\"loss\"] = loss_results[\"val_loss\"]\n","        self.log_dict(loss_results)\n","\n","        # random sample_generate\n","        # 기존에 있는 test data가 아니라 random sample에서 생성하는 것\n","        sample_gens = self.sample_generate(real_img.shape[0])  # batch size 만큼 넣음\n","\n","        # image logging은 tensorboard, wandb 따로 해야 함\n","        # 여기에서는 wandb에 맞게 코딩할 예정\n","        # logger에 direct로 접근해서 logging할 예정 : logger.experiment\n","        # - wandb.logger, tensorboard summary writer와 같은 의미\n","        # - 두 개 이상을 동시에 하고 있으면 list 형태로 가져옴\n","        # - 0번째를 wandb라고 가정\n","        self.logger.experiment[0].log({\n","            \"inputs\": wandb.Image(real_img),\n","            # reconstruct는 flatten 된 형태 -> view(-1) 필요\n","            # -1 다음에는 channel, height, width 입력\n","            \"recons\": wandb.Image(recons.view(-1, self.cfg.data.C, self.cfg.data.H, self.cfg.data.W)),\n","            \"sample_gens\": wandb.Image(sample_gens), # sample generator\n","        })\n","\n","        return loss_results"],"metadata":{"id":"kUZ38HCt_gR7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Configuration Def"],"metadata":{"id":"-euIEprEIddh"}},{"cell_type":"code","source":["# data configs\n","data_fashion_mnist_cfg = {\n","    \"name\": \"fahion_mnist\",\n","    \"data_root\": os.path.join(os.getcwd(), \"data\"), # data 저장할 곳\n","    \"transforms\": [\n","        {\n","            \"name\": \"ToTensor\",\n","            # callable한 object를 keyword argument로 받기\n","            \"kwargs\": {}\n","        }\n","    ],\n","    \"W\": 28,\n","    \"H\": 28,\n","    \"C\": 1,  # 흑백\n","    \"n_class\": 10,\n","}\n","cfg = OmegaConf.create(data_fashion_mnist_cfg)\n","print(OmegaConf.to_yaml(cfg))\n","\n","# model configs\n","model_mnist_vanilla_vae_cfg = {\n","    \"name\": \"VanillaVAE\",\n","    \"latent_dim\": 4,\n","    # encoder\n","    \"posterior\": {\n","        \"hidden_dim\": [28*28, 512, 256]\n","    },\n","    # decoder\n","    \"prior\": {\n","        \"hidden_dim\": [256, 512, 28*28]\n","    },\n","    \"kld_weight\": 1,\n","}\n","\n","model_mnist_vanilla_gan_cfg = {\n","    \"name\": \"VanillaVAE\",\n","    \"latent_dim\": 128,\n","    \"generator\": {\n","        \"mlp_modules\": [\n","            {\n","                \"in_feat\": 128,       # latent_dim이 128이니까 여기도 128이어야 함\n","                \"out_feat\": 256,\n","                \"normalize\": False,\n","                \"activation\": \"LeakyReLU\"\n","            },\n","            {\n","                \"in_feat\": 256,       # latent_dim이 128이니까 여기도 128이어야 함\n","                \"out_feat\": 512,\n","                \"normalize\": True,\n","                \"activation\": \"LeakyReLU\"\n","            },\n","            {\n","                \"in_feat\": 512,       # latent_dim이 128이니까 여기도 128이어야 함\n","                \"out_feat\": 1024,\n","                \"normalize\": True,\n","                \"activation\": \"LeakyReLU\"\n","            },            \n","            {\n","                \"in_feat\": 1024,       # latent_dim이 128이니까 여기도 128이어야 함\n","                \"out_feat\": 1*28*28,   # C × H × W (image size)\n","                \"normalize\": False,\n","                \"activation\": \"Tanh\"   # -1~1 사이 return\n","            },                                    \n","        ]\n","    },\n","    \"discriminator\": {\n","        \"mlp_modules\": [\n","            {\n","                \"in_feat\": 1*28*28,   # C × H × W (image size)\n","                \"out_feat\": 512,\n","                \"normalize\": True,\n","                \"activation\": \"LeakyReLU\"\n","            },\n","            {\n","                \"in_feat\": 512,       # latent_dim이 128이니까 여기도 128이어야 함\n","                \"out_feat\": 256,\n","                \"normalize\": True,\n","                \"activation\": \"LeakyReLU\"\n","            },\n","            {\n","                \"in_feat\": 256,       # latent_dim이 128이니까 여기도 128이어야 함\n","                \"out_feat\": 1,        # layer 더 쌓아도 되지만 좀 비대칭적으로 만들어 봄\n","                \"normalize\": False,\n","                \"activation\": \"Sigmoid\"  # 확률값\n","            }                       \n","        ]           \n","    },\n","    \"kld_weight\": 1,\n","}\n","\n","# optimizer configs\n","# Google \"torch optimizer\" : https://github.com/jettify/pytorch-optimizer\n","#    RAdam : https://github.com/jettify/pytorch-optimizer/blob/master/torch_optimizer/radam.py\n","#    -> 여기 def _init_() 부분 참고해서 작성\n","# config_utils.py 파일 보면 optimizer를 for loop으로 돌림\n","#   = optimizers를 list 형태로 여러 개 지정 가능\n","#   fine tuning, 모델 두 개 동시 학습시킬 때 등에 사용\n","opt_cfg = {\n","    \"optimizers\": [\n","        {\"name\": \"RAdam\",\n","         \"kwargs\": {\n","             \"lr\": 1e-3,\n","             \"betas\": (0.9, 0.999),\n","             \"eps\": 1e-8,\n","             \"weight_decay\": 0,\n","             }\n","         }\n","    ],\n","    \"lr_schedulers\": [\n","        {\"name\": None,  # lr_scheduler 안 쓰려면 none이라고 지정 (config_utils.py 참고)\n","         \"kwargs\": {}\n","         }\n","    ]\n","}\n","\n","# discriminator 성능을 점점 약화시키면 전체 모델 성능은 높아질 수 있음\n","gan_opt_cfg = {\n","    \"discriminator\": {\n","        \"optimizer\" :{\n","            \"name\": \"RAdam\",\n","            \"kwargs\": {\n","                \"lr\": 0.0002,\n","            }\n","        },\n","        \"lr_scheduler\": {\n","            \"name\": None,\n","        }\n","    }, \n","    \"generator\": {\n","        \"optimizer\": {\n","\n","        }\n","    }\n","\n","}\n","\n","_merged_cfg_presets = {\n","    \"vanilla_vae_fashion_mnist\": {\n","        \"data\": data_fashion_mnist_cfg,\n","        \"model\": model_mnist_vanilla_vae_cfg,\n","        \"opt\": opt_cfg,   # optimizer\n","    },\n","    \"vanilla_gan_fashion_mnist\": {\n","        \"data\": data_fashion_mnist_cfg,\n","        \"model\": model_mnist_vanilla_gan_cfg,\n","        \"opt\": gan_opt_cfg,   # optimizer\n","    },\n","}\n"," \n","### hydra composition ###\n","\n","# clear hydra instance\n","# notebook에서 여러 번 실행하면 이상하게 나올 수 있어서 clear 먼저 해줄 것\n","# notebook 아니면 이렇게 할 필요 없음\n","hydra.core.global_hydra.GlobalHydra.instance().clear()  \n","\n","# register preset configs\n","register_config(_merged_cfg_presets)\n","\n","# initializing\n","hydra.initialize(config_path=None)\n","\n","# compose\n","# cfg = hydra.compose(\"vanilla_vae_fashion_mnist\")  # 위에서 사용한 _merged_cfg_presets 키 가져오기\n","cfg = hydra.compose(\"vanilla_gan_fashion_mnist\") \n","\n","####\n","\n","# override some cfg\n","run_name = f\"{datetime.now().isoformat(timespec='seconds')}-{cfg.model.name}-{cfg.data.name}\"\n","print(OmegaConf.to_yaml(cfg))\n","\n","## Define train configs\n","project_root_dir = os.path.join(\n","    drive_project_root, \"runs\", \"generative-dnn-tutorial_fashion_mnist_runs\"  # 현재 우리가 작업 중인 곳\n",")\n","\n","save_dir = os.path.join(project_root_dir, run_name)\n","run_root_dir = os.path.join(project_root_dir, run_name)\n","\n","# train configs\n","train_cfg = {\n","    \"train_batch_size\": 256,\n","    \"val_batch_size\": 64,\n","    \"test_batch_size\": 64,\n","    \"train_val_split\": [0.9, 0.1],\n","    \"run_root_dir\": run_root_dir,\n","    \"trainer_kwargs\": {\n","        \"accelerator\": \"dp\", # 하나의 gpu로 할 때는 dp로 하지만 multiple gpu인 경우 ddp 등 설정 가능\n","        \"gpus\": \"0\"  # -> 0번 gpu 사용하기\n","        # \"gpus\": None, # -> 일단 CPU로 확인해 보겠다 : CPU로 할 때 에러에 대한 설명이 더 친절함 (특히 pytorch)\n","        \"max_epochs\": 50,    \n","        # 1.0 : train epoch가 끝날 때 validation check을 하겠다\n","        # 0.5 : train epoch가 절반 돌았을 때 validation check 하겠다\n","        # integer인 경우 : 몇 step마다 돌 지 설정하는 것\n","        \"val_check_interval\": 1.0,\n","        \"log_every_n_steps\": 100,   # 100번 step마다 한다\n","        \"flush_logs_every_n_steps\": 100,\n","    },    \n","}\n","\n","\n","# logger configs\n","log_cfg = {\n","    \"loggers\": {\n","        \"WandbLogger\": { \n","            \"project\": \"fastcampus_generative_fashion_mnist_tutorials\",\n","            \"name\": run_name,\n","            \"tags\": [\"fastcampus_generative_fashion_mnist_tutorials\"],\n","            \"save_dir\": run_root_dir,\n","        },\n","        \"TensorBoardLogger\": {\n","            \"save_dir\": project_root_dir,\n","            \"name\": run_name,\n","            },\n","    },\n","    \"callbacks\": {\n","        \"ModelCheckpoint\": {\n","            \"save_top_k\": 3,\n","            \"monitor\": \"val_loss\",\n","            \"mode\": \"min\",\n","            \"verbose\": True,\n","            \"dirpath\": os.path.join(run_root_dir, \"weights\"),\n","            \"filename\": \"{epoch}-{val_loss:.3f}\"  # VAE는 accuracy가 없음\n","            },\n","        \"EarlyStopping\": {\n","            \"monitor\": \"val_loss\",\n","            \"mode\": \"min\",\n","            # 실제 generation 문제는 classification보다 훨씬 불안정한 경우가 많아서\n","            # patience 값을 10 정도로 더 크게 두거나\n","            # 아예 없애고 regularize를 추가하기도 함\n","            \"patience\": 3,\n","            \"verbose\": True,\n","            }\n","    }\n","}\n","\n","# unlock config & set train, log cfg\n","# 이 코드 없으면 에러 발생 : Key 'train' is not in struct\n","OmegaConf.set_struct(cfg, False)\n","\n","cfg.train = train_cfg\n","cfg.log = log_cfg\n","\n","# lock config\n","# OmegaConf.set_struct(cfg, True)\n","print(OmegaConf.to_yaml(cfg))"],"metadata":{"id":"T0sjg1W0IRSA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data & dataloader"],"metadata":{"id":"GsSyBeC_hZ2s"}},{"cell_type":"code","source":["# get transforms from torch.vision\n","def get_transform(cfg: DictConfig):\n","    transforms_list = []\n","    for tfm in cfg.data.transforms:\n","        if hasattr(transforms, tfm.name):\n","            # getattr(transforms, tfm.name)\n","            # torch vision에서 transforms의 이름이 tfm.name과 같으면 transforms에 있는 함수를 가져오라는 뜻\n","            transforms_list.append(\n","                getattr(transforms, tfm.name)(**tfm.kwargs)\n","            )\n","        else:\n","            raise ValueError(\n","                f\"Not supported transform {tfm} in torch.vision.transform\"\n","            )\n","\n","    return transforms.Compose(transforms_list)\n","\n","transform = get_transform(cfg)\n","\n","def get_datasets(cfg: DictConfig, download: bool = True) -> Dict[str, torch.utils.data.Dataset]:\n","    data_root = cfg.data.data_root\n","    datasets = {}\n","\n","    if cfg.data.name == \"fashion_mnist\":\n","        fashion_mnist_dataset = FashionMNIST(data_root, download=download, train=True, transform=transform)\n","        datasets = dataset_split(fashion_mnist_dataset, split=cfg.train.train_val_split)  # dictionary  형태\n","        datasets[\"test\"] = FashionMNIST(data_root, download=download, train=False, transform=transforms.ToTensor())    \n","    else:\n","        raise NotImplementedError(\"Not supported dataset yet\")\n","    return datasets\n","\n","datasets = get_datasets(cfg, download=True)  # 얼마 안 크니 download는 True로 함\n","\n","train_dataset = datasets[\"train\"]\n","val_dataset = datasets[\"val\"]\n","test_dataset = datasets[\"test\"]\n","\n","# save dataset size\n","cfg.data.num_train_imgs = len(datasets[\"train\"])\n","cfg.data.num_val_imgs = len(datasets[\"val\"])\n","cfg.data.num_test_imgs = len(datasets[\"test\"])\n","\n","# define dataloader\n","# batch 단위로 데이터를 묶을 예정\n","train_batch_size = cfg.train.train_batch_size\n","val_batch_size = cfg.train.val_batch_size\n","test_batch_size = cfg.train.test_batch_size\n","\n","# num_workers : 병렬 processing \n","# 1로 설정하는 경우 노트북 환경에서 dp, ddp의 pytorch가 에러나는 경우가 있어 일단 0으로 설정함\n","train_dataloader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=0\n",")\n","\n","val_dataloader = torch.utils.data.DataLoader(\n","    val_dataset, batch_size=val_batch_size, shuffle=False, num_workers=0\n",")\n","\n","test_dataloader = torch.utils.data.DataLoader(\n","    test_dataset, batch_size=val_batch_size, shuffle=False, num_workers=0\n",")"],"metadata":{"id":"Qhliz2d3hZ5j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 모델 선언 및 손실 함수, 최적화(optimizer) 정의, Tensorboard Logger 정의\n","\n","- 아래 셀 값들 조절해서 다른 모델 만들어 볼 수 있음"],"metadata":{"id":"F90LNgLP2ta5"}},{"cell_type":"markdown","source":["### define model"],"metadata":{"id":"16u_coLyhzc8"}},{"cell_type":"code","source":["# model define\n","\n","def get_pl_model(cfg: DictConfig, checkpoint_path: Optional[str] = None):\n","\n","    if cfg.model.name == 'VanillaVAE':\n","        model = VanillaVAE(cfg)\n","    elif cfg.model.name == 'VanillaGAN':\n","        model = VanillaGAN(cfg)\n","    else:\n","        raise NotImplementedError()\n","    \n","    if checkpoint_path is not None:\n","        model = model.load_from_checkpoint(cfg=cfg, checkpoint_path=checkpoint_path)\n","    return model\n","\n","model = get_pl_model(cfg)\n","# print(model)"],"metadata":{"id":"Pb43JfUSoz9I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### GPU setting"],"metadata":{"id":"TjwwBXD-uAFB"}},{"cell_type":"code","source":["# # gpu = None  # 코드 에러 날 때 cpu는 잘 작동하는지 확인하기 위해 gpu=None으로 설정\n","# gpu = 0   # gpu를 0번 쓰겠다는 의미"],"metadata":{"id":"2LIELBSBuBJ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logger = get_loggers(cfg)\n","callbacks = get_callbacks(cfg)\n","\n","trainer = pl.Trainer(\n","    callbacks=callbacks,\n","    logger=logger,\n","    default_root_dir=cfg.train.run_root_dir,\n","    num_sanity_val_steps=2,\n","    **cfg.train.trainer_kwargs\n",")"],"metadata":{"id":"nItlsYuDqhDs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%load_ext tensorboard\n","%tensorboard --logdir /content/drive/MyDrive/\\#fastcampus/runs/dnn-tutorial-fashion-mnist-run\n","\n","# pytorch lightning으로 학습\n","trainer.fit(model, train_dataloader, val_dataloader)"],"metadata":{"id":"TpO0z8mMrHrh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test"],"metadata":{"id":"53m-A6t2t5ZP"}},{"cell_type":"code","source":["ckpt_path = os.path.join(\n","    cfg.log.callbacks.ModelCheckpoint.dirpath,\n","    \"epoch-=8-val_loss=2.999.ckpt\"  # 파일 이름 그대로 가져오기\n",")\n","\n","model = get_pl_model(cfg, ckpt_path).eval()\n","print(model)"],"metadata":{"id":"5S9d-3a_t5rx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_interpolation_images(\n","        model,\n","        axis1=0,\n","        axis2=1,\n","        latent_dim=4,\n","        save_img_path=None,\n","        range1=np.arange(-2, 2, 0.2),\n","        range2=np.arange(-2, 2, 0.2),\n","    ):\n","        assert len(range1) == len(range2)\n","        z = []\n","        for i in range1:\n","            for j in range2:\n","                cur = [0. for _ in range(latent_dim)]\n","                cur[axis1] = i\n","                cur[axis2] = j\n","                z.append(cur)\n","        \n","        z = torch.Tensor(z)\n","        out = model.sample_generate(z=z)\n","        out = vutils.make_grid(out, nrow=len(range1))\n","\n","        if save_img_path is None:\n","            save_img_path = f\"interpolation_results_{axis1}vs{axis2}.png\"\n","        \n","        vutils.save_image(out, save_img_path)\n","\n","# 모든 경우의 수에 대해 생성\n","create_interpolation_images(model, 0, 1)\n","create_interpolation_images(model, 0, 2)\n","create_interpolation_images(model, 0, 3)\n","create_interpolation_images(model, 1, 2)\n","create_interpolation_images(model, 1, 3)\n","create_interpolation_images(model, 2, 3)"],"metadata":{"id":"FwF_NuWjuXOj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"WuhuVp-SjaPp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"lEZh37HOwPFr"},"execution_count":null,"outputs":[]}]}