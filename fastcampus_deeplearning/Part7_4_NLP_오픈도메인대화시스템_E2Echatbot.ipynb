{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part7_4_NLP_오픈도메인대화시스템_E2Echatbot.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP3P/FD/VigBpBia1TlPG0K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"PQHP1XP_ZeTW"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["drive_project_root = 'drive/MyDrive/ColabNotebooks/'"],"metadata":{"id":"DgVDsFp-aETC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install pytorch-crf"],"metadata":{"id":"t_uff0_9aEVu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install gensim==3.4.0"],"metadata":{"id":"UJaUl5PxaEXa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install sentencepiece"],"metadata":{"id":"Q4GZnq_xaEaI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import sys\n","import json\n","import torch\n","import re\n","import math\n","import random\n","import pandas as pd\n","import numpy as np\n","\n","import torch.utils.data as data\n","from torch.nn import Transformer\n","from torch import nn\n","\n","from torch.autograd import Variable \n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader\n","\n","import sentencepiece as spm\n","\n","from tqdm import tqdm\n","from tqdm import trange\n","import torch.nn.functional as F\n","#from torch.utils.tensorboard import SummaryWriter\n","\n","from drive.MyDrive.ColabNotebooks.src.model import Tformer, save\n","from drive.MyDrive.ColabNotebooks.src.dataset import Preprocessing, MakeDataset\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"Jq1r3WAdaEex"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 오픈 도메인 대화 시스템\n","- 생성 기반 방식으로 실습할 예정\n","  - encoder, decoder로 이루어진 end to end 방식 -> E2E 챗봇이라고 함\n","  - 처음부터 끝까지 deep learning을 사용\n","  \n"],"metadata":{"id":"-dRF36GXaEhs"}},{"cell_type":"markdown","source":["### E2E chatbot 실습\n","\n","- Transformer 모델 사용 예정\n","  - attention을 통해 문장의 어느 부분이 중요한지 판단하고 문맥 파악"],"metadata":{"id":"eH6WYrlFaEke"}},{"cell_type":"markdown","source":["---\n","# 1. Chit Chat based Transformer"],"metadata":{"id":"TV95KzmFcql1"}},{"cell_type":"markdown","source":["### 1.1 Data Processing\n","- https://github.com/songys/Chatbot_data"],"metadata":{"id":"8qKsbzVFbU_c"}},{"cell_type":"code","source":["train_data = pd.read_csv(drive_project_root+'data/dataset/ChatbotData.csv')\n","train_data.head()"],"metadata":{"id":"KKr8X5P4acGY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.2 sentence piece 로 vocab생성\n","- SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing\n","  - Taku Kudo, John Richardson, Google"],"metadata":{"id":"B_FiELb1acJB"}},{"cell_type":"markdown","source":["RNN은 기본적으로 vocab의 크기가 계산량에 영향을 주고 있습니다.\n","그래서 적당한 크기의 vocab을 사용하게 됩니다. 문제는 여기서 많이 발생합니다.\n","우리는 vocab을 만들때 미등록 단어가 발생하게 되고 실제로 입력으로 들어왔을때 UNK토큰으로 대체하게 됩니다.\n","이 과정에서 정보의 손실이 발생하고 성능의 문제를 일으킬수 있습니다.\n","그런 점을 보완하고자 sentencepiece를 tokenizer로 사용하려고 합니다.\n","sentencepiece의 기본 아이디어는 단어(word)의 부분단어(subword)로 모든 단어를 표현하고자 하는게 아이디어입니다.\n","이때 사용하는게 단어들의 빈도수를 사용하여 subword로 나눌지 말지를 판단하게 됩니다."],"metadata":{"id":"sWwg7G5racLJ"}},{"cell_type":"code","source":["corpus = drive_project_root+\"data/dataset/chit-chat_corpus.txt\"\n","prefix = \"chatbot\"\n","vocab_size = 16000\n","spm.SentencePieceTrainer.train(\n","    # 7을 더하는 이유 : PAD, UNK, BOS, EOS, SEP, CLS, MASK 등을 사용하기 위함\n","    f\"--input={corpus} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n","    \" --model_type=bpe\" +\n","    \" --max_sentence_length=999999\" + # 문장 최대 길이\n","    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n","    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n","    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n","    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n","    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰"],"metadata":{"id":"DOngLsKZbJi7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.3 Load & Test"],"metadata":{"id":"cuVPmrEGbJlv"}},{"cell_type":"code","source":["vocab_file = \"chatbot.model\"\n","vocab = spm.SentencePieceProcessor()\n","vocab.load(vocab_file)\n","line = \"3박4일 정도 놀러가고 싶다\"\n","pieces = vocab.encode_as_pieces(line)\n","ids = vocab.encode_as_ids(line)\n","\n","\n","print(line)\n","print(pieces)\n","print(ids)"],"metadata":{"id":"_QN8EM8abJoi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Preprocessing:\n","    '''\n","    데이터의 최대 token길이가 10이지만\n","    실제 환경에서는 얼마의 길이가 들어올지 몰라 적당한 길이 부여\n","    '''\n","    \n","    def __init__(self, max_len = 20):\n","        self.max_len = max_len\n","        self.PAD = 0\n","    \n","    def pad_idx_sequencing(self, q_vec):\n","        q_len = len(q_vec)\n","        diff_len = q_len - self.max_len\n","        if(diff_len>0):\n","            q_vec = q_vec[:self.max_len]\n","            q_len = self.max_len\n","        else:\n","            pad_vac = [0] * abs(diff_len)\n","            q_vec += pad_vac\n","\n","        return q_vec\n","    \n","    def make_batch(self):\n","        pass\n","\n","class ChitChatDataset(data.Dataset):\n","    def __init__(self, x_tensor, y_tensor, labels):\n","        super(ChitChatDataset, self).__init__()\n","\n","        self.x = x_tensor\n","        self.y = y_tensor\n","        self.labels = labels\n","        \n","    def __getitem__(self, index):\n","        return self.x[index], self.y[index], self.labels[index]\n","\n","    def __len__(self):\n","        return len(self.x)\n","    \n","class MakeDataset:\n","    def __init__(self):\n","        \n","        self.chitchat_data_dir = drive_project_root+\"data/dataset/ChatbotData.csv\"\n","        \n","        self.prep = Preprocessing()\n","        vocab_file = \"chatbot.model\"\n","        self.transformers_tokenizer = spm.SentencePieceProcessor()\n","        self.transformers_tokenizer.load(vocab_file)\n","    \n","    def encode_dataset(self, dataset):\n","        token_dataset = []\n","        for data in dataset:\n","            # [2], [3] : 앞뒤로 BOS, EOS 붙여줌 (begin of sentence, end of sentence)\n","            token_dataset.append( [2] + self.transformers_tokenizer.encode_as_ids(data) + [3])\n","        return token_dataset\n","\n","    def make_chitchat_dataset(self, train_ratio = 0.8):\n","        chitchat_dataset = pd.read_csv(self.chitchat_data_dir)\n","        Qs = chitchat_dataset[\"Q\"].tolist()\n","        As = chitchat_dataset[\"A\"].tolist()\n","        label = chitchat_dataset[\"label\"].tolist()\n","        \n","        Qs = self.encode_dataset(Qs)\n","        As = self.encode_dataset(As)\n","        \n","        self.prep.max_len = 40\n","        x, y = [], []\n","        for q, a in zip(Qs,As):\n","            x.append(self.prep.pad_idx_sequencing(q))\n","            y.append(self.prep.pad_idx_sequencing(a))\n","        x = torch.tensor(x)\n","        y = torch.tensor(y)\n","        x_len = x.size()[0]\n","        train_size = int(x_len*train_ratio)\n","        \n","        if(train_ratio == 1.0):\n","            train_x = x[:train_size]\n","            train_y = y[:train_size]\n","            train_label = label[:train_size]\n","            train_dataset = ChitChatDataset(train_x,train_y,train_label)\n","            return train_dataset, None\n","        else:\n","            train_x = x[:train_size]\n","            train_y = y[:train_size]\n","            train_label = label[:train_size]\n","\n","            test_x = x[train_size+1:]\n","            test_y = y[train_size+1:]\n","            test_label = label[train_size+1:]\n","\n","            train_dataset = ChitChatDataset(train_x,train_y,train_label)\n","            test_dataset = ChitChatDataset(test_x,test_y,test_label)\n","\n","            return train_dataset, test_dataset"],"metadata":{"id":"ypcqMiZLbg_m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = MakeDataset()\n","\n","# train, validation 나누고 싶은 경우 1.0 -> 0.8 등으로 수정 필요\n","train_dataset, test_dataset = dataset.make_chitchat_dataset(1.0)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","#test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True)"],"metadata":{"id":"BR0JgASnbhCP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.4 Attention Is All You Need\n","- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n","- tensorflow transformer chatbot code : https://blog.tensorflow.org/2019/05/transformer-chatbot-tutorial-with-tensorflow-2.html"],"metadata":{"id":"yoVv0bivbhEv"}},{"cell_type":"code","source":["class Tformer(nn.Module):\n","    def __init__(self, num_tokens, dim_model, num_heads, dff, num_layers, dropout_p=0.5):\n","        super(Tformer, self).__init__()\n","        self.transformer = Transformer(dim_model, num_heads, dim_feedforward=dff, num_encoder_layers=num_layers, num_decoder_layers=num_layers,dropout=dropout_p)\n","        self.pos_encoder = PositionalEncoding(dim_model, dropout_p)\n","        self.encoder = nn.Embedding(num_tokens, dim_model)\n","\n","        self.pos_encoder_d = PositionalEncoding(dim_model, dropout_p)\n","        self.encoder_d = nn.Embedding(num_tokens, dim_model)\n","\n","        self.dim_model = dim_model\n","        self.num_tokens = num_tokens\n","\n","        self.linear = nn.Linear(dim_model, num_tokens)\n","\n","    def generate_square_subsequent_mask(self, sz):\n","        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","        return mask\n","\n","    def forward(self, src, tgt, srcmask, tgtmask, srcpadmask, tgtpadmask):\n","        src = self.encoder(src) * math.sqrt(self.dim_model)\n","        src = self.pos_encoder(src)\n","\n","        tgt = self.encoder_d(tgt) * math.sqrt(self.dim_model)\n","        tgt = self.pos_encoder_d(tgt)\n","\n","        output = self.transformer(src.transpose(0,1), tgt.transpose(0,1), srcmask, tgtmask, src_key_padding_mask=srcpadmask, tgt_key_padding_mask=tgtpadmask)\n","        output = self.linear(output)\n","        return output\n","\n","# embedding은 단어 위치 학습 불가 ex) 나는 학생이다 = 학생이다 나는\n","# -> 위치 정보를 담기 위해 positional encoding 함\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)\n","\n","def gen_attention_mask(x):\n","    mask = torch.eq(x, 0)\n","    return mask"],"metadata":{"id":"J76IHUAEbhHO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- GPU 필요"],"metadata":{"id":"BGqhKcVzeZDE"}},{"cell_type":"code","source":["model = Tformer(\n","     num_tokens=vocab_size+7, dim_model=256, num_heads=8, dff=512, num_layers=2, dropout_p=0.1\n"," ).cuda()"],"metadata":{"id":"wVpc9ZmJbhJw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr = 1e-4\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","MAX_LENGTH = 40"],"metadata":{"id":"AVryjH1tbhMb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epoch = 70\n","save_dir = drive_project_root+\"data/pretraining/4_chitchat_transformer_model/\"\n","save_prefix = \"chitchat_transformer\"\n","prev_loss_all = float(\"inf\")\n","train_steps = 0\n","test_steps = 0\n","model.train()\n","for i in range(epoch):\n","    batchloss = 0.0\n","    progress = tqdm(train_dataloader)\n","    for (inputs, y, _) in progress:\n","        optimizer.zero_grad()\n","\n","        dec_inputs = y[:,:-1]\n","        outputs = y[:,1:]\n","        \n","        src_mask = model.generate_square_subsequent_mask(MAX_LENGTH).cuda()\n","        src_padding_mask = gen_attention_mask(inputs).cuda()\n","        tgt_mask = model.generate_square_subsequent_mask(MAX_LENGTH-1).cuda()\n","        tgt_padding_mask = gen_attention_mask(dec_inputs).cuda()\n","\n","        result = model(inputs.long().cuda(), dec_inputs.long().cuda(), src_mask, tgt_mask, src_padding_mask,tgt_padding_mask)\n","        loss = criterion(result.permute(1,2,0), outputs.long().cuda())\n","        progress.set_description(\"{:0.3f}\".format(loss))\n","\n","        train_steps += 1\n","        loss.backward()\n","        optimizer.step()\n","        batchloss += loss\n","    \n","    print(\"train epoch:\",i+1,\"|\",\"loss:\",batchloss.cpu().item() / len(train_dataloader))\n","\n","# ===========================================================================\n","# train, validation set 나눈 경우\n","# validation에서 loss가 낮은 step 저장하는 코드\n","# ===========================================================================\n","#     model.eval()\n","#     test_batchloss = 0.0\n","#     progress_test = tqdm(test_dataloader)\n","#     for (inputs, y, _) in progress_test:\n","\n","#         dec_inputs = y[:,:-1]\n","#         outputs = y[:,1:]\n","        \n","#         src_mask = model.generate_square_subsequent_mask(MAX_LENGTH).cuda()\n","#         src_padding_mask = gen_attention_mask(inputs).cuda()\n","#         tgt_mask = model.generate_square_subsequent_mask(MAX_LENGTH-1).cuda()\n","#         tgt_padding_mask = gen_attention_mask(dec_inputs).cuda()\n","\n","#         result = model(inputs.long().cuda(), dec_inputs.long().cuda(), src_mask, tgt_mask, src_padding_mask,tgt_padding_mask)\n"," \n","#         loss = criterion(result.permute(1,2,0), outputs.long().cuda())\n","#         progress_test.set_description(\"{:0.3f}\".format(loss.cpu().item()))\n","\n","#         test_steps += 1\n","#         test_batchloss += loss.cpu().item()\n","#     loss_all = test_batchloss/len(test_dataloader)\n","#     print(\"test epoch:\",i+1,\"|\",\"loss:\",loss_all)\n","#     model.train()\n","#     if(loss_all<prev_loss_all):\n","#         prev_loss_all = loss_all\n","#         save(model, save_dir, save_prefix + \"_\" + str(round(loss_all,6)), i)"],"metadata":{"id":"NAkaF-pYbhPT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss"],"metadata":{"id":"B9qRKt6PcRvV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save(model, save_dir, save_prefix + \"_\" + str(round(loss.cpu().item(),6)), i)"],"metadata":{"id":"cg631CyncR0H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_sentence(sentence):\n","    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","    sentence = sentence.strip()\n","    return sentence\n","\n","def evaluate(sentence):\n","    sentence = preprocess_sentence(sentence)\n","    input = torch.tensor([[2] + vocab.encode_as_ids(sentence) + [3]]).cuda()\n","    output = torch.tensor([[2]]).cuda()\n","\n","    # 디코더의 예측 시작\n","    model.eval()\n","    for i in range(MAX_LENGTH):\n","        src_mask = model.generate_square_subsequent_mask(input.shape[1]).cuda()\n","        tgt_mask = model.generate_square_subsequent_mask(output.shape[1]).cuda()\n","\n","        src_padding_mask = gen_attention_mask(input).cuda()\n","        tgt_padding_mask = gen_attention_mask(output).cuda()\n","\n","        predictions = model(input, output, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask).transpose(0,1)\n","        # 현재(마지막) 시점의 예측 단어를 받아온다.\n","        predictions = predictions[:, -1:, :]\n","        predicted_id = torch.LongTensor(torch.argmax(predictions.cpu(), axis=-1))\n","\n","\n","        # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n","        if torch.equal(predicted_id[0][0], torch.tensor(3)):\n","            break\n","\n","        # 마지막 시점의 예측 단어를 출력에 연결한다.\n","        # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n","        output = torch.cat([output, predicted_id.cuda()], axis=1)\n","\n","    return torch.squeeze(output, axis=0).cpu().numpy()\n","\n","def predict(sentence):\n","    prediction = evaluate(sentence)\n","    predicted_sentence = vocab.Decode(list(map(int,[i for i in prediction if i < vocab_size+7])))\n","\n","    print('Input: {}'.format(sentence))\n","    print('Output: {}'.format(predicted_sentence))\n","\n","    return predicted_sentence"],"metadata":{"id":"4vnD31qmcR4j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_state_dict(torch.load(drive_project_root+\"data/pretraining/save/4_chitchat_transformer_model/chitchat_transformer_1.215381_steps_81.pt\"))\n","\n","model.eval()"],"metadata":{"id":"7rpzcpvpcR7Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = predict(\"난 뭘 해야 할까?\")"],"metadata":{"id":"PoICLvGycR9m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = predict(\"힘들다\")"],"metadata":{"id":"e63QOAIIcSAM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = predict(\"난 혼자인게 좋아\")"],"metadata":{"id":"SzqODVYmbhRm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = predict(\"결혼해줘\")"],"metadata":{"id":"z3BgUHtCbJt2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","# 2. E2E Dialog"],"metadata":{"id":"6a6R0dtDfD68"}},{"cell_type":"code","source":["class E2E_dialog:\n","    def __init__(self, dataset, model_path):\n","        self.vocab = dataset.transformers_tokenizer\n","        self.vocab_size = dataset.transformers_tokenizer.vocab_size()\n","        \n","        self.model = Tformer(num_tokens=self.vocab_size, dim_model=256, num_heads=8, dff=512, num_layers=2, dropout_p=0.1)\n","        # GPU로 학습했기 때문에 CPU로 loading 하려면 device type 적어줘야 함\n","        # - cpu인 경우 : cpu\n","        # - gpu인 경우 : cuda\n","        device = torch.device('cuda')\n","        self.model.load_state_dict(torch.load(model_path, map_location=device))\n","        self.model.eval()\n","        self.MAX_LENGTH = 50\n","        \n","    # 특수문자 제거 -> 성능 저하 유발하기 때문\n","    def preprocess_sentence(self, sentence):\n","        sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","        sentence = sentence.strip()\n","        return sentence\n","\n","    def evaluate(self, sentence):\n","        sentence = self.preprocess_sentence(sentence)\n","        input = torch.tensor([[2] + self.vocab.encode_as_ids(sentence) + [3]]) # 문장 양 옆에 <BOS>, <EOS> 넣기\n","        output = torch.tensor([[2]])  # <BOS>\n","\n","        # decoder 예측 시작\n","        ps = []\n","        for i in range(self.MAX_LENGTH):\n","            src_mask = self.model.generate_square_subsequent_mask(input.shape[1])\n","            tgt_mask = self.model.generate_square_subsequent_mask(output.shape[1])\n","\n","            src_padding_mask = self.model.gen_attention_mask(input)\n","            tgt_padding_mask = self.model.gen_attention_mask(output)\n","            \n","            # 첫 ouput은 <BOS>\n","            # 그 다음에 올 단어는 뭔지 prediction -> softmax를 통해 가장 높은 확률 단어 뽑기\n","            # <EOS>가 나올 때까지 반복\n","            predictions = self.model(input, output, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask).transpose(0,1)\n","            # 현재(마지막) 시점의 예측 단어를 받아온다.\n","            predictions = predictions[:, -1:, :]\n","            predictions = torch.softmax(predictions.view(-1).cpu(), dim=0)\n","            predictions = torch.max(predictions, axis = -1)\n","            predicted_p = predictions.values\n","            ps.append(predicted_p)\n","            predicted_id =predictions.indices.view(1,1)\n","\n","\n","            # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n","            if torch.equal(predicted_id[0][0], torch.tensor(3)):\n","                break\n","\n","            # 마지막 시점의 예측 단어를 출력에 연결한다.\n","            # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n","            output = torch.cat([output, predicted_id], axis=1)\n","\n","        # softmax를 통해 확률이 가장 높은 단어를 뽑을 때,\n","        # 그 단어의 확률들을 list로 만들고 그 list의 평균내기\n","        # = 이 문장에 등장할 평균 확률\n","        return torch.squeeze(output, axis=0).cpu().numpy(), (sum(ps)/len(ps)).detach().numpy()\n","\n","    def predict(self, sentence):\n","        prediction, predicted_sentence_p = self.evaluate(sentence)\n","        predicted_sentence = self.vocab.Decode(list(map(int,[i for i in prediction if i < self.vocab_size])))\n","\n","        print('Input: {}'.format(sentence))\n","        print('Output: {}'.format(predicted_sentence))\n","\n","        return predicted_sentence, predicted_sentence_p"],"metadata":{"id":"QynTQINMfD9s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chitchat_pretrain_path = drive_project_root+\"data/pretraining/save/4_chitchat_transformer_model/chitchat_transformer_1.215381_steps_81.pt\""],"metadata":{"id":"n-u9_2QSfEAJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = MakeDataset()\n","e2e = E2E_dialog(dataset,chitchat_pretrain_path)"],"metadata":{"id":"--vBBiM9fECz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 목적지향 대화시스템 vs 오픈도메인 대화시스템\n","- 목적지향 대화시스템\n","  - Dialog system은 엄밀히 말하면 3개의 deep learning model이 동작함 (OOD, intent, slot)\n","  - 이 모델들이 무겁지 않고 빠른 모델이다 보니 시간이 오래 걸리지 않음\n","  - DM에서 BFS를 통해 search 하는 과정에서 그래프가 크면 시간이 더 걸리게 되지만 웬만해서는 아래 e2e 결과만큼 오래 걸리지는 않음\n","- 오픈도메인 대화시스템\n","  - e2e chatbot 모델 자체가 크기 때문에 오래 걸림\n","  - 따라서 오픈 도메인 대화 시스템은 사용자가 delay를 못 느끼게 시간을 빠르게 하는 것도 매우 중요한 문제"],"metadata":{"id":"uXp1r-EXhHa1"}},{"cell_type":"code","source":["%%time\n","s, p = e2e.predict(\"난 뭘 해야 할까?\")"],"metadata":{"id":"Vne-izibfEFh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["float(p)"],"metadata":{"id":"f3OhB1fffEIX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"XGJ2cu9SfETJ"},"execution_count":null,"outputs":[]}]}