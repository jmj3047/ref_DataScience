{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part4_6_Fashion_Mnist_Configuration_Management_tensorflow.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["8gmIaEsDowHC"],"authorship_tag":"ABX9TyMnVOb+51DYvaoBdFA7wHxU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Tensorflow\n","- `Tensorflow`가 예전에는 쓰기 어려운 모델이었음 (코딩할 줄 아는 사람들만 사용)\n","- 그래서 `pytorch`가 많이 쓰이다 보니, `Tensorflow`에서도 쉽게 사용할 수 있는 `Keras` 만듦\n","- `Tensorflow 2.0`에서는 `keras`와 합쳐진 `tf.keras.Model`이나 `Sequential` 많이 사용\n","- `Tensorflow`에서 train step, test step을 사용하는 class 구조는 `pytorch lightening`과 비슷\n","  - `pytorch lightening` : `pytorch`를 더 쉽게 사용하기 위한 library\n","- Optimizer : Tensorflow addon\n","  - https://www.tensorflow.org/addons/overview?hl=ko\n","  - https://github.com/tensorflow/addons\n","    - 여러 tensorflow 개발자들이 다양한 optimizer 구현 코드 업로드"],"metadata":{"id":"8gmIaEsDowHC"}},{"cell_type":"markdown","source":["## Hydra\n","- Facebook에서 제공하는 범용적인 configuration management tool\n","- 'hydra config' 검색 : https://hydra.cc/docs/intro/\n","- Omegaconf library를 기반으로 만들어짐\n","  - Documentation : https://omegaconf.readthedocs.io/en/2.1_branch/\n","  - https://github.com/omry/omegaconf : 이 개발자가 facebook에 가서 만든 게 hydra\n","  - MLP 같은 작은 모델은 `__init__(self, input_dim: int, h1_dim: int, h2_dim: int, out_dim: int)` 이런 식으로 써도 되지만, 모델이 커질수록 `init` 안에 들어가야 할 *argument 많아지고 관리가 어려워짐\n","    - 그래서 configuration tool을 이용해서 관리하는 것이 권장\n","    - tensorflow에 hyperparameter도 같은 역할이고 이건 tensorflow와 연동이 되는 장점이 있지만 wandB 등 다른 tool과 연동이 잘 안 되는 단점\n","    - omegaconf가 structure 관리에도 유리\n","\n","## Efficient Network\n","- https://www.tensorflow.org/api_docs/python/tf/keras/applications/efficientnet"],"metadata":{"id":"Tg5_VZBezJlZ"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"bfadOzkbuIB4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import sys\n","sys.path.append('/content/drive/MyDrive/#fastcampus')\n","drive_project_root = '/content/drive/MyDrive/#fastcampus'\n","!pip install -r '/content/drive/MyDrive/#fastcampus/requirements.txt'"],"metadata":{"id":"4AAPJkUPpFvb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pip install tensorflow_addons"],"metadata":{"id":"WNUrJmSfA8Ll"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pip install wandb"],"metadata":{"id":"wVE8r_8EBA9l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pip install omegaconf"],"metadata":{"id":"tp5T9_vf0XfV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datetime import datetime\n","\n","import numpy as np\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from omegaconf import OmegaConf, DictConfig    # DictConfig is for time checking\n","\n","import hydra\n","from hydra.core.config_store import ConfigStore\n","\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","\n","import wandb"],"metadata":{"id":"wmIGtqQXpfJr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from config_utils_tf import flatten_dict, register_config, get_optimizer_element, get_callbacks"],"metadata":{"id":"T8mcCpw73M-i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GPU 확인"],"metadata":{"id":"G8_oGfQKp2cT"}},{"cell_type":"code","source":["tf.config.list_physical_devices()"],"metadata":{"id":"NMr3t8-QpzX0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"L76O6F6_p3Ud"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["https://www.tensorflow.org/</br>\n","- https://www.tensorflow.org/overview/?hl=ko</br>\n","- 튜토리얼 : https://www.tensorflow.org/tutorials?hl=ko\n","- API > Tensorflow : 각 함수에 대한 설명\n","  - 구글에 'Tensorflow API 한글' 검색하면 번역본도 볼 수 있음\n","\n","초보자용 vs 전문가용\n","- 수업에서는 전문가용으로 할 예정\n","- 초보자용에서 사용하는 Sequential 버전(순차적으로 build 하는 방법)에는 한계가 있기 때문\n","- 실제 현업/연구에서는 Sequential 거의 안 씀"],"metadata":{"id":"eV6g35xTqKhF"}},{"cell_type":"markdown","source":["## define gpu\n","- https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy\n","- This strategy is typically used for training on one machine with multiple GPUs.\n","- 아래 코드 결과 보면 GPU 0번 잡아서 가져옴"],"metadata":{"id":"2lfmx_jAp5ac"}},{"cell_type":"code","source":["# mirrored_strategy = tf.distribute.MirroredStrategy()"],"metadata":{"id":"N0sq47Enq1fU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## make model"],"metadata":{"id":"L7L9eTkdq1qI"}},{"cell_type":"code","source":["class MLP(tf.keras.Model):\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__()\n","\n","        # tensorflow는 pytorch와 다르게 flatten을 하지 않아도 되지만\n","        # pytorch와 비슷한 구조로 코딩하기 위해 여기서는 썼음\n","        self.flatten = tf.keras.layers.Flatten()\n","\n","        # tf nn module vs keras module\n","        # 1) nn module : tensorflow 1.0에서 사용, 기능이 조금 더 많음\n","        # 2) keras : tensorflow 2.0에서 사용\n","        self.linear1 = tf.keras.layers.Dense(input_dim=cfg.input_dim, units=cfg.h1_dim)\n","        # self.linear2 = tf.keras.layers.Dense(input_dim=h1_dim, units=h2_dim)\n","        # -> 이렇게 써도 되지만, pytorh보다 keras는 flexibility가 있어서 input_dim 생략해도 알아서 인지함\n","        self.linear2 = tf.keras.layers.Dense(units=cfg.h2_dim)\n","        self.linear3 = tf.keras.layers.Dense(units=cfg.out_dim)\n","        self.relu = tf.nn.relu\n","    \n","    # tensorflow에서는 'training=Fasle' 구문 꼭 넣기를 권장함\n","    # 나중에 regularization에서 drop out 할 때 이 부분을 조절할 수 있어야 함\n","    # - 학습일 때는 켜고, evaluation 때는 끄고\n","    def call(self, input, training=False):\n","        x = self.flatten(input)\n","        x = self.relu(self.linear1(x))\n","        x = self.relu(self.linear2(x))\n","        out = self.linear3(x)\n","        out = tf.nn.softmax(out)  # output을 확률값으로 바꿈\n","        return out\n","    \n","    # GradientTape() 구현\n","    # 따로 구현해도 되지만 class 안에 이렇게 넣어주면 나중에 더 코드가 깔끔해짐\n","    def train_step(self, data):\n","        # pass\n","        images, labels = data\n","        \n","        with tf.GradientTape() as tape:\n","            outputs = self(images, training=True)\n","            preds = tf.argmax(outputs, 1)\n","\n","            # 위에서 out이 softmax 안 거친 경우, 여기 넣을 때 softmax 처리 해줘야 함\n","            loss = self.compiled_loss(\n","                labels, outputs\n","            )\n","\n","        # compute gradients\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","\n","        # update weights\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","        \n","        # update the metrics\n","        self.compiled_metrics.update_state(labels, preds)\n","\n","        # return a dict mapping metrics names to current values\n","        logs = {m.name: m.result() for m in self.metrics}\n","        logs.update({\"loss\": loss})\n","        return logs\n","    \n","    def test_step(self, data):\n","        # pass\n","        images, labels = data\n","        outputs = self(images, training=False)\n","        preds = tf.argmax(outputs, 1)\n","        loss = self.compiled_loss(\n","            labels, outputs\n","        )\n","\n","        # update the metrics\n","        self.compiled_metrics.update_state(labels, preds)\n","\n","        # return a dict mapping metrics names to current values\n","        logs = {m.name: m.result() for m in self.metrics}\n","        logs.update({\"test_loss\": loss})\n","        return logs"],"metadata":{"id":"wn2XnFMFq1tW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dropout"],"metadata":{"id":"zYcW3GJlfKWm"}},{"cell_type":"code","source":["# tf.keras.Model 대신 MLP 써도 됨\n","# 그러면 위에서 썼던 MLP class를 가져오는 것이니\n","# MLP 모델과 달라진 __init__, call만 코드 써서 일부 수정해 주고\n","# 그대로 써도 되는 train_step, test_step는 생략 가능\n","class MLPWithDropout(tf.keras.Model):\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__()\n","        self.flatten = tf.keras.layers.Flatten()\n","        self.linear1 = tf.keras.layers.Dense(input_dim=cfg.input_dim, units=cfg.h1_dim)\n","        self.linear2 = tf.keras.layers.Dense(units=cfg.h2_dim)\n","        self.linear3 = tf.keras.layers.Dense(units=cfg.out_dim)\n","        self.dropout = tf.keras.layers.Dropout(cfg.dropout_prob)\n","        self.relu = tf.nn.relu\n","    \n","    def call(self, input, training=False):\n","        x = self.flatten(input)\n","        x = self.relu(self.linear1(x))\n","        x = self.dropout(x, training=training)\n","        x = self.relu(self.linear2(x))\n","        x = self.dropout(x, training=training)\n","        out = self.linear3(x)\n","        out = tf.nn.softmax(out)\n","        return out\n","\n","    def train_step(self, data):\n","        # pass\n","        images, labels = data\n","        \n","        with tf.GradientTape() as tape:\n","            outputs = self(images, training=True)\n","            preds = tf.argmax(outputs, 1)\n","\n","            # 위에서 out이 softmax 안 거친 경우, 여기 넣을 때 softmax 처리 해줘야 함\n","            loss = self.compiled_loss(\n","                labels, outputs\n","            )\n","\n","        # compute gradients\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","\n","        # update weights\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","        \n","        # update the metrics\n","        self.compiled_metrics.update_state(labels, preds)\n","\n","        # return a dict mapping metrics names to current values\n","        logs = {m.name: m.result() for m in self.metrics}\n","        return logs\n","    \n","    def test_step(self, data):\n","        # pass\n","        images, labels = data\n","        outputs = self(images, training=False)\n","        preds = tf.argmax(outputs, 1)\n","        loss = self.compiled_loss(\n","            labels, outputs\n","        )\n","\n","        # update the metrics\n","        self.compiled_metrics.update_state(labels, preds)\n","\n","        # return a dict mapping metrics names to current values\n","        logs = {m.name: m.result() for m in self.metrics}\n","        return logs     "],"metadata":{"id":"1AbjCxDNe9jV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### CNN\n","- init, call 부분은 다시 짜고 train_step, test_step은 위 코드 그대로 사용\n","- tf.kears.layers.Conv2D\n","  - https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D\n","  - filters : channel 수 의미 (RGB : 3)"],"metadata":{"id":"xyf2W9N403RN"}},{"cell_type":"code","source":["class ConvBatchNormMaxPool(tf.keras.layers.Layer):\n","\n","    def __init__(\n","        self,\n","        conv2d_filters,\n","        conv2d_kernel_size,\n","        conv2d_strides,\n","        conv2d_padding,\n","        maxpool2d_pool_size,\n","        maxpool2d_strides,\n","        maxpool2d_padding,\n","        ):\n","        super().__init__()\n","        self.conv2d =tf.keras.layers.Conv2D(\n","            filters=conv2d_filters,\n","            kernel_size=conv2d_kernel_size,\n","            strides=conv2d_strides,\n","            padding=conv2d_padding,\n","        )\n","        self.batchnorm = tf.keras.layers.BatchNormalization()\n","        self.maxpool2d = tf.keras.layers.MaxPool2D(\n","            pool_size=maxpool2d_pool_size,\n","            strides=maxpool2d_strides,\n","            padding=maxpool2d_padding,\n","        )\n","\n","    def call(self, input):\n","        # Conv2D -> Batch Normalization -> Activation -> Maxpooling (-> Dropout)\n","        x = self.conv2d(input)\n","        x = self.batchnorm(x)\n","        x = tf.keras.activations.relu(x)   # 이것도 위에 option을 줘서 relu 등 여러 가지를 시도해 볼 수 있음\n","        out = self.maxpool2d(x)\n","        return out\n","\n","class CNN(tf.keras.Model):\n","\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__()\n","        # Conv2D -> Batch Normalization -> Activation -> Maxpooling (-> Dropout)\n","        self.layer1 = ConvBatchNormMaxPool(\n","            cfg.layer_1.conv2d_filters,\n","            cfg.layer_1.conv2d_kernel_size,\n","            cfg.layer_1.conv2d_strides,\n","            cfg.layer_1.conv2d_padding,\n","            cfg.layer_1.maxpool2d_pool_size,\n","            cfg.layer_1.maxpool2d_strides,\n","            cfg.layer_1.maxpool2d_padding,\n","        )\n","        self.layer2 = ConvBatchNormMaxPool(\n","            cfg.layer_2.conv2d_filters,\n","            cfg.layer_2.conv2d_kernel_size,\n","            cfg.layer_2.conv2d_strides,\n","            cfg.layer_2.conv2d_padding,\n","            cfg.layer_2.maxpool2d_pool_size,\n","            cfg.layer_2.maxpool2d_strides,\n","            cfg.layer_2.maxpool2d_padding,\n","        )\n","\n","        # Batch Layer classification을 위해 두 가지 방법 사용 가능\n","        # 방법1. Global pooling -> 바로 dense + output layer (softmax) 적용\n","        # 방법2. Flatten -> dense로 연결\n","        # 지금은 방법2를 시도해 볼 것임\n","        self.flatten = tf.keras.layers.Flatten()\n","        self.fc1 = tf.keras.layers.Dense(cfg.fc_1.units)\n","        self.fc2 = tf.keras.layers.Dense(cfg.fc_2.units)\n","        self.fc3 = tf.keras.layers.Dense(cfg.fc_3.units)\n","        self.dropout = tf.keras.layers.Dropout(cfg.dropout_prob)\n","    \n","    def call(self, input, training=False):\n","        input = tf.expand_dims(input, -1)  # [batch * 28 * 28 * 1] : 이렇게 filter 차원을 추가해 줘야 convolutional layer shape에 맞게 됨\n","        x = self.layer1(input)\n","        x = self.layer2(x)\n","        x = self.flatten(x)\n","        x = self.fc1(x)                  # fully connect\n","        x = self.dropout(x, training)    # training을 넣어야 evaluation에서 정상 작동\n","        x = self.fc2(x)\n","        out = self.fc3(x)\n","        out = tf.nn.softmax(out)         # softmax를 여기 말고 나중에 loss식 계산할 때 해도 괜찮음\n","        return out\n","\n","    def train_step(self, data):\n","        # pass\n","        images, labels = data\n","        \n","        with tf.GradientTape() as tape:\n","            # images = (batch * 28 * 28)\n","            # -> convolutional layer에서는 filter가 필요\n","            # -> def call 부분에서 input의 차원을 늘려줘야 함 : expand_dim\n","            outputs = self(images, training=True)\n","            preds = tf.argmax(outputs, 1)\n","\n","            # 위에서 out이 softmax 안 거친 경우, 여기 넣을 때 softmax 처리 해줘야 함\n","            loss = self.compiled_loss(\n","                labels, outputs\n","            )\n","\n","        # compute gradients\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","\n","        # update weights\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","        \n","        # update the metrics\n","        self.compiled_metrics.update_state(labels, preds)\n","\n","        # return a dict mapping metrics names to current values\n","        logs = {m.name: m.result() for m in self.metrics}\n","        return logs\n","    \n","    def test_step(self, data):\n","        # pass\n","        images, labels = data\n","        outputs = self(images, training=False)\n","        preds = tf.argmax(outputs, 1)\n","        loss = self.compiled_loss(\n","            labels, outputs\n","        )\n","\n","        # update the metrics\n","        self.compiled_metrics.update_state(labels, preds)\n","\n","        # return a dict mapping metrics names to current values\n","        logs = {m.name: m.result() for m in self.metrics}\n","        return logs     "],"metadata":{"id":"t01FxHOC03Ub"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Efficient Net\n","- https://www.tensorflow.org/api_docs/python/tf/keras/applications/efficientnet/EfficientNetB0"],"metadata":{"id":"9Lt2tvVq3XRk"}},{"cell_type":"code","source":["class EfficientNetFinetune(tf.keras.Model):\n","\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__()\n","        # getattr(A, B) : B에 매칭되는 object를 A에서 가져옴\n","        self.efficientnet = getattr(\n","            tf.keras.applications.efficientnet,\n","            cfg.efficient_net_model_name\n","        )(**cfg.kwargs)\n","\n","        # Freeze\n","        self.efficientnet.trainable = cfg.efficient_net_weight_trainable\n","\n","        # Resizing 안 해도 되는데 혹시 모르니 함\n","        # input을 image net과 size 맞추기 위함\n","        self.resize=tf.keras.layers.Resizing(224, 224)\n","        self.avgpool = tf.keras.layers.GlobalAveragePooling2D(name=\"avg_pool\")\n","        self.out_dense = tf.keras.layers.Dense(units=cfg.classes)        \n","    \n","    def call(self, input, training=False):\n","        input = tf.expand_dims(input, -1)  # [batch, 28, 28, 1]\n","        x = self.resize(input)             # [batch, 224, 224, 1]\n","\n","        # imagenet은 RGB 채널이 있기 때문에 channel을 1이 아닌 3으로 바꿔줘야 함\n","        # 아래 방법은 권장하지 않는 약간 무식한 방법 (axis=-2 : 채널 2개 추가)\n","        x = tf.stack([x, x, x], axis=-2)   # [batch, 224, 224, 3] \n","\n","        x = self.efficientnet(x, training=training)\n","\n","        # build top\n","        x = self.avgpool(x)\n","        out = self.out_dense(x)\n","        out = tf.nn.softmax(out)\n","        return out\n","\n","    def train_step(self, data):\n","        # pass\n","        images, labels = data\n","        \n","        with tf.GradientTape() as tape:\n","            # images = (batch * 28 * 28)\n","            # -> convolutional layer에서는 filter가 필요\n","            # -> def call 부분에서 input의 차원을 늘려줘야 함 : expand_dim\n","            outputs = self(images, training=True)\n","            preds = tf.argmax(outputs, 1)\n","\n","            # 위에서 out이 softmax 안 거친 경우, 여기 넣을 때 softmax 처리 해줘야 함\n","            loss = self.compiled_loss(\n","                labels, outputs\n","            )\n","\n","        # compute gradients\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","\n","        # update weights\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","        \n","        # update the metrics\n","        self.compiled_metrics.update_state(labels, preds)\n","\n","        # return a dict mapping metrics names to current values\n","        logs = {m.name: m.result() for m in self.metrics}\n","        return logs\n","    \n","    def test_step(self, data):\n","        # pass\n","        images, labels = data\n","        outputs = self(images, training=False)\n","        preds = tf.argmax(outputs, 1)\n","        loss = self.compiled_loss(\n","            labels, outputs\n","        )\n","\n","        # update the metrics\n","        self.compiled_metrics.update_state(labels, preds)\n","\n","        # return a dict mapping metrics names to current values\n","        logs = {m.name: m.result() for m in self.metrics}\n","        return logs "],"metadata":{"id":"AxpAQi0Q3XZe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Configuration 정의"],"metadata":{"id":"uDuzP0sY4b0_"}},{"cell_type":"markdown","source":["### data configuration"],"metadata":{"id":"8r1ww_KV6p-r"}},{"cell_type":"code","source":["data_fashion_mnist_cfg: dict = {\n","    \"n_class\": 10,\n","    \"train_val_split\": [0.9, 0.1],\n","    \"train_val_shuffle\": True,\n","    \"train_val_shuffle_buffer_size\": 1024,\n","    \"test_shuffle\": False,\n","    \"tset_shuffle_buffer_size\": 1024,\n","}"],"metadata":{"id":"NGVZjvXZ6qAO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### model configuration"],"metadata":{"id":"LId0Hw-W6qJr"}},{"cell_type":"code","source":["model_mnist_mlp_cfg: dict = {\n","    \"name\": \"MLP\",\n","    \"data_normalize\": True,\n","    \"input_dim\": 28*28*1,\n","    \"h1_dim\": 128,\n","    \"h2_dim\": 64,\n","    \"out_dim\": 10,\n","}\n","\n","model_mnist_mlp_with_dropout_cfg: dict = {\n","    \"name\": \"MLPWithDropout\",\n","    \"data_normalize\": True,\n","    \"input_dim\": 128*128*1,\n","    \"h1_dim\": 128,\n","    \"h2_dim\": 64,\n","    \"out_dim\": 10,\n","    \"dropout_prob\": 0.3,\n","}\n","\n","model_mnist_cnn_cfg: dict = {\n","    \"name\": \"CNN\",\n","    \"data_normalize\": True,\n","    'layer_1' : {\n","        \"conv2d_filters\": 32,    # output filter : 아무 값이나 넣어도 됨\n","        \"conv2d_kernel_size\": [3, 3],  # 2D니까 2*2로 넣어줘야 함\n","        \"conv2d_strides\": [1, 1],\n","        # padding\n","        # same : shape을 유지하면서 padding\n","        # valid : no padding\n","        \"conv2d_padding\": \"same\",\n","        \"maxpool2d_pool_size\": [2, 2],\n","        \"maxpool2d_strides\": [2, 2],\n","        \"maxpool2d_padding\": \"valid\",\n","    },\n","    'layer_2' : {\n","        \"conv2d_filters\": 64,\n","        \"conv2d_kernel_size\": [3, 3],\n","        \"conv2d_strides\": [1, 1],\n","        \"conv2d_padding\": \"valid\",\n","        \"maxpool2d_pool_size\": [2, 2],\n","        \"maxpool2d_strides\": [1, 1],   # 너무 많은 정보가 사라지지 않게 [1, 1]로 줄임\n","        \"maxpool2d_padding\": \"valid\",      \n","    },\n","    'fc_1': {\"units\":512},\n","    'fc_2': {\"units\":128},   # classification을 위해 점점 줄여서\n","    'fc_3': {\"units\":10},    # 최종 class 개수\n","    \"dropout_prob\": 0.25,\n","}\n","\n","model_mnist_efficient_finetune_cfg: dict = {\n","    \"name\": \"EfficientNetFinetune\",\n","    \"data_normalize\": False,\n","    \"efficient_net_model_name\": \"EfficientNetB0\",\n","    \"classes\": 10,\n","    \"efficient_net_weight_trainable\": False,   # True : 학습할 수 있는 parameter가 늘어남\n","\n","    # 어떤 걸 입력해야 할 지 모르니 keyword argument 형태로 넣기\n","    \"kwargs\": {\n","        \"include_top\": False,\n","        \"weights\": \"imagenet\",\n","    }\n","}\n"],"metadata":{"id":"nALBOsFe3voa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### optimizer configuration"],"metadata":{"id":"DW0a1cVelHKl"}},{"cell_type":"code","source":["adam_warmup_lr_sch_opt_cfg = {\n","    \"optimizer\": {\n","        \"name\": \"Adam\",\n","        \"other_kwargs\": {},\n","    },\n","    \"lr_scheduler\": {\n","        \"name\": \"LinearWarmupLRSchedule\",\n","        \"kwargs\": {\n","            \"lr_peak\": 1e-3,\n","            \"warmup_end_steps\": 1500,\n","        }\n","    }\n","}\n","\n","# RAdam은 scheduler 필요 없었음\n","radam_no_lr_sch_opt_cfg = {\n","    \"optimizer\": {\n","        \"name\": \"RectifiedAdam\",\n","        \"learning_rate\": 1e-3,\n","        \"other_kwargs\": {},\n","    },\n","    \"lr_scheduler\": None\n","}\n","\n","# train_cfg\n","train_cfg: dict = {\n","    \"train_batch_size\": 128,\n","    \"val_batch_size\": 32,\n","    \"tset_batch_size\": 32,\n","    \"max_epochs\": 50,\n","    \"distribute_strategy\": \"MirroredStrategy\",   # colab(notebook)이 아니고 다른 server에서 하면 다른 strategy 필요\n","}\n","\n","_merged_cfg_presets = {\n","    \"cnn_fashion_mnist_radam\": {\n","        \"data\": data_fashion_mnist_cfg,\n","        \"model\": model_mnist_cnn_cfg,\n","        \"opt\": radam_no_lr_sch_opt_cfg,\n","        \"train\": train_cfg,\n","    },\n","    \"mlp_with_dropout_fahion_mnist_adam_with_warmup_lr_schedule\": {\n","        \"data\": data_fashion_mnist_cfg,\n","        \"model\": model_mnist_mlp_with_dropout_cfg,\n","        \"opt\": adam_warmup_lr_sch_opt_cfg,\n","        \"train\": train_cfg,\n","    }\n","}\n","\n","### hydra composition ###\n","# clear hydra instance -> Jupyter 환경에서 할 때는 일단 instance clear 하기\n","hydra.core.global_hydra.GlobalHydra.instance().clear()\n","\n","# register preset configs\n","register_config(_merged_cfg_presets)\n","\n","# initialization\n","hydra.initialize(config_path=None)    # yaml을 쓰고 있고 외부에서 하면 config_path 지정해야 함\n","\n","# using_config_key = \"cnn_fashion_mnist_radam\"\n","using_config_key = \"mlp_with_dropout_fahion_mnist_adam_with_warmup_lr_schedule\"\n","cfg = hydra.compose(using_config_key)\n","\n","# define & override log _cfg\n","model_name = cfg.model.name\n","run_dirname = \"dnn-tutorial-fashion-mnist-runs-tf\"\n","run_name = f\"{datetime.now().isoformat(timespec='seconds')}-{using_config_key}-{model_name}\"\n","log_dir = os.path.join(drive_project_root, \"runs\", run_dirname, run_name)\n","\n","log_cfg = {\n","    \"run_name\": run_name,\n","    \"callbacks\": {\n","        \"TensorBoard\": {\n","            \"log_dir\": log_dir,\n","            \"update_freq\": 1,\n","        },\n","        \"EarlyStopping\": {\n","            \"patience\": 3,\n","            \"verbose\": True,\n","        }\n","    },\n","    \"wandb\": {\n","        \"project\": \"fastcampus_fashion_mnist_tutorials_tf\",\n","        \"name\": run_name,\n","        \"tags\": [\"fastcampus_fashion_mnist_tutorials_tf\"],\n","        \"reinit\": True,\n","        \"sync_tensorboard\": True,\n","    }\n","}\n","\n","# unlock struct of config & set log config\n","OmegaConf.set_struct(cfg, False)\n","cfg.log = log_cfg\n","\n","# relock config\n","OmegaConf.set_struct(cfg, True)\n","print(OmegaConf.to_yaml(cfg))\n","\n","# save yaml\n","# with open(os.path.join(log_dir, \"config.yaml\")) as f:\n","# with open(\"config.yaml\", \"w\") as f:\n","#     OmegaConf.save(cfg, f)\n","\n","# This would open the file we saved above\n","# and tell you the result of the model and its configs (weights, ...)\n","# You can check it whenever you want\n","# OmegaConf.load()"],"metadata":{"id":"StvxdanulHNa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_distribute_strategy(strategy_name: str, **kwargs):\n","    return getattr(tf.distribute, strategy_name)(**kwargs)\n","\n","distribute_strategy = get_distribute_strategy(cfg.train.distribute_strategy)"],"metadata":{"id":"hlbgAzj0lHQs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with distribute_strategy.scope():\n","\n","    # dataset 정의 =====================================================================\n","    fashion_mnist = tf.keras.datasets.fashion_mnist\n","    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n","\n","    # normalization\n","    if cfg.model.data_normalize:\n","        x_train = x_train/255.0\n","        x_test = x_test/255.0\n","\n","    # train/val splits\n","    assert sum(cfg.data.train_val_split) == 1.0\n","    train_size = int(len(x_train) * cfg.data.train_val_split[0])\n","    val_size = len(x_train) - train_size\n","\n","    # train, test dataset 정의\n","    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size=1024)\n","    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n","\n","    if cfg.data.train_val_shuffle:\n","        dataset = dataset.shuffle(buffer_size=cfg.data.train_val_shuffle_buffer_size,)\n","    \n","    if cfg.data.test_shuffle:\n","        test_dataset = test_dataset.shuffle(buffer_size=cfg.data.test_shuffle_buffer_size,)\n","\n","    # train dataset을 train과 validation으로 나누기\n","    # tensorflow에서는 아래와 같이 take, skip 사용해서 데이터 많이 나눔\n","    train_dataset = dataset.take(train_size)   # train은 dataset에서 train_size만큼 take하고\n","    val_dataset = dataset.skip(train_size)     # val은 전체 dataset에서 train_size만큼 skip하고 남은 것\n","\n","    # 검증\n","    print(f'train total : {len(dataset)} (train : {len(train_dataset)}, validation : {len(val_dataset)})')\n","    print(f'test : {len(test_dataset)}')\n","\n","    # dataloader 정의 ==================================================================\n","    train_batch_size = cfg.train.train_batch_size\n","    val_batch_size = cfg.train.val_batch_size\n","    test_batch_size = cfg.train.tset_batch_size\n","\n","    # drop_remainder=True : memory size가 안 맞으면 error 나는 것 방지 (pytorch는 이런 것 자동으로 처리함 = tensorflow와 차이점)\n","    train_dataloader = train_dataset.batch(train_batch_size, drop_remainder=True)\n","    val_dataloader = val_dataset.batch(val_batch_size, drop_remainder=True)\n","    test_dataloader = test_dataset.batch(test_batch_size, drop_remainder=True)\n","\n","sample_example = next(iter(train_dataloader))\n","print(sample_example)"],"metadata":{"id":"dzDsYhtQ3vrS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## define model\n","\n","LinearWarmupLRScheduler 하는 이유\n","- SGD는 다른 optimizer 대비 learning rate 값에 매우 민감\n","  - learning rate를 잘 setting 해야 성능이 좋게 나옴 (Adam보다 더 좋게 나오기도 함)\n","- 따라서 optimizer와 함께 learning rate도 tuning 하는 게 원래는 좋음\n","- 그러나 학습 속도가 너무 느려지는 단점\n","\n","warmup을 하기 어려운 상황이면?\n","- Rectified Adam으로 먼저 테스트 해 보고, optimizer는 조절해도 거의 결과 비슷하게 나오니, 모델링 부분을 업데이트 해 보기\n","- Rectified Adam에도 tuning 할 수 있는 요소 많음\n","  - https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/RectifiedAdam"],"metadata":{"id":"04O0P4wLPmC2"}},{"cell_type":"code","source":["# 모델 정의\n","def get_model(cfg: DictConfig):\n","    if cfg.model.name == 'CNN':\n","        model = CNN(cfg.model)\n","    elif cfg.model.name == \"EfficientNetFinetune\":\n","        model = EfficientNetFinetune(cfg.model)\n","    elif cfg.model.name == 'MLP':\n","        model = MLP(cfg.model)\n","    elif cfg.model.name == 'MLPWithDropout':\n","        model = MLPWithDropout(cfg.model)\n","    else:\n","        raise NotImplementedError\n","    \n","    return model\n","\n","with distribute_strategy.scope():\n","    model = get_model(cfg)\n","\n","    # define loss\n","    loss_function = tf.losses.SparseCategoricalCrossentropy()\n","\n","    # define optimizer & scheduler\n","    optimizer, scheduler = get_optimizer_element(\n","        cfg.opt.optimizer, cfg.opt.lr_scheduler\n","    )\n","\n","    model.compile(\n","        loss = loss_function,\n","        optimizer = optimizer,\n","        metrics = [tf.keras.metrics.Accuracy()],\n","    )\n","\n","    # model build\n","    # 이 부분 생략해도 되지만 build를 해 놓으면 나중에 debugging하기 좋음 -> 권장\n","    # batch 1 : 임의로 설정\n","    # model.build((1, 28*28*1))\n","    model.build((1, 28, 28))      # This build code is for 'CNN'\n","\n","# 만약 build 안 하고 summary 하면 build, fit을 하거나 input shape를 넣으라고 경고 뜸\n","# fit은 학습이기 때문에 무거운 감이 있고 빠르게 하기 위해 build 선호\n","model.summary()"],"metadata":{"id":"P1kMS7uKq1wU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## get callbacks"],"metadata":{"id":"rapd69GTRAuU"}},{"cell_type":"code","source":["callbacks = get_callbacks(cfg.log)"],"metadata":{"id":"lSbWZOHRSsGW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## wandb setup\n","\n","- https://docs.wandb.ai/guides/integrations/tensorflow\n","- sync_tensorboard=True : tensorflow에 적혀있는 걸 wandb에 업로드"],"metadata":{"id":"W3kBknhvHV3r"}},{"cell_type":"code","source":["# flatten_dict(cfg)   # 전부 flatten 하게 바꿔주는 함수 -> nested 구조를 모두 under bar 형태로 바꿈"],"metadata":{"id":"SpD4-l8og5RW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wandb.init(\n","    config= flatten_dict(cfg),\n","    **cfg.log.wandb\n",")"],"metadata":{"id":"WxPeYZkVHWBV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["경로 잘 찾고 있는지 확인"],"metadata":{"id":"YGTecT2yVqp8"}},{"cell_type":"code","source":["! ls /content/drive/MyDrive/\\#fastcampus/runs/"],"metadata":{"id":"1u3ORwmUVijX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tensorboard load하기 : load extension\n","%load_ext tensorboard\n","\n","# 경로 지정 : terminal 문법이기 때문에 #을 # 그대로 인지하려면 앞에 '\\' 써줘야 함\n","%tensorboard --logdir /content/drive/MyDrive/\\#fastcampus/runs/\n","\n","model.fit(\n","    train_dataloader,\n","    validation_data=val_dataloader,\n","    epochs=cfg.train.max_epochs,\n","    callbacks=callbacks,\n",")"],"metadata":{"id":"HubFa3A2Szm9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## model testing"],"metadata":{"id":"F9Tx9Zu5Tg5Y"}},{"cell_type":"code","source":["model.evaluate(test_dataloader)"],"metadata":{"id":"83w4bBslWGNw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["auc curve"],"metadata":{"id":"XnbF1YQ_WM1p"}},{"cell_type":"code","source":["test_labels_list = []\n","test_preds_list = []\n","test_outputs_list = []\n","\n","for i, (test_images, test_labels) in enumerate(tqdm(test_dataloader, position=0, leave=True, desc='testing')):\n","    with mirrored_strategy.scope():\n","        test_outputs = model(test_images)\n","    test_preds = tf.argmax(test_outputs, 1)\n","\n","    final_outs = test_outputs.numpy()\n","    test_outputs_list.extend(final_outs)\n","\n","    test_preds_list.extend(test_preds.numpy())\n","    test_labels_list.extend(test_labels.numpy())\n","\n","test_preds_list = np.array(test_preds_list)\n","test_labels_list = np.array(test_labels_list)\n","\n","test_accuracy = np.mean(test_preds_list == test_labels_list)\n","print(f'\\nacc: {test_accuracy*100}%')"],"metadata":{"id":"5K99rt_2WJ1O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["roc curve"],"metadata":{"id":"Ib1UnftTXUkw"}},{"cell_type":"code","source":["from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score\n","\n","fpr = {}\n","tpr = {}\n","thresh = {}\n","n_class = 10\n","\n","for i in range(n_class):\n","    fpr[i], tpr[i], thresh[i] = roc_curve(test_labels_list, np.array(test_outputs_list)[:, i], pos_label=i)"],"metadata":{"id":"1-guDbjmXhR3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fpr"],"metadata":{"id":"ncHDtbpCX8oJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["plot"],"metadata":{"id":"a32IhqbCYOrO"}},{"cell_type":"code","source":["for i in range(n_class):\n","    plt.plot(fpr[i], tpr[i], linestyle=\"--\", label=f\"Class {i} vs Rest\") \n","\n","plt.title(\"Multi-class ROC Curve\")\n","plt.xlabel(\"Flase Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.legend(loc=\"best\")\n","plt.show()"],"metadata":{"id":"r7tdD3yhYR9Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["auc score\n","- multi class이기 때문에 multi_class, average option 안 넣어주면 error 발생"],"metadata":{"id":"uglGAHtqY6rq"}},{"cell_type":"code","source":["auc_score = roc_auc_score(test_labels_list, test_outputs_list, multi_class=\"ovo\", average=\"macro\")"],"metadata":{"id":"vfISE8sBYoeY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'auc score : {auc_score*100}')"],"metadata":{"id":"Z01rKFr4Y2Uv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"2pWod1MIZILf"},"execution_count":null,"outputs":[]}]}