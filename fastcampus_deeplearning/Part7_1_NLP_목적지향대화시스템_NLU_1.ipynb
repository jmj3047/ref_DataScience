{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"목적지향대화시스템_NLU_1.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNpTbeqfJJua6/fvCaDvvJn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 목적지향 대화시스템 NLU 시스템"],"metadata":{"id":"6Ko5hJVVr8Dq"}},{"cell_type":"markdown","source":["### word2vec\n","- 보통 딥러닝은 입력층과 출력층 사이에 layer가 충분히 쌓여야 하는데 이건 하나의 입력층만 있어서 딥러닝이라고 하기 뭐함\n","- 그리고 그 입력층에는 활성화 함수가 없어서 projection layer라고도 불림\n","- 두 가지 모델 있음\n","  - CBOW : W(t)라는 중심 단어를 주변 단어들을 통해 예측\n","  - Skip-gram : W(t)라는 중심 단어를 통해 주변 단어들을 예측\n","  - 많은 논문에서는 skip-gram 성능이 더 좋다고 함"],"metadata":{"id":"U57ovwPNr8AU"}},{"cell_type":"code","source":["pip install gensim==3.4.0"],"metadata":{"id":"LDiW87sIZY-s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install sentencepiece"],"metadata":{"id":"Ddm8fxQ6Jgqf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install pytorch-crf"],"metadata":{"id":"TfQPiisNJhb0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"l8rNOONocM0k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["drive_project_root = 'drive/MyDrive/Colab Notebooks/'"],"metadata":{"id":"wwRMdy3wcloT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import sys\n","import json\n","import torch\n","import random\n","\n","import numpy as np\n","import pandas as pd\n","\n","# conda install pytorch -c pytorch\n","# pip install torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.data as data\n","\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader\n","\n","from tqdm import tqdm\n","from tqdm import trange\n","\n","# conda install gensim==3.4.0\n","# pip install --upgrade gensim==3.4.0\n","from gensim.models import Word2Vec\n","from gensim.models.callbacks import CallbackAny2Vec\n","\n","#pip install sentencepiece\n","#pip install pytorch-crf\n","from src.dataset import Preprocessing\n","from src.model import EpochLogger, MakeEmbed, save\n","\n","'''\n","pip install pytorch-crf\n","https://pytorch-crf.readthedocs.io/en/stable/\n","'''\n","from torchcrf import CRF\n","\n","from torch.autograd import Variable \n"],"metadata":{"id":"Rud4OI66r8GT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. Embedding\n","### 1.1 Data Processing\n","- Intent Dataset을 corpus로 활용하여 Word2vec 학습을 위한 데이터 처리"],"metadata":{"id":"aUHudIqhZhCq"}},{"cell_type":"code","source":["class Preprocessing:\n","\n","    '''\n","    데이터의 최대 token길이가 10이지만\n","    실제 환경에서는 얼마의 길이가 들어올지 몰라 적당한 길이 부여\n","    '''\n","    \n","    def __init__(self, max_len = 20):\n","        self.max_len = max_len\n","        self.PAD = 0\n","    \n","    def pad_idx_sequencing(self, q_vec):\n","        q_len = len(q_vec)\n","        diff_len = q_len - self.max_len\n","        if(diff_len>0):\n","            q_vec = q_vec[:self.max_len]\n","            q_len = self.max_len\n","        else:\n","            pad_vac = [0] * abs(diff_len)\n","            q_vec += pad_vac\n","\n","        return q_vec\n","\n","class MakeDataset:\n","    def __init__(self):\n","        \n","        self.intent_data_dir = drive_project_root+\"data/dataset/intent_data.csv\"\n","        self.prep = Preprocessing()\n","    \n","    def tokenize(self, sentence):\n","        ''' 띄어쓰기 단위로 tokenize 적용'''\n","        return sentence.split()\n","    \n","    def tokenize_dataset(self, dataset):\n","        ''' Dataset에 tokenize 적용'''\n","        token_dataset = []\n","        for data in dataset:\n","            token_dataset.append(self.tokenize(data))\n","        return token_dataset\n","    \n","    def make_embed_dataset(self, ood = False):\n","        embed_dataset = pd.read_csv(self.intent_data_dir)\n","        embed_dataset = embed_dataset[\"question\"].to_list()\n","        embed_dataset = self.tokenize_dataset(embed_dataset)\n","\n","        return embed_dataset         "],"metadata":{"id":"GMBsC6fRr8Jo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = MakeDataset()\n","embed_dataset = dataset.make_embed_dataset()\n","# embed_dataset"],"metadata":{"id":"I9mnDe9Cr8MT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.2 Embedding"],"metadata":{"id":"vZCSbClza3mS"}},{"cell_type":"code","source":["class EpochLogger(CallbackAny2Vec):\n","    '''Callback to log information about training'''\n","    '''https://radimrehurek.com/gensim/models/callbacks.html'''\n","    '''학습 중간에 프린트를 하기 위한 logger'''\n","    def __init__(self):\n","        self.epoch = 0\n","        \n","    def on_epoch_begin(self, model):\n","        print(\"Epoch #{} start\".format(self.epoch))\n","\n","    def on_epoch_end(self, model):\n","        print(\"Epoch #{} end\".format(self.epoch))\n","        self.epoch += 1\n","\n","class MakeEmbed:\n","    '''https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec'''\n","    '''https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#online-training-resuming-training'''\n","    def __init__(self):\n","        self.model_dir = \"./\"\n","        self.vector_size = 300 # 임베딩 사이즈\n","        self.window_size = 3   # 몇개의 단어로 예측을 할것인지\n","        self.workers = 8       # 학습 스레드의 수\n","        self.min_count = 2     # 단어의 최소 빈도수 (해당 수 미만은 버려진다) -> 너무 적은 수의 단어는 정보가 약하고 학습이 잘 안 돼서\n","        self.iter = 1000       # 1epoch당 학습 수\n","        self.sg = 1            # 1: skip-gram, 0: CBOW\n","        self.model_file = drive_project_root+\"/data/pretraining/word2vec_skipgram_{}_{}_{}\".format(self.vector_size, self.window_size, self.min_count)\n","        self.epoch_logger = EpochLogger()  # 시작, 끝을 알려주기 위한 callback 함수\n","\n","    def word2vec_init(self): # word2vec 초기화 및 세팅\n","        self.word2vec = Word2Vec(size=self.vector_size,\n","                         window=self.window_size,\n","                         workers=self.workers,\n","                         min_count=self.min_count,\n","                         compute_loss=True,\n","                         iter=self.iter)\n","\n","    def word2vec_build_vocab(self, dataset): # 단어장 만들기\n","        self.word2vec.build_vocab(dataset)\n","        \n","    def word2vec_most_similar(self, query): # 비슷한 단어 계산\n","        print(self.word2vec.most_similar(query))\n","        \n","    def word2vec_train(self,embed_dataset, epoch = 0): # 학습\n","        if(epoch == 0):\n","            epoch = self.word2vec.epochs + 1\n","        self.word2vec.train(\n","            sentences=embed_dataset,\n","            total_examples=self.word2vec.corpus_count,\n","            epochs=epoch,\n","            callbacks=[self.epoch_logger]\n","        )\n","\n","        self.word2vec.save(self.model_file + '.gensim')\n","        self.vocab = self.word2vec.wv.index2word\n","        self.vocab = {word: i for i, word in enumerate(self.vocab)}\n","        \n","    def load_word2vec(self):\n","\n","        # 지정해 놓은 위치에 파일이 없는 경우\n","        if not os.path.exists(self.model_file+'.gensim'):\n","            raise Exception(\"모델 로딩 실패 \"+ self.model_file+'.gensim')\n","\n","        # 파일이 있는 경우 model load하고 vocab 생성\n","        # word2vec에 학습 데이터(vocab)에 없는 단어가 존재하면 error 발생함\n","        # 이를 막아주기 위해 <UNK> : unknown token insert -> index 1\n","        # padding을 할 거기 때문에 pad token도 추가 -> index 0\n","        # 최종 결과를 vocab에 저장\n","        self.word2vec = Word2Vec.load(self.model_file+'.gensim')\n","        self.vocab = self.word2vec.wv.index2word\n","        self.vocab.insert(0,\"<UNK>\") # vocab애 없는 토큰 등장할 경우를 대비한 <UNK> 토큰을 vocab에 삽입, index 1\n","        self.vocab.insert(0,\"<PAD>\") # 길이를 맞추기 위한 padding을 위해 <PAD> 토큰을 vacab에 삽입, index 0\n","        self.vocab = {word: i for i, word in enumerate(self.vocab)}\n","        \n","    def query2idx(self, query):\n","        sent_idx = []\n","\n","        for word in query:\n","            if(self.vocab.get(word)):\n","                idx = self.vocab[word]\n","            else:\n","                idx = 1\n","\n","            sent_idx.append(idx)\n","\n","        return sent_idx"],"metadata":{"id":"pXL1M0sUa3pH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embed = MakeEmbed()\n","\n","embed.word2vec_init()\n","\n","embed.word2vec.build_vocab(embed_dataset)"],"metadata":{"id":"-uMv_CGza3t1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 학습이 되지 않은 word2vec에게 '미세먼지'와 가장 가까운 단어가 무엇인지 질문\n","- random하게 나옴"],"metadata":{"id":"Bnnan6LXeJjM"}},{"cell_type":"code","source":["embed.word2vec.wv.most_similar(\"미세먼지\")"],"metadata":{"id":"zmpq4BjNa3wt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 10번의 epoch 학습\n","- 학습 후 word2vec에게 '미세먼지'와 가장 가까운 단어가 무엇인지 질문 : 먼지, 날씨, 모레 등"],"metadata":{"id":"JGWVE7tkeRS1"}},{"cell_type":"code","source":["embed.word2vec_train(embed_dataset,10)"],"metadata":{"id":"9AeU2Z13dZMJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embed.word2vec.wv.most_similar(\"미세먼지\")"],"metadata":{"id":"FitDEQ0FeGgO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.3 word2index"],"metadata":{"id":"kDQB_DLueGiz"}},{"cell_type":"code","source":["# 문장 하나 가져오기\n","sentence = embed_dataset[0]\n","sentence   # token화 됨"],"metadata":{"id":"-WE3AHe-eGlX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["query2idx 함수\n","- query가 list이기 때문에 for문을 돌면서 단어 하나씩을 가져오고 그게 우리 단어장 안에 있는지\n","- 있으면 해당 index를 가져오고 없으면 index를 1로 출력"],"metadata":{"id":"y62oXVzWeosx"}},{"cell_type":"code","source":["embed.query2idx(sentence)"],"metadata":{"id":"jt-_MtmreGn_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w2i = embed.query2idx(sentence)"],"metadata":{"id":"bTxFQlMweGq9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["padding 추가\n","- 보통 NLP에서는 길이 제한을 위해 사용\n","- 장점 : 메모리도 아끼고 정보를 얼만큼만 받을지 사전에 설정\n","- 방법 : 문장 길이가 padding보다 적으면 0으로 채우고, 길면 padding만큼만으로 자름"],"metadata":{"id":"f6j7xEDJe2_p"}},{"cell_type":"code","source":["class Preprocessing:\n","    '''\n","    데이터의 최대 token길이가 10이지만\n","    실제 환경에서는 얼마의 길이가 들어올지 몰라 적당한 길이 부여\n","    '''\n","    \n","    def __init__(self, max_len = 20):\n","        self.max_len = max_len\n","        self.PAD = 0\n","    \n","    def pad_idx_sequencing(self, q_vec):\n","        q_len = len(q_vec)\n","        diff_len = q_len - self.max_len\n","        if(diff_len>0):\n","            q_vec = q_vec[:self.max_len]\n","            q_len = self.max_len\n","        else:\n","            pad_vac = [0] * abs(diff_len)\n","            q_vec += pad_vac\n","\n","        return q_vec"],"metadata":{"id":"4YmO0JDUeGtV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset.prep.pad_idx_sequencing(w2i)"],"metadata":{"id":"kmsB-7ldeGv-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.4 word2vec load"],"metadata":{"id":"dtO2otOweGy6"}},{"cell_type":"markdown","source":["- load 전 index 보면 '미세먼지'가 0번, '추천해'가 1번"],"metadata":{"id":"QqtLLtfJgJQs"}},{"cell_type":"code","source":["list(embed.vocab)[:5]"],"metadata":{"id":"qrRqVDPmfMI5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embed.load_word2vec()"],"metadata":{"id":"nTGLUCSBfMLd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(embed.vocab)  # vocab 개수 : 1481"],"metadata":{"id":"qPqdhEpNfMN8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- load 후 PAD, UNK 추가됨\n","- index가 하나씩 밀리면서 '미세먼지'는 2번, '추천해'는 3번이 됨"],"metadata":{"id":"0_UFO-o_gSWK"}},{"cell_type":"code","source":["embed.vocab"],"metadata":{"id":"6SXdKMM1fMSs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","# 2. Intent Classification (의도 분류)\n","\n","### 2.1 Data Processing"],"metadata":{"id":"bjdHIJzsdZPb"}},{"cell_type":"code","source":["class MakeDataset:\n","    def __init__(self):\n","        \n","        self.intent_label_dir = \"./data/dataset/intent_label.json\"\n","        self.intent_data_dir = \"./data/dataset/intent_data.csv\"\n","        \n","        self.intent_label = self.load_intent_label()\n","        self.prep = Preprocessing()\n","    \n","    def load_intent_label(self):\n","        ''' 미리 만들어 둔 예측해야할 intent label 로드'''\n","        f = open(self.intent_label_dir, encoding=\"UTF-8\") \n","        intent_label = json.loads(f.read())\n","        self.intents = list(intent_label.keys())\n","        return intent_label\n","    \n","    def tokenize(self, sentence):\n","        ''' 띄어쓰기 단위로 tokenize 적용'''\n","        return sentence.split()\n","    \n","    def tokenize_dataset(self, dataset):\n","        ''' Dataset에 tokenize 적용'''\n","        token_dataset = []\n","        for data in dataset:\n","            token_dataset.append(self.tokenize(data))\n","        return token_dataset\n","\n","    def make_intent_dataset(self, embed):\n","        ''' intent 분류를 위한 Dataset 생성'''\n","        intent_dataset = pd.read_csv(self.intent_data_dir) # 데이터 로딩\n","\n","        labels = [self.intent_label[label] for label in intent_dataset[\"label\"].to_list()] # label -> index\n","            \n","        intent_querys = self.tokenize_dataset(intent_dataset[\"question\"].tolist()) # 사용자 발화 -> tokenize (embedding과 동일)\n","        \n","        dataset = list(zip(intent_querys, labels)) # (사용자 발화, intent) 형태로 가공\n","        intent_train_dataset, intent_test_dataset = self.word2idx_dataset(dataset, embed) # word2index\n","        return intent_train_dataset, intent_test_dataset\n","    \n","    def word2idx_dataset(self, dataset ,embed, train_ratio = 0.8):\n","        embed_dataset = []\n","        question_list, label_list = [], []\n","        flag = True\n","        random.shuffle(dataset) #  훈련용과 검증용으로 나눌때 intent 편형이 나타나지 않도록 데이터 셔플\n","        for query, label in dataset :\n","            q_vec = embed.query2idx(query) # 사용자 발화 index화\n","            q_vec = self.prep.pad_idx_sequencing(q_vec) # 사용자 발화 최대길이까지 padding\n","\n","            question_list.append(torch.tensor([q_vec]))\n","            label_list.append(torch.tensor([label]))\n","\n","        x = torch.cat(question_list)\n","        y = torch.cat(label_list)\n","\n","        # 학습용과 검증용으로 나누기\n","        x_len = x.size()[0]\n","        y_len = y.size()[0]\n","        if(x_len == y_len):\n","            train_size = int(x_len*train_ratio)\n","            \n","            train_x = x[:train_size]\n","            train_y = y[:train_size]\n","\n","            test_x = x[train_size+1:]\n","            test_y = y[train_size+1:]\n","            \n","            # TensorDataset으로 감싸기\n","            '''\n","             PyTorch의 TensorDataset은 tensor를 감싸는 Dataset입니다.\n","\n","             인덱싱 방식과 길이를 정의함으로써 이것은 tensor의 첫 번째 차원을 따라 반복, 인덱스, 슬라이스를 위한 방법을 제공합니다.\n","\n","             훈련할 때 동일한 라인에서 독립 변수와 종속 변수에 쉽게 접근할 수 있습니다.\n","            '''\n","            train_dataset = TensorDataset(train_x,train_y)\n","            test_dataset = TensorDataset(test_x,test_y)\n","            \n","            return train_dataset, test_dataset\n","            \n","        else:\n","            print(\"ERROR x!=y\")\n","            "],"metadata":{"id":"rDWhTkpQWGRi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = MakeDataset()\n","intent_dataset = pd.read_csv(dataset.intent_data_dir)\n","intent_dataset.head()"],"metadata":{"id":"dbn9Fmp8WGUS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["intent_dataset.groupby(['label']).count() "],"metadata":{"id":"4ypU4nWqWGWr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embed = MakeEmbed()\n","embed.load_word2vec()\n","\n","batch_size = 128\n","\n","intent_train_dataset, intent_test_dataset = dataset.make_intent_dataset(embed)\n","\n","# 한번의 iter당 Batch size의 x, y를 제공한다.\n","train_dataloader = DataLoader(intent_train_dataset, batch_size=batch_size, shuffle=True) \n","\n","test_dataloader = DataLoader(intent_test_dataset, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"MMsO6jaXWGZk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["intent_train_dataset.tensors"],"metadata":{"id":"kZTJfgH6WGce"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2.2 Modeling : Convolutional Neural Networks for Sentence Classification\n","- Yoon Kim, New York University\n","- tensorflow code : https://github.com/SeonbeomKim/TensorFlow-TextCNN/blob/master/TextCNN.py"],"metadata":{"id":"5PGMU2uyWGfg"}},{"cell_type":"code","source":["class textCNN(nn.Module):\n","    \n","    def __init__(self, w2v, dim, kernels, dropout, num_class):\n","        super(textCNN, self).__init__()\n","        # Word2vec으로 미리 학습해둔 임베딩 적용\n","        vocab_size = w2v.size()[0]\n","        emb_dim = w2v.size()[1]\n","        self.embed = nn.Embedding(vocab_size+2, emb_dim) # +2 : <UNK>, <PAD> -> 학습을 미리 하지 않았기 때문에 이 부분만 copy하기 위함\n","        self.embed.weight[2:].data.copy_(w2v)\n","        # self.embed.weight.requires_grad = False # 임베딩 레이어 학습 유무\n","        \n","        # 윈도우 사이즈가 다른 각각의 conv layer 를 nn.ModuleList로 저장\n","        # nn.Conv2d(in_channels, out_channels, kernel_size)\n","        self.convs = nn.ModuleList([nn.Conv2d(1, dim, (w, emb_dim)) for w in kernels])\n","        #Dropout layer\n","        self.dropout = nn.Dropout(dropout)\n","        \n","        #FC layer\n","        self.fc = nn.Linear(len(kernels)*dim, num_class)\n","        \n","    def forward(self, x):\n","        emb_x = self.embed(x)\n","        emb_x = emb_x.unsqueeze(1)\n","\n","        con_x = [conv(emb_x) for conv in self.convs] # 각 사이즈 별 결과를 list로 저장, \n","        #[(out_channels, conv결과 길이),...]\n","\n","        pool_x = [F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in con_x] # 각 사이즈별 max_pool 결과 저장\n","        #[(256,1),...]\n","\n","        fc_x = torch.cat(pool_x, dim=1) # concat하여 fc layer의 입력 형태로 만듬\n","        #(768,1) : 256 * 3 = 768\n","\n","        fc_x = fc_x.squeeze(-1) # 차원 맞추기\n","        #(768)\n","        fc_x = self.dropout(fc_x)\n","        logit = self.fc(fc_x)\n","        return logit\n","\n","# 모델의 가중치 저장을 위한 코드\n","def save(model, save_dir, save_prefix, epoch):\n","    if not os.path.isdir(save_dir):\n","        os.makedirs(save_dir)\n","    save_prefix = os.path.join(save_dir, save_prefix)\n","    save_path = '{}_steps_{}.pt'.format(save_prefix, epoch)\n","    torch.save(model.state_dict(), save_path)"],"metadata":{"id":"hgvr6L2VWGiU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["weights = embed.word2vec.wv.vectors # word2vec weight\n","weights = torch.FloatTensor(weights)\n","\n","num_class = len(dataset.intent_label) \n","model = textCNN(weights, 256, [3,4,5], 0.5, num_class)  # [3,4,5]: convolution, 0.5: Dropout, num_class: 4 (intent 총 개수)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"],"metadata":{"id":"EcjIMGbXWGpu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"id":"BpoRAwSIYOns"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2.3 Training"],"metadata":{"id":"uRWP92XhYOp8"}},{"cell_type":"code","source":["%%time\n","epoch = 10\n","prev_acc = 0\n","save_dir = \"./data/pretraining/1_intent_clsf_model/\"\n","save_prefix = \"intent_clsf\"\n","for i in range(epoch):\n","    steps = 0\n","    model.train() \n","    #for data in train_dataloader:\n","    with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n","        for data in tepoch:\n","            tepoch.set_description(f\"Epoch {i}\")\n","            x = data[0]\n","            target = data[1]\n","            logit = model.forward(x)\n","            \n","            optimizer.zero_grad()\n","            loss = F.cross_entropy(logit, target) # loass function\n","            loss.backward()\n","            optimizer.step()\n","\n","            corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n","            accuracy = 100.0 * corrects/x.size()[0]\n","            tepoch.set_postfix(loss=loss.item(), accuracy= accuracy.numpy())\n","            \n","    model.eval() # weight 업데이트 금지\n","    steps = 0\n","    accuarcy_list = []\n","    #for data in test_dataloader:\n","    with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n","        for data in tepoch:\n","            tepoch.set_description(f\"Epoch {i}\")\n","            x = data[0]\n","            target = data[1]\n","\n","            logit = model.forward(x)\n","            loss = F.cross_entropy(logit, target)\n","            corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n","            accuracy = 100.0 * corrects/x.size()[0]\n","            accuarcy_list.append(accuracy.tolist())\n","            \n","            tepoch.set_postfix(loss=loss.item(), accuracy= sum(accuarcy_list)/len(accuarcy_list))\n","    \n","    # epoch 당 검증 셋의 정확도를 계산하고 이전 정확도 보다 높으면 저장     \n","    acc = sum(accuarcy_list)/len(accuarcy_list)\n","    if(acc>prev_acc):\n","        prev_acc = acc\n","        save(model, save_dir, save_prefix+\"_\"+str(round(acc,3)), i)"],"metadata":{"id":"9X5PBveSYOsi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2.3 Load & Test"],"metadata":{"id":"9JNwSjWOYOuf"}},{"cell_type":"code","source":["model.load_state_dict(torch.load(\"./data/pretraining/save/1_intent_clsf_model/intent_clsf_97.217_steps_33.pt\"))\n","\n","model.eval()"],"metadata":{"id":"C_TCEKUyYOxt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","q = \"제주도 오늘 날씨 알려줘\"\n","\n","x = dataset.prep.pad_idx_sequencing(embed.query2idx(dataset.tokenize(q)))\n","\n","x = torch.tensor(x)\n","f = model(x.unsqueeze(0))\n","\n","intent = dataset.intents[torch.argmax(f).tolist()]\n","\n","print(\"발화 : \" + q)\n","print(\"의도 : \" + intent)"],"metadata":{"id":"wWrFUOVyYO0P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","# 3. Entity Recognition"],"metadata":{"id":"pReOobSkyjCT"}},{"cell_type":"markdown","source":["### 3.1 Data Processing"],"metadata":{"id":"ZMl3eqrIyjEr"}},{"cell_type":"code","source":["'''\n"," PyTorch의 TensorDataset은 기본적으로 x[index], y[index]를 제공합니다.\n"," 그 외에 추가로 제공하고 싶은게(문장 길이) 있으면 아래와 같이 커스텀이 가능합니다.\n"," 여기서는 입력되는 문장의 길이를 제공 받아야해서 아래와 같이 커스텀을 하였습니다.\n","'''\n","class EntityDataset(data.Dataset):\n","    def __init__(self, x_tensor, y_tensor, lengths):\n","        super(EntityDataset, self).__init__()\n","\n","        self.x = x_tensor\n","        self.y = y_tensor\n","        self.lengths = lengths\n","        \n","    def __getitem__(self, index):\n","        return self.x[index], self.y[index], self.lengths[index]\n","\n","    def __len__(self):\n","        return len(self.x)\n","    \n","class MakeDataset:\n","    def __init__(self):\n","        \n","        self.entity_label_dir = drive_project_root+\"data/dataset/entity_label.json\"\n","        self.entity_data_dir = drive_project_root+\"data/dataset/entity_data.csv\"\n","        \n","        self.entity_label = self.load_entity_label()\n","        self.prep = Preprocessing()\n","        \n","    def load_entity_label(self):\n","        f = open(self.entity_label_dir, encoding=\"UTF-8\")\n","        entity_label = json.loads(f.read())\n","        self.entitys = list(entity_label.keys())\n","        return entity_label\n","    \n","    \n","    def tokenize(self, sentence):\n","        return sentence.split()\n","    \n","    def tokenize_dataset(self, dataset):\n","        token_dataset = []\n","        for data in dataset:\n","            token_dataset.append(self.tokenize(data))\n","        return token_dataset\n","    \n","    def make_entity_dataset(self, embed):\n","        entity_dataset = pd.read_csv(self.entity_data_dir)\n","        entity_querys = self.tokenize_dataset(entity_dataset[\"question\"].tolist())\n","        labels = []\n","        for label in entity_dataset[\"label\"].to_list():\n","            temp = []\n","            for entity in label.split():\n","                temp.append(self.entity_label[entity])\n","            labels.append(temp)\n","        dataset = list(zip(entity_querys, labels))\n","        entity_train_dataset, entity_test_dataset = self.word2idx_dataset(dataset, embed)\n","        return entity_train_dataset, entity_test_dataset\n","    \n","    def word2idx_dataset(self, dataset ,embed, train_ratio = 0.8):\n","        embed_dataset = []\n","        question_list, label_list, lengths = [], [], []\n","        flag = True\n","        random.shuffle(dataset)\n","        \n","        # 발화와 label 모두 padding 필요\n","        # 발화와 label 길이가 같아야 하기 때문\n","        for query, label in dataset :\n","            q_vec = embed.query2idx(query)\n","            lengths.append(len(q_vec))\n","            \n","            q_vec = self.prep.pad_idx_sequencing(q_vec)\n","\n","            question_list.append(torch.tensor([q_vec]))\n","\n","            label = self.prep.pad_idx_sequencing(label) # \n","            label_list.append(label)\n","            flag = False\n","\n","\n","        x = torch.cat(question_list)\n","        y = torch.tensor(label_list)\n","\n","        x_len = x.size()[0]\n","        y_len = y.size()[0]\n","        if(x_len == y_len):\n","            train_size = int(x_len*train_ratio)\n","            \n","            train_x = x[:train_size]\n","            train_y = y[:train_size]\n","\n","            test_x = x[train_size+1:]\n","            test_y = y[train_size+1:]\n","\n","            train_length = lengths[:train_size]\n","            test_length = lengths[train_size+1:]\n","\n","            train_dataset = EntityDataset(train_x,train_y,train_length)\n","            test_dataset = EntityDataset(test_x,test_y,test_length)\n","            \n","            return train_dataset, test_dataset\n","            \n","        else:\n","            print(\"ERROR x!=y\")"],"metadata":{"id":"kUbE-lXRyjIF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = MakeDataset()"],"metadata":{"id":"T7O8wLo-yjLF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Inside, Out, Begin, End, Single\n","#IO :  TAG라면 I 을 아니면 O 로 태그\n","#BIO : TAG의 길이가 2이상이면 첫 번째 단어는 B를 붙이고 그 뒤의 단어들은 I를 붙인다\n","#BIOES : BIO에서 단어의 길이가 3이상인 단어는 마지막 단어에 E를 붙인다. 그리고 단어의 길이가 1이라면, S를 붙인다\n","# S : 단독\n","# B : 복합의 시작 (단독 사용 불가)\n","# I : 복합의 중간 (단독 사용 불가)\n","# E : 복합의 끝  (단독 사용 불가)\n","# O : 의미 없음\n","dataset.entity_label"],"metadata":{"id":"d4vS8_P6yjOD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["entity_dataset = pd.read_csv(dataset.entity_data_dir)\n","\n","entity_dataset.head()"],"metadata":{"id":"K7qCFzrPzeX4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- intent classification은 label이 balance 해야 했는데, 여기서는 한 단어의 tag가 무엇이냐가 중요해서 balance 필요 없음"],"metadata":{"id":"3Blyh9iR0ILq"}},{"cell_type":"code","source":["entity_dataset.groupby(['label']).count() "],"metadata":{"id":"Z7VtrOayzecX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.2 Bidirectional LSTM-CRF Models for Sequence Tagging\n","- Zhiheng Huang,Wei Xu,Kai Yu\n","- tensorflow code : https://github.com/ngoquanghuy99/POS-Tagging-BiLSTM-CRF/blob/main/model.py\n","- tensorflow, keras code : https://github.com/floydhub/named-entity-recognition-template/blob/master/ner.ipynb"],"metadata":{"id":"ABJPEkIUzeff"}},{"cell_type":"code","source":["class BiLSTM_CRF(nn.Module):\n","\n","    def __init__(self, w2v, tag_to_ix, hidden_dim, batch_size):\n","        super(BiLSTM_CRF, self).__init__()\n","        self.embedding_dim = w2v.size()[1]\n","        self.hidden_dim = hidden_dim\n","        self.vocab_size =  w2v.size()[0]\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","        self.batch_size = batch_size\n","        self.START_TAG = \"<START_TAG>\"\n","        self.STOP_TAG = \"<STOP_TAG>\"\n","        \n","        self.word_embeds = nn.Embedding(self.vocab_size+2, self.embedding_dim)\n","        self.word_embeds.weight[2:].data.copy_(w2v)\n","        #self.word_embeds.weight.requires_grad = False\n","        \n","        # LSTM 파라미터 정의\n","        # bidirectional : 양방향 LSTM\n","        # num_layers    : layer의 수\n","        # batch_first   : pytorch에서 LSTM 입력의 기본값은 (Length,batch,Hidden) 순서 이므로 (batch,Length,Hidden)로 바꿔주기 위함\n","        # nn.LSTM(input_size, hidden_size, batch_first, num_layers)\n","        # hidden_size = hidden_dim // 2 인 이유는 bidirectional = True 이기 떄문입니다.\n","        self.lstm = nn.LSTM(self.embedding_dim, hidden_dim // 2, batch_first=True, num_layers=1, bidirectional=True)\n","    \n","        # LSTM의 출력을 태그 공간으로 대응시킵니다.\n","        # LSTM 출력을 CRF 입력으로 받기 전에 fully connected neural network를 통과해야 함\n","        # CRF 입력이 <문장 길이, 출력 tag 수>이기 때문\n","        # = 출력 tag 공간으로 mapping 하기 위해 필요\n","        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n","    \n","        self.hidden = self.init_hidden()\n","        \n","        # 출력층의 규칙학습을 위한 CRF 세팅\n","        self.crf = CRF(self.tagset_size, batch_first=True)\n","        \n","    def init_hidden(self):#(h,c)\n","        return (torch.randn(2, self.batch_size, self.hidden_dim // 2),  # bidirectional이기 때문에 2\n","                torch.randn(2, self.batch_size, self.hidden_dim // 2))\n","\n","    def forward(self, sentence):\n","        # Bi-LSTM으로부터 배출 점수를 얻습니다.\n","        self.batch_size = sentence.size()[0]\n","        self.hidden = self.init_hidden()\n","        #(2,128,128),(2,128,128)\n","        embeds = self.word_embeds(sentence)\n","        #(128,20,300)\n","        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n","        #(128,20,256),((2,128,128),(2,128,128))\n","        lstm_feats = self.hidden2tag(lstm_out)#(batch, length,tagset_size)\n","        #(128,20,17)\n","        return lstm_feats\n","    \n","    def decode(self, logits, mask):\n","        \"\"\"\n","        Viterbi Decoding의 구현체입니다.\n","        CRF 레이어의 출력을 prediction으로 변형합니다.\n","        :param logits: 모델의 출력 (로짓)\n","        :param mask: 마스킹 벡터\n","        :return: 모델의 예측 (prediction)\n","        \n","        각 단어의 자리마다\n","          word 1의 태그 확률        |  word2의 태그 확률\n","         'O': 확률0,              | 'O': 확률A,\n","         'B-DATE': 확률1,         | 'B-DATE': 확률B\n","         'B-LOCATION': 확률2,     | 'B-LOCATION': 확률C,\n","         'B-PLACE': 확률3,        | 'B-PLACE': 확률D,  \n","         'B-RESTAURANT': 확률4,   | 'B-RESTAURANT': 확률E,  \n","         'E-DATE': 확률5,         | 'E-DATE': 확률F,   \n","         'E-LOCATION': 확률6,     | 'E-LOCATION': 확률G, \n","         'E-PLACE': 확률7,        | 'E-PLACE': 확률H,  \n","         'E-RESTAURANT': 확률8,   | 'E-RESTAURANT': 확률I, \n","         'I-DATE': 확률9,         | 'I-DATE': 확률J,    \n","         'I-RESTAURANT': 확률10,  | 'I-RESTAURANT': 확률K,\n","         'S-DATE': 확률11,        | 'S-DATE': 확률L,      \n","         'S-LOCATION': 확률12,    | 'S-LOCATION': 확률M,\n","         'S-PLACE': 확률13,       | 'S-PLACE': 확률N,  \n","         'S-RESTAURANT': 확률14,  | 'S-RESTAURANT': 확률O,\n","         '<START_TAG>': 확률15,   | '<START_TAG>': 확률P, \n","         '<STOP_TAG>': 확률15,    | '<STOP_TAG>': 확률Q,\n","         \n","         각각의 높은 확률을 뽑는 것은 보통의 딥러닝 방식으로 B단독이나 I단독, E단독같은 문제를 야기할 수 있습니다.\n","         태그들의 확률 값을 받아서 \n","         CRF는 태그들의 의존성을 학습할수 있어서 태그 시퀀스의 확률이 가장 높은 확률을 가지는 예측 시퀀스를 선택한다.\n","         그래서 B단독이나 I단독, E단독과 같은 문제를 없애줍니다.\n","         예를 들어 B-DATE, O 와 같은걸 출력하지 않습니다. (CRF는 S-DATE, O 라고 출력합니다.)\n","        \"\"\"\n","\n","        return self.crf.decode(logits, mask)\n","    \n","    def compute_loss(self, label, logits, mask):\n","        \"\"\"\n","        학습을 위한 total loss를 계산합니다.\n","        :param label: label\n","        :param logits: logits\n","        :param mask: mask vector\n","        :return: total loss\n","        \"\"\"\n","\n","        log_likelihood = self.crf(logits, label, mask=mask, reduction='mean')\n","        return - log_likelihood  # Negative log likelihood loss"],"metadata":{"id":"QK0JTr2dzeiT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embed = MakeEmbed()\n","embed.load_word2vec()\n","\n","entity_train_dataset, entity_test_dataset = dataset.make_entity_dataset(embed)\n","\n","train_dataloader = DataLoader(entity_train_dataset, batch_size=128, shuffle=True)\n","\n","test_dataloader = DataLoader(entity_test_dataset, batch_size=128, shuffle=True)"],"metadata":{"id":"yqLj1v3Qzek_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["entity_train_dataset.x"],"metadata":{"id":"FAq0Q8aKzent"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["entity_train_dataset.y"],"metadata":{"id":"B9drCKmw0TwD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["weights = embed.word2vec.wv.vectors\n","weights = torch.FloatTensor(weights)\n","\n","# entity_label : 우리가 tag할 entity 개수 (17개)\n","# 256 : hidden dimension\n","# 128 : batch\n","model = BiLSTM_CRF(weights, dataset.entity_label, 256, 128)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","model.train()"],"metadata":{"id":"iA4JG9nu0Tys"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.3 Training\n","- 현재 코드에서는 accuracy는 나오지 않음\n","- loss가 줄어들고 있으면 학습이 잘 되고 있는 것"],"metadata":{"id":"YarWC2r-0T1b"}},{"cell_type":"code","source":["epoch = 5\n","prev_acc = 0\n","save_dir = \"./data/pretraining/1_entity_recog_model/\"\n","save_prefix = \"entity_recog\"\n","for i in range(epoch):\n","    steps = 0\n","    model.train()\n","    #for data in train_dataloader:\n","    with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n","        for data in tepoch:\n","            tepoch.set_description(f\"Epoch {i}\")\n","            x = data[0]\n","            y = data[1]\n","            length = data[2]\n","            \n","            logits = model.forward(x)\n","            # CRF 할 때 문장 길이를 맞추기 위해 넣었던 padingd token은 굳이 필요없으니 그 부분을 마스킹하기위한 코드\n","            # 우리는 length값이 존재하여 length값을 이용해서 마스크를 생성해도 가능\n","            # 하지만 코드 간략화를 위해 pytorch에 where 함수를 이용해 마스크 생성\n","            # torch.where 함수 설명 : https://runebook.dev/ko/docs/pytorch/generated/torch.where\n","            mask = torch.where(x > 0, torch.tensor([1.]), torch.tensor([0.])).type(torch.uint8)\n","            loss = model.compute_loss(y,logits,mask)\n","            \n","            loss.backward()\n","            optimizer.step()\n","\n","            tepoch.set_postfix(loss=loss.item())\n","            \n","    model.eval()\n","    steps = 0\n","    accuarcy_list = []\n","    #for data in test_dataloader:\n","    with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n","        for data in tepoch:\n","            tepoch.set_description(f\"Epoch {i}\")\n","            x = data[0]\n","            y = data[1]\n","            length = data[2]\n","            \n","            mask = torch.where(x > 0, torch.tensor([1.]), torch.tensor([0.])).type(torch.uint8)\n","            logits = model.forward(x)\n","            \n","            predicts = model.decode(logits,mask)\n","            # decode함수를 통해 정확도 계산\n","            corrects = []\n","            for target, leng, predict in zip(y, length, predicts):\n","                corrects.append(target[:leng].tolist() == predict)\n","\n","            accuracy = 100.0 * sum(corrects)/len(corrects)\n","            accuarcy_list.append(accuracy)\n","            \n","            loss = model.compute_loss(y,logits,mask)\n","            tepoch.set_postfix(loss=loss.item(), accuracy= sum(accuarcy_list)/len(accuarcy_list))\n","            \n","        acc = sum(accuarcy_list)/len(accuarcy_list)\n","        if(acc>prev_acc):\n","            prev_acc = acc\n","            save(model, save_dir, save_prefix+\"_\"+str(round(acc,3)), i)"],"metadata":{"id":"oN2g2yR1zert"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.4 Load & Test"],"metadata":{"id":"sl7HB_0uyjQ0"}},{"cell_type":"code","source":["model.load_state_dict(torch.load(\"./data/pretraining/save/1_entity_recog_model/entity_recog_97.192_steps_7.pt\"))\n","\n","model.eval()"],"metadata":{"id":"Mg2-aGlXyjU7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","q = \"이번 주 날씨\"\n","x = dataset.prep.pad_idx_sequencing(embed.query2idx(dataset.tokenize(q)))\n","\n","x = torch.tensor(x)\n","f = model(x.unsqueeze(0))\n","\n","mask = torch.where(x > 0, torch.tensor([1.]), torch.tensor([0.])).type(torch.uint8)\n","\n","predict = model.decode(f,mask.view(1,-1))\n","\n","# S : 단독\n","# B : 복합의 시작\n","# I : 복합의 중간\n","# E : 복합의 끝\n","tag = [dataset.entitys[p] for p in predict[0]]\n","for i, j in zip(q.split(' '),tag):\n","    print(\"단어 : \"+i+\" , \"+\"태그 : \"+j)"],"metadata":{"id":"7cKy4ZrH2EFA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","q = \"나 내일 제주도 여행 가는데 미세먼지 알려줘\"\n","x = dataset.prep.pad_idx_sequencing(embed.query2idx(dataset.tokenize(q)))\n","\n","x = torch.tensor(x)\n","f = model(x.unsqueeze(0))\n","\n","mask = torch.where(x > 0, torch.tensor([1.]), torch.tensor([0.])).type(torch.uint8)\n","\n","predict = model.decode(f,mask.view(1,-1))\n","\n","# S : 단독\n","# B : 복합의 시작\n","# I : 복합의 중간\n","# E : 복합의 끝\n","tag = [dataset.entitys[p] for p in predict[0]]\n","for i, j in zip(q.split(' '),tag):\n","    print(\"단어 : \"+i+\" , \"+\"태그 : \"+j)"],"metadata":{"id":"wGVMcvqR2EIE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","# 4. OOD (Out of Domain) 분류\n","\n","### Deep Averaging Network\n","- 단어 embedding의 평균을 내고 linear layer에 들어가서 문장 embedding 생성\n","- 정확도가 살짝 낮아도 모델이 가벼워서 학습 속도가 매우 빠름\n","  - Transformer는 layer가 많아서 계산량이 많다 보니 속도가 좀 느림\n"],"metadata":{"id":"m7Lp8CXQ2Yk3"}},{"cell_type":"markdown","source":["### 4.1 Data Processing"],"metadata":{"id":"TUkfNi6Q2YoB"}},{"cell_type":"code","source":["\n","\n","class MakeDataset:\n","    def __init__(self):\n","        \n","        self.intent_ood_label_dir = \"./data/dataset/intent_label_with_ood.json\"\n","        self.intent_data_dir = \"./data/dataset/intent_data.csv\"\n","        self.ood_data_dir = \"./data/dataset/ood_data.csv\"\n","        \n","        self.intent_ood_label = self.load_intent_ood_label()\n","        self.prep = Preprocessing()\n","    \n","    def load_intent_ood_label(self):\n","        f = open(self.intent_ood_label_dir, encoding=\"UTF-8\")\n","        intent_ood_label = json.loads(f.read())\n","        self.intents_ood = list(intent_ood_label.keys())\n","        return intent_ood_label\n","    \n","    def tokenize(self, sentence):\n","        return sentence.split()\n","    \n","    def tokenize_dataset(self, dataset):\n","        token_dataset = []\n","        for data in dataset:\n","            token_dataset.append(self.tokenize(data))\n","        return token_dataset\n","\n","    def make_ood_dataset(self, embed):\n","        intent_dataset = pd.read_csv(self.intent_data_dir)\n","        ood_dataset = pd.read_csv(self.ood_data_dir)#.sample(frac=1).reset_index(drop=True)\n","        intent_dataset = pd.concat([intent_dataset,ood_dataset])\n","        labels = []\n","        for label in intent_dataset[\"label\"].to_list():\n","            if(label == \"OOD\"):\n","                labels.append(0)\n","            else:\n","                labels.append(1)\n","            \n","        intent_querys = self.tokenize_dataset(intent_dataset[\"question\"].tolist())\n","        \n","        dataset = list(zip(intent_querys, labels))\n","        intent_train_dataset, intent_test_dataset = self.word2idx_dataset(dataset, embed)\n","        return intent_train_dataset, intent_test_dataset\n","    \n","    \n","    def word2idx_dataset(self, dataset ,embed, train_ratio = 0.8):\n","        embed_dataset = []\n","        question_list, label_list = [], []\n","        flag = True\n","        random.shuffle(dataset)\n","        for query, label in dataset :\n","            q_vec = embed.query2idx(query)\n","            q_vec = self.prep.pad_idx_sequencing(q_vec)\n","\n","            question_list.append(torch.tensor([q_vec]))\n","\n","            label_list.append(torch.tensor([label]))\n","\n","        x = torch.cat(question_list)\n","        y = torch.cat(label_list)\n","\n","        x_len = x.size()[0]\n","        y_len = y.size()[0]\n","        if(x_len == y_len):\n","            train_size = int(x_len*train_ratio)\n","            \n","            train_x = x[:train_size]\n","            train_y = y[:train_size]\n","\n","            test_x = x[train_size+1:]\n","            test_y = y[train_size+1:]\n","            \n","            train_dataset = TensorDataset(train_x,train_y)\n","            test_dataset = TensorDataset(test_x,test_y)\n","            \n","            return train_dataset, test_dataset\n","            \n","        else:\n","            print(\"ERROR x!=y\")\n","            \n"],"metadata":{"id":"J5QVEjt52Yqi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = MakeDataset()"],"metadata":{"id":"Tt543BCB2YtJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.read_csv(dataset.ood_data_dir)"],"metadata":{"id":"7H1sYBwa2Yvg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.read_csv(dataset.intent_data_dir)"],"metadata":{"id":"6c5PBj7L2YyA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embed = MakeEmbed()\n","embed.load_word2vec()\n","\n","batch_size = 128\n","\n","ood_train_dataset, ood_test_dataset = dataset.make_ood_dataset(embed)\n","\n","train_dataloader = DataLoader(ood_train_dataset, batch_size=batch_size, shuffle=True)\n","\n","test_dataloader = DataLoader(ood_test_dataset, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"aqV_I1nU3GEb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.2 Deep Unordered Composition Rivals Syntactic Methods for Text Classification\n","- Iyyer\n","- tensorflow code(tf-hub로 제공) : https://tfhub.dev/google/universal-sentence-encoder/4\n","- keras code : https://github.com/candlewill/Vecamend-master2/blob/master/dan.py#L41"],"metadata":{"id":"BfK7N1gD3GIt"}},{"cell_type":"code","source":["class DAN(nn.Module):\n","    \n","    def __init__(self, w2v, dim, dropout, num_class = 2):\n","        super(DAN, self).__init__()\n","        #load pretrained embedding in embedding layer.\n","        vocab_size = w2v.size()[0]\n","        emb_dim = w2v.size()[1]\n","        self.embed = nn.Embedding(vocab_size+2, emb_dim)\n","        self.embed.weight[2:].data.copy_(w2v)\n","        #self.embed.weight.requires_grad = False\n","        \n","        self.dropout1 = nn.Dropout(dropout)\n","        self.bn1 = nn.BatchNorm1d(emb_dim)\n","        self.fc1 = nn.Linear(emb_dim, dim)\n","        self.dropout2 = nn.Dropout(dropout)\n","        self.bn2 = nn.BatchNorm1d(dim)\n","        self.fc2 = nn.Linear(dim, num_class)\n","        \n","    def forward(self, x):\n","        emb_x = self.embed(x)\n","        #(128,20,300)\n","        x = emb_x.mean(dim=1)\n","        #(128,300)\n","        x = self.dropout1(x)\n","        x = self.bn1(x)\n","        x = self.fc1(x)\n","        #(128,256)\n","        x = self.dropout2(x)\n","        x = self.bn2(x)\n","        logit = self.fc2(x)\n","        #(128,2 )\n","        return logit"],"metadata":{"id":"t7JIQyFi3GSy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["weights = embed.word2vec.wv.vectors\n","weights = torch.FloatTensor(weights)\n","\n","model = DAN(weights, 256, 0.5, 2)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","model.train()"],"metadata":{"id":"rtrT6aLv3GVt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.3 Training"],"metadata":{"id":"TeEMpKHL3Zqn"}},{"cell_type":"code","source":["epoch = 5\n","prev_acc = 0\n","save_dir = drive_project_root+\"/data/pretraining/1_ood_clsf_model//\"\n","save_prefix = \"ood_clsf\"\n","for i in range(epoch):\n","    steps = 0\n","    model.train()\n","    #for data in train_dataloader:\n","    with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n","        for data in tepoch:\n","            tepoch.set_description(f\"Epoch {i}\")\n","            x = data[0]\n","            target = data[1]\n","            logit = model.forward(x)\n","            optimizer.zero_grad()\n","            loss = F.cross_entropy(logit, target)\n","            loss.backward()\n","            optimizer.step()\n","\n","            corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n","            accuracy = 100.0 * corrects/x.size()[0]\n","            tepoch.set_postfix(loss=loss.item(), accuracy= accuracy.numpy())\n","            \n","    model.eval()\n","    steps = 0\n","    accuarcy_list = []\n","    # 현재 accuracy가 이전 accuracy보다 높으면 저장\n","    #for data in test_dataloader:\n","    with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n","        for data in tepoch:\n","            tepoch.set_description(f\"Epoch {i}\")\n","            x = data[0]\n","            target = data[1]\n","\n","            logit = model.forward(x)\n","            loss = F.cross_entropy(logit, target)\n","            corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n","            accuracy = 100.0 * corrects/x.size()[0]\n","            accuarcy_list.append(accuracy.tolist())\n","            \n","            tepoch.set_postfix(loss=loss.item(), accuracy= sum(accuarcy_list)/len(accuarcy_list))\n","            \n","    acc = sum(accuarcy_list)/len(accuarcy_list)\n","    if(acc>prev_acc):\n","        prev_acc = acc\n","        save(model, save_dir, save_prefix+\"_\"+str(round(acc,3)), i)"],"metadata":{"id":"MAwNnv6N3ZtC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.4 Load & Test"],"metadata":{"id":"hiI_w_vm3Zvk"}},{"cell_type":"code","source":["model.load_state_dict(torch.load(\"./data/pretraining/save/1_ood_clsf_model/ood_clsf_99.724_steps_5.pt\"))\n","\n","model.eval()"],"metadata":{"id":"qJrpv2u03Zye"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- %%time : 이 cell이 돌 때 걸리는 시간"],"metadata":{"id":"wUrxbxzE5gIh"}},{"cell_type":"code","source":["%%time\n","q = \"제주도\"\n","\n","x = dataset.prep.pad_idx_sequencing(embed.query2idx(dataset.tokenize(q)))\n","\n","x = torch.tensor(x)\n","f = model(x.unsqueeze(0))\n","\n","y = torch.argmax(f).tolist()\n","\n","print(\"발화 : \" + q)\n","if(not y):\n","    print(\"ood\")\n","else:\n","    print(\"intent\")\n"],"metadata":{"id":"e3lRu6X33GYh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","q = \"제주도 날씨\"\n","\n","x = dataset.prep.pad_idx_sequencing(embed.query2idx(dataset.tokenize(q)))\n","\n","x = torch.tensor(x)\n","f = model(x.unsqueeze(0))\n","\n","y = torch.argmax(f).tolist()\n","\n","print(\"발화 : \" + q)\n","if(not y):\n","    print(\"ood\")\n","else:\n","    print(\"intent\")\n"],"metadata":{"id":"QHPtdA_93Gbb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"BhIunl9H3Ghb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Bi0tNkuF2Y0d"},"execution_count":null,"outputs":[]}]}