{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part4_10_VAE.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"mount_file_id":"1Yp0yH2RjfScp9VM5ATexx5ttY00Ns4cs","authorship_tag":"ABX9TyPMWqe8OFPer3G+4pW0V/JM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Fashion Mnist DNN Tutorial [CNN & Multi-layer Perceptron (MLP)]\n","\n","Efficient Network\n","- https://github.com/lukemelas/EfficientNet-PyTorch\n","- image 시리즈 중 auto ML을 통해 효율적으로 구현한 것\n","\n","pytorchLightning & Hydra\n","- config_utils.py 파일 사용할 예정 : 해당 파일 import 하기\n","- 아래 코드에서 중복되는 부분은 모두 삭제 : class WarmupLR, class EarlyStopping, def softmax\n","- https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html\n","\n","외부 파일 가져오기 & requirements 설치"],"metadata":{"id":"M-WPUCURrOHF"}},{"cell_type":"code","source":["!pwd\n","\n","# mount\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import sys\n","\n","drive_project_root = '/content/drive/MyDrive/#fastcampus'\n","sys.path.append(drive_project_root)\n","\n","!ls"],"metadata":{"id":"nlOJHTqlrXqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["!pip install -r '/content/drive/MyDrive/#fastcampus/requirements.txt'"],"metadata":{"id":"4zXqMRJx_4sI"}},{"cell_type":"code","source":["!pip install -r '/content/drive/MyDrive/#fastcampus/requirements.txt'"],"metadata":{"id":"bhw1vAW3zuBE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install torch-optimizer"],"metadata":{"id":"htTM48WtVYBZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["wandb 오류 있을 때 : `wandb.flush`"],"metadata":{"id":"mVflxznMvrp3"}},{"cell_type":"code","source":["pip install wandb"],"metadata":{"id":"PLYu27ZkVasl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install omegaconf"],"metadata":{"id":"p_8AzO0Zkrbf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install efficientnet_pytorch"],"metadata":{"id":"YdNWgLlmYbHX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["https://pytorch.org/vision/stable/transforms.html\n","\n","★ OmegaConf\n","- https://omegaconf.readthedocs.io/en/2.1_branch/\n","- hyperparameter configuration을 관리하기 위한 open source library\n","- DictConfig : Dictionary 형태의 configuration\n","- Hydra도 omegaconf를 기반으로 만들어짐\n","  - Hydra는 무겁기 때문에 omegaconf를 먼저 거치고, hydra 사용"],"metadata":{"id":"eX4npp3cxT4p"}},{"cell_type":"markdown","source":["gpu 확인"],"metadata":{"id":"qWQbtu32WqXA"}},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","print(gpu_info)"],"metadata":{"id":"QBp8qPTCWuFN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install Hydra"],"metadata":{"id":"755H-R1HlEZA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install hydra-core"],"metadata":{"id":"r79gbQX-mGkX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip uninstall torchmetrics"],"metadata":{"id":"liRxI5U_AMeC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install torchmetrics==0.5"],"metadata":{"id":"M3q3tz-EAMmT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install pytorch-lightning"],"metadata":{"id":"oK4SoCyolwaq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 추상화를 하는 class -> 중복 코드를 없애기 위함\n","from abc import abstractmethod, ABC\n","\n","from typing import Optional, Dict, List, Union, Any, Optional, Iterable, Callable\n","from functools import partial\n","from collections import Counter, OrderedDict\n","from datetime import datetime\n","import random\n","import math\n","\n","import numpy as np\n","from tqdm import tqdm   # 진행율\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F     # relu 등 함수 모음\n","from torch.utils.data import DataLoader\n","import pytorch_lightning as pl\n","\n","# data & models\n","from torchvision.datasets import FashionMNIST\n","from torchvision import transforms    # feature engineering 전처리 작업 효율적으로 할 수 있게 도와줌\n","import torchvision.utils as vutils   # for 이미지 저장\n","\n","# For configuration\n","from omegaconf import OmegaConf, DictConfig\n","import hydra\n","from hydra.core.config_store import ConfigStore\n","\n","# For logger\n","from torch.utils.tensorboard import SummaryWriter\n","import wandb\n","os.environ[\"WANDB_START_METHOD\"] = \"thread\""],"metadata":{"id":"WC1nxqJKs72F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["강사가 미리 만들어놓은 코드가 들어있는 파일들"],"metadata":{"id":"JBbfROJWyvbG"}},{"cell_type":"code","source":["from data_utils import dataset_split\n","from config_utils import flatten_dict, register_config, configure_optimizers_from_cfg, get_loggers, get_callbacks"],"metadata":{"id":"2PDva3Bvw9_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Base 모델 정의"],"metadata":{"id":"8hve9NTN_OOT"}},{"cell_type":"code","source":["# Define Model\n","class BaseGenerativeModel(pl.LightningModule):\n","    def __init__(self, cfg: DictConfig):\n","        pl.LightningModule.__init__(self)\n","        self.cfg = cfg\n","    \n","    @abstractmethod\n","    def forward(self, x):\n","        raise NotImplementedError()\n","\n","    def sample_generate(self, *args, **kwargs):\n","        raise NotImplementedError()\n","    \n","    def loss_function(self, *args, **kwargs):\n","        pass\n","\n","    def configure_optimizers(self):\n","        self._optimizers, self._schedulers = configure_optimizers_from_cfg(\n","            self.cfg, self\n","        )\n","        return self._optimizers, self._schedulers\n","\n","    def training_step(self, batch, batch_idx):\n","        pass\n","    \n","    def validation_step(self, batch, batch_idx):\n","        pass\n","\n","class VanillaVAE(BaseGenerativeModel):\n","    def __init__(self, cfg: DictConfig):\n","        super.__init__(cfg)\n","        self.latent_dim = cfg.model.latent_dim\n","\n","        # mlp를 이용할 예정\n","        # mlp_module 부분을 꼭 mlp가 아니라 cnn 등 다른 거 사용하면서 실험해 볼 것\n","\n","        # define posterior (encoder) modules\n","        posterior_mlp_modules_list = []\n","        prev_dim = cfg.model.posterior.hidden_dims[0]\n","\n","        for h_dim in cfg.model.posterior.hidden_dims[1:]:\n","            posterior_mlp_modules_list.append(nn.Linear(prev_dim, h_dim))\n","            prev_dim = h_dim\n","\n","        # * : argument 하나씩 풀어서 입력\n","        self.posterior_mlp_modules = nn.Sequential(\n","            *posterior_mlp_modules_list\n","        )\n","\n","        # define latent encode\n","        self.posterior_mu = nn.Linear(\n","            cfg.model.posteriror.hidden_dims[-1], cfg.model.latent_dim\n","        )\n","\n","        self.posteriorlog_log_var = nn.Linear(\n","            cfg.model.posterior.hidden_dims[-1], cfg.model.latent_dim\n","        )\n","\n","        # define prior (decoder) modules\n","        prior_mlp_modules_list = []\n","        prev_dim = self.latent_dim\n","        for h_dim in cfg.model.prior.hidden_dims:\n","            prior_mlp_modules_list.append(nn.Linear(prev_dim, h_dim))\n","            prev_dim = h_dim\n","        self.prior_mlp_modules = nn.Sequential(*prior_mlp_modules_list)\n","\n","    def encode(self, input):\n","        hidden = self.posterior_mlp_modules(input)\n","        mu = self.posterior_mu(hidden)\n","        log_var = self.posterior_log_var(hidden)\n","        return mu, log_var\n","\n","    def reparameterize(self, mu, log_var):\n","        std = torch.exp(0.5 * log_var)\n","        epsilon = torch.randn_like(std)\n","        return mu + (epsilon * std)\n","\n","    def decode(self, z):\n","        result = self.prior_mlp_modules(z)\n","        return torch.sigmoid(result)\n","\n","    def forward(self, x):\n","        # 아직 코드 안 짰을 때 일단 이렇게 해 놓기\n","        # raise NotImplementedError()\n","\n","        # return reconstruction, mu, log_variance\n","        # -> 결과적으로 reconstruction, latent variable을 return 해야 함\n","\n","        # input : [batch, 1, 28, 28]\n","        input = x.view(-1, self.cfg.model.posterior.hidden_dims[0])\n","        mu, log_var = self.encode(input)\n","        z = self.reparameterize(mu, log_var)\n","        return self.decode(z), mu, log_var\n","\n","    def sample_generate(\n","        self,\n","        num_samples: int = 64,  # for training (*batch_size) or random sampling\n","        z: Optional[torch.Tensor] = None,  # for manual sample generation\n","        ):\n","            # raise NotImplementedError()\n","            if z is None:\n","                z = torch.randn(num_samples, self.latent_dim)  # num_samples : batch size\n","            else:\n","                num_samples = z.shape[0]\n","            assert z.shape[-1] == self.latent_dim\n","            z = z.to(self.device)     # GPU inference하면 어떻게 될 지 모르니 일단 device 태워 놓기\n","            samples = self.decode(z) \n","            return samples.view(num_samples, self.cfg.data.C, self.cfg.data.H, self.cfg.data.W)   # 이미 flatten 되어 있기 때문에 -1 할 필요 없음\n","    \n","    def loss_function(\n","        self, \n","        recons, \n","        real_img, \n","        mu, \n","        log_var, \n","        kld_weight, \n","        mode=\"train\"\n","        ) -> dict:  # dictionary 형태로 정의 : 여러 개 loss를 iter할 것이기 때문\n","            assert mode in [\"train\", \"val\", \"test\"]\n","            \n","            # reconstruction loss\n","            recons_loss = F.binary_cross_entropy(\n","                recons,                  # input  : mlp 모델은 항상 이 부분이 flatten 되어 있는 상태일 것임\n","                real_img.view(-1, self.cfg.model.prior.hidden_dims[-1]),    # target : 실제 정답 값 -> 따라서 얘도 flatten 해 줘야 함 (view(-1, ))\n","                reduction=\"sum\"\n","            )\n","\n","            # kld loss\n","            kld_loss = 0.5 * torch.sum(\n","                mu.pow(2) + log_var.exp() - log_var - 1\n","                )\n","            \n","            # summation\n","            loss = recons_loss + kld_weight * kld_loss\n","            loss_result = {\n","                # loss를 그냥 가져다 쓰면 너무 커서 시스템이 폭발하게 됨\n","                # 따라서 특히 generative 모델에서 VAE 생성할 때 전체 학습 데이터만큼 나눠주는 게 일반적\n","                f\"{mode}_loss\": loss / self.cfg.data[f\"num_{mode}_imgs\"],\n","                # reconstruction loss\n","                f\"{mode}_reconstruction_loss\": recons_loss / self.cfg.data[f\"num_{mode}_imgs\"],\n","                # kld loss\n","                f\"{mode}_kld_loss\": kld_loss / self.cfg.data[f\"num_{mode}_imgs\"],\n","            }\n","\n","            return loss_result\n","    \n","    def training_step(self, batch, batch_idx):\n","        real_img, labels = batch\n","        recons, mu, log_var = self.forward(real_img)\n","        loss_results = self.loss_function(\n","            recons,\n","            real_img,\n","            mu,\n","            log_var,\n","            kld_weight=self.cfg.model.kld_weight,\n","            mode=\"train\"\n","        )\n","\n","        loss_results[\"loss\"] = loss_results[\"train_loss\"]\n","        self.log_dict(loss_results)\n","        return loss_results\n","    \n","    def validation_step(self, batch, batch_idx):\n","        real_img, labels = batch\n","        recons, mu, log_var = self.forward(real_img)\n","        loss_results = self.loss_function(\n","            recons,\n","            real_img,\n","            mu,\n","            log_var,\n","            kld_weight=self.model.kld_weight,\n","            mode=\"val\"\n","        )\n","\n","        # loss_results[\"loss\"] = loss_results[\"val_loss\"]\n","        self.log_dict(loss_results)\n","\n","        # random sample_generate\n","        # 기존에 있는 test data가 아니라 random sample에서 생성하는 것\n","        sample_gens = self.sample_generate(real_img.shape[0])  # batch size 만큼 넣음\n","\n","        # image logging은 tensorboard, wandb 따로 해야 함\n","        # 여기에서는 wandb에 맞게 코딩할 예정\n","        # logger에 direct로 접근해서 logging할 예정 : logger.experiment\n","        # - wandb.logger, tensorboard summary writer와 같은 의미\n","        # - 두 개 이상을 동시에 하고 있으면 list 형태로 가져옴\n","        # - 0번째를 wandb라고 가정\n","        self.logger.experiment[0].log({\n","            \"inputs\": wandb.Image(real_img),\n","            # reconstruct는 flatten 된 형태 -> view(-1) 필요\n","            # -1 다음에는 channel, height, width 입력\n","            \"recons\": wandb.Image(recons.view(-1, self.cfg.data.C, self.cfg.data.H, self.cfg.data.W)),\n","            \"sample_gens\": wandb.Image(sample_gens), # sample generator\n","        })\n","\n","        return loss_results"],"metadata":{"id":"U6JdjBPk_ORe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Configuration Def"],"metadata":{"id":"-euIEprEIddh"}},{"cell_type":"code","source":["# data configs\n","data_fashion_mnist_cfg = {\n","    \"name\": \"fahion_mnist\",\n","    \"data_root\": os.path.join(os.getcwd(), \"data\"), # data 저장할 곳\n","    \"transforms\": [\n","        {\n","            \"name\": \"ToTensor\",\n","            # callable한 object를 keyword argument로 받기\n","            \"kwargs\": {}\n","        }\n","    ],\n","    \"W\": 28,\n","    \"H\": 28,\n","    \"C\": 1,  # 흑백\n","    \"n_class\": 10,\n","}\n","cfg = OmegaConf.create(data_fashion_mnist_cfg)\n","print(OmegaConf.to_yaml(cfg))\n","\n","# model configs\n","model_mnist_vanilla_vae_cfg = {\n","    \"name\": \"VanillaVAE\",\n","    \"latent_dim\": 4,\n","    # encoder\n","    \"posterior\": {\n","        \"hidden_dim\": [28*28, 512, 256]\n","    },\n","    # decoder\n","    \"prior\": {\n","        \"hidden_dim\": [256, 512, 28*28]\n","    },\n","    \"kld_weight\": 1,\n","}\n","\n","# optimizer configs\n","# Google \"torch optimizer\" : https://github.com/jettify/pytorch-optimizer\n","#    RAdam : https://github.com/jettify/pytorch-optimizer/blob/master/torch_optimizer/radam.py\n","#    -> 여기 def _init_() 부분 참고해서 작성\n","# config_utils.py 파일 보면 optimizer를 for loop으로 돌림\n","#   = optimizers를 list 형태로 여러 개 지정 가능\n","#   fine tuning, 모델 두 개 동시 학습시킬 때 등에 사용\n","opt_cfg = {\n","    \"optimizers\": [\n","        {\"name\": \"RAdam\",\n","         \"kwargs\": {\n","             \"lr\": 1e-3,\n","             \"betas\": (0.9, 0.999),\n","             \"eps\": 1e-8,\n","             \"weight_decay\": 0,\n","             }\n","         }\n","    ],\n","    \"lr_schedulers\": [\n","        {\"name\": None,  # lr_scheduler 안 쓰려면 none이라고 지정 (config_utils.py 참고)\n","         \"kwargs\": {}\n","         }\n","    ]\n","}\n","\n","_merged_cfg_presets = {\n","    \"vanilla_vae_fashion_mnist\": {\n","        \"data\": data_fashion_mnist_cfg,\n","        \"model\": model_mnist_vanilla_vae_cfg,\n","        \"opt\": opt_cfg,   # optimizer\n","    },\n","}\n"," \n","### hydra composition ###\n","\n","# clear hydra instance\n","# notebook에서 여러 번 실행하면 이상하게 나올 수 있어서 clear 먼저 해줄 것\n","# notebook 아니면 이렇게 할 필요 없음\n","hydra.core.global_hydra.GlobalHydra.instance().clear()  \n","\n","# register preset configs\n","register_config(_merged_cfg_presets)\n","\n","# initializing\n","hydra.initialize(config_path=None)\n","\n","# compose\n","cfg = hydra.compose(\"vanilla_vae_fashion_mnist\")  # 위에서 사용한 _merged_cfg_presets 키 가져오기\n","\n","####\n","\n","# override some cfg\n","run_name = f\"{datetime.now().isoformat(timespec='seconds')}-{cfg.model.name}-{cfg.data.name}\"\n","print(OmegaConf.to_yaml(cfg))\n","\n","## Define train configs\n","project_root_dir = os.path.join(\n","    drive_project_root, \"runs\", \"generative-dnn-tutorial_fashion_mnist_runs\"  # 현재 우리가 작업 중인 곳\n",")\n","\n","save_dir = os.path.join(project_root_dir, run_name)\n","run_root_dir = os.path.join(project_root_dir, run_name)\n","\n","# train configs\n","train_cfg = {\n","    \"train_batch_size\": 256,\n","    \"val_batch_size\": 64,\n","    \"test_batch_size\": 64,\n","    \"train_val_split\": [0.9, 0.1],\n","    \"run_root_dir\": run_root_dir,\n","    \"trainer_kwargs\": {\n","        \"accelerator\": \"dp\", # 하나의 gpu로 할 때는 dp로 하지만 multiple gpu인 경우 ddp 등 설정 가능\n","        \"gpus\": \"0\"  # -> 0번 gpu 사용하기\n","        # \"gpus\": None, # -> 일단 CPU로 확인해 보겠다 : CPU로 할 때 에러에 대한 설명이 더 친절함 (특히 pytorch)\n","        \"max_epochs\": 50,    \n","        # 1.0 : train epoch가 끝날 때 validation check을 하겠다\n","        # 0.5 : train epoch가 절반 돌았을 때 validation check 하겠다\n","        # integer인 경우 : 몇 step마다 돌 지 설정하는 것\n","        \"val_check_interval\": 1.0,\n","        \"log_every_n_steps\": 100,   # 100번 step마다 한다\n","        \"flush_logs_every_n_steps\": 100,\n","    },    \n","}\n","\n","\n","# logger configs\n","log_cfg = {\n","    \"loggers\": {\n","        \"WandbLogger\": { \n","            \"project\": \"fastcampus_generative_fashion_mnist_tutorials\",\n","            \"name\": run_name,\n","            \"tags\": [\"fastcampus_generative_fashion_mnist_tutorials\"],\n","            \"save_dir\": run_root_dir,\n","        },\n","        \"TensorBoardLogger\": {\n","            \"save_dir\": project_root_dir,\n","            \"name\": run_name,\n","            },\n","    },\n","    \"callbacks\": {\n","        \"ModelCheckpoint\": {\n","            \"save_top_k\": 3,\n","            \"monitor\": \"val_loss\",\n","            \"mode\": \"min\",\n","            \"verbose\": True,\n","            \"dirpath\": os.path.join(run_root_dir, \"weights\"),\n","            \"filename\": \"{epoch}-{val_loss:.3f}\"  # VAE는 accuracy가 없음\n","            },\n","        \"EarlyStopping\": {\n","            \"monitor\": \"val_loss\",\n","            \"mode\": \"min\",\n","            # 실제 generation 문제는 classification보다 훨씬 불안정한 경우가 많아서\n","            # patience 값을 10 정도로 더 크게 두거나\n","            # 아예 없애고 regularize를 추가하기도 함\n","            \"patience\": 3,\n","            \"verbose\": True,\n","            }\n","    }\n","}\n","\n","# unlock config & set train, log cfg\n","# 이 코드 없으면 에러 발생 : Key 'train' is not in struct\n","OmegaConf.set_struct(cfg, False)\n","\n","cfg.train = train_cfg\n","cfg.log = log_cfg\n","\n","# lock config\n","# OmegaConf.set_struct(cfg, True)\n","print(OmegaConf.to_yaml(cfg))"],"metadata":{"id":"T0sjg1W0IRSA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data & dataloader"],"metadata":{"id":"GsSyBeC_hZ2s"}},{"cell_type":"code","source":["# get transforms from torch.vision\n","def get_transform(cfg: DictConfig):\n","    transforms_list = []\n","    for tfm in cfg.data.transforms:\n","        if hasattr(transforms, tfm.name):\n","            # getattr(transforms, tfm.name)\n","            # torch vision에서 transforms의 이름이 tfm.name과 같으면 transforms에 있는 함수를 가져오라는 뜻\n","            transforms_list.append(\n","                getattr(transforms, tfm.name)(**tfm.kwargs)\n","            )\n","        else:\n","            raise ValueError(\n","                f\"Not supported transform {tfm} in torch.vision.transform\"\n","            )\n","\n","    return transforms.Compose(transforms_list)\n","\n","transform = get_transform(cfg)\n","\n","def get_datasets(cfg: DictConfig, download: bool = True) -> Dict[str, torch.utils.data.Dataset]:\n","    data_root = cfg.data.data_root\n","    datasets = {}\n","\n","    if cfg.data.name == \"fashion_mnist\":\n","        fashion_mnist_dataset = FashionMNIST(data_root, download=download, train=True, transform=transform)\n","        datasets = dataset_split(fashion_mnist_dataset, split=cfg.train.train_val_split)  # dictionary  형태\n","        datasets[\"test\"] = FashionMNIST(data_root, download=download, train=False, transform=transforms.ToTensor())    \n","    else:\n","        raise NotImplementedError(\"Not supported dataset yet\")\n","    return datasets\n","\n","datasets = get_datasets(cfg, download=True)  # 얼마 안 크니 download는 True로 함\n","\n","train_dataset = datasets[\"train\"]\n","val_dataset = datasets[\"val\"]\n","test_dataset = datasets[\"test\"]\n","\n","# save dataset size\n","cfg.data.num_train_imgs = len(datasets[\"train\"])\n","cfg.data.num_val_imgs = len(datasets[\"val\"])\n","cfg.data.num_test_imgs = len(datasets[\"test\"])\n","\n","# define dataloader\n","# batch 단위로 데이터를 묶을 예정\n","train_batch_size = cfg.train.train_batch_size\n","val_batch_size = cfg.train.val_batch_size\n","test_batch_size = cfg.train.test_batch_size\n","\n","# num_workers : 병렬 processing \n","# 1로 설정하는 경우 노트북 환경에서 dp, ddp의 pytorch가 에러나는 경우가 있어 일단 0으로 설정함\n","train_dataloader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=0\n",")\n","\n","val_dataloader = torch.utils.data.DataLoader(\n","    val_dataset, batch_size=val_batch_size, shuffle=False, num_workers=0\n",")\n","\n","test_dataloader = torch.utils.data.DataLoader(\n","    test_dataset, batch_size=val_batch_size, shuffle=False, num_workers=0\n",")"],"metadata":{"id":"Qhliz2d3hZ5j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 모델 선언 및 손실 함수, 최적화(optimizer) 정의, Tensorboard Logger 정의\n","\n","- 아래 셀 값들 조절해서 다른 모델 만들어 볼 수 있음"],"metadata":{"id":"F90LNgLP2ta5"}},{"cell_type":"markdown","source":["### define model"],"metadata":{"id":"16u_coLyhzc8"}},{"cell_type":"code","source":["# model define\n","\n","def get_pl_model(cfg: DictConfig, checkpoint_path: Optional[str] = None):\n","\n","    if cfg.model.name == 'VanillaVAE':\n","        model = VanillaVAE(cfg)\n","    else:\n","        raise NotImplementedError()\n","    \n","    if checkpoint_path is not None:\n","        model = model.load_from_checkpoint(cfg=cfg, checkpoint_path=checkpoint_path)\n","    return model\n","\n","model = get_pl_model(cfg)\n","# print(model)"],"metadata":{"id":"Pb43JfUSoz9I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### GPU setting"],"metadata":{"id":"TjwwBXD-uAFB"}},{"cell_type":"code","source":["# # gpu = None  # 코드 에러 날 때 cpu는 잘 작동하는지 확인하기 위해 gpu=None으로 설정\n","# gpu = 0   # gpu를 0번 쓰겠다는 의미"],"metadata":{"id":"2LIELBSBuBJ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logger = get_loggers(cfg)\n","callbacks = get_callbacks(cfg)\n","\n","trainer = pl.Trainer(\n","    callbacks=callbacks,\n","    logger=logger,\n","    default_root_dir=cfg.train.run_root_dir,\n","    num_sanity_val_steps=2,\n","    **cfg.train.trainer_kwargs\n",")"],"metadata":{"id":"nItlsYuDqhDs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%load_ext tensorboard\n","%tensorboard --logdir /content/drive/MyDrive/\\#fastcampus/runs/dnn-tutorial-fashion-mnist-run\n","\n","# pytorch lightning으로 학습\n","trainer.fit(model, train_dataloader, val_dataloader)"],"metadata":{"id":"TpO0z8mMrHrh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test"],"metadata":{"id":"53m-A6t2t5ZP"}},{"cell_type":"code","source":["ckpt_path = os.path.join(\n","    cfg.log.callbacks.ModelCheckpoint.dirpath,\n","    \"epoch-=8-val_loss=2.999.ckpt\"  # 파일 이름 그대로 가져오기\n",")\n","\n","model = get_pl_model(cfg, ckpt_path).eval()\n","print(model)"],"metadata":{"id":"5S9d-3a_t5rx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_interpolation_images(\n","        model,\n","        axis1=0,\n","        axis2=1,\n","        latent_dim=4,\n","        save_img_path=None,\n","        range1=np.arange(-2, 2, 0.2),\n","        range2=np.arange(-2, 2, 0.2),\n","    ):\n","        assert len(range1) == len(range2)\n","        z = []\n","        for i in range1:\n","            for j in range2:\n","                cur = [0. for _ in range(latent_dim)]\n","                cur[axis1] = i\n","                cur[axis2] = j\n","                z.append(cur)\n","        \n","        z = torch.Tensor(z)\n","        out = model.sample_generate(z=z)\n","        out = vutils.make_grid(out, nrow=len(range1))\n","\n","        if save_img_path is None:\n","            save_img_path = f\"interpolation_results_{axis1}vs{axis2}.png\"\n","        \n","        vutils.save_image(out, save_img_path)\n","\n","# 모든 경우의 수에 대해 생성\n","create_interpolation_images(model, 0, 1)\n","create_interpolation_images(model, 0, 2)\n","create_interpolation_images(model, 0, 3)\n","create_interpolation_images(model, 1, 2)\n","create_interpolation_images(model, 1, 3)\n","create_interpolation_images(model, 2, 3)"],"metadata":{"id":"FwF_NuWjuXOj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"WuhuVp-SjaPp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"lEZh37HOwPFr"},"execution_count":null,"outputs":[]}]}