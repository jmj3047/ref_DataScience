{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01.MLP.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"mount_file_id":"1Yp0yH2RjfScp9VM5ATexx5ttY00Ns4cs","authorship_tag":"ABX9TyMbK8Gf3NMIxC7wVIGWkPQS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Multi-layer Perceptron (MLP)\n","\n","외부 파일 가져오기 & requirements 설치"],"metadata":{"id":"M-WPUCURrOHF"}},{"cell_type":"code","source":["!pwd"],"metadata":{"id":"nlOJHTqlrXqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["mount하기"],"metadata":{"id":"NBVwXbp9sTOX"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"FX6LeILdsLqm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import sys\n","sys.path.append('/content/drive/MyDrive/#fastcampus')"],"metadata":{"id":"BhmPHKhOsgq2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"id":"emNw2m84ssvm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["!pip install -r '/content/drive/MyDrive/#fastcampus/requirements.txt'"],"metadata":{"id":"XLiVpLa8swr_"}},{"cell_type":"markdown","source":["https://pytorch.org/vision/stable/transforms.html"],"metadata":{"id":"eX4npp3cxT4p"}},{"cell_type":"code","source":["import numpy as np\n","from tqdm import tqdm   # 진행율\n","import matplotlib.pyplot as plt\n","import torch\n","from torch import nn\n","import torch.nn.functional as F # relu 등 함수 모음\n","from torch import optim\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from torchvision.datasets import FashionMNIST\n","from torchvision import transforms    # feature engineering 전처리 작업 효율적으로 할 수 있게 도와줌\n","from torch.utils.data import random_split  # dataset split"],"metadata":{"id":"WC1nxqJKs72F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["왼쪽 파일 부분 보면 'data' 라는 폴더가 새로 생겼고 FahionMNIST data download 된 것 볼 수 있음</br>\n","'raw' 파일 안에 압축 파일들 생성됨<br>\n","https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html\n","- 링크 들어가서 보면 getitem으로 정의되어 있는 데이터라는 것 알 수 있음\n","- 이런 dataset은 iterator가 아니기 때문에 list 같이 index를 통해 특정 value 바로 접근 가능\n","- iterator dataset은 next 함수 이용해서 접근 가능"],"metadata":{"id":"GycvOEvm37Un"}},{"cell_type":"code","source":["data_root = os.path.join(os.getcwd(), 'data')\n","\n","# 전처리 부분 (preprocessing) & 데이터 셀 출력\n","transform = transforms.Compose(\n","    [\n","     transforms.ToTensor(),\n","     transforms.Normalize([0.5], [0.5]), # mean, std\n","    ]\n",")\n","fashion_mnist_dataset = FashionMNIST(data_root, download=True, train=True, transform=transform)"],"metadata":{"id":"TYBdBoCiuWC2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["list처럼 특정 이미지 바로 접근하기\n","- normalize해서 다운받았기 때문에 어느 정도 normalize된 데이터"],"metadata":{"id":"ry5nP6sq5xuf"}},{"cell_type":"code","source":["fashion_mnist_dataset[0][0]"],"metadata":{"id":"1ptvG-uSw9rI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["첫 번째 데이터의 label"],"metadata":{"id":"bQUy-R7-5wqw"}},{"cell_type":"code","source":["fashion_mnist_dataset[0][1]"],"metadata":{"id":"OpWL-n_Kw9zg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["dataset split"],"metadata":{"id":"tNSmdPNNw92u"}},{"cell_type":"code","source":["dset = random_split(fashion_mnist_dataset, [int(len(fashion_mnist_dataset)*0.7), int(len(fashion_mnist_dataset)*0.3)])"],"metadata":{"id":"c4rEP9sxw95-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dset[0]"],"metadata":{"id":"OIcK5ccrw98R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["data_utils\n","- 강사가 미리 만들어놓은 코드"],"metadata":{"id":"JBbfROJWyvbG"}},{"cell_type":"code","source":["from data_utils import dataset_split"],"metadata":{"id":"2PDva3Bvw9_R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datasets = dataset_split(fashion_mnist_dataset, split=[0.9, 0.1])\n","print(datasets)"],"metadata":{"id":"6mJ5o4cVyOuE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["dataloader 정의"],"metadata":{"id":"9WOvaXA62p3x"}},{"cell_type":"code","source":["train_dataset = datasets[\"train\"]\n","val_dataset = datasets[\"val\"]\n","\n","# batch 단위로 데이터를 묶을 예정\n","train_batch_size = 100\n","val_batch_size = 10\n","\n","# num_workers : 병렬 processing\n","train_dataloader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=1\n",")\n","\n","val_dataloader = torch.utils.data.DataLoader(\n","    val_dataset, batch_size=val_batch_size, shuffle=False, num_workers=1\n",")"],"metadata":{"id":"XLr9B81AyYQz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for sample_batch in train_dataloader:\n","    print(sample_batch)\n","    # 그림과 label 각각에 대한 shape 확인\n","    print(sample_batch[0].shape, sample_batch[1].shape)\n","    break"],"metadata":{"id":"NuxnwfgXzDjY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 모델 정의 (Multi-layer Perceptron) (MLP)"],"metadata":{"id":"ThJn_qDv2tST"}},{"cell_type":"markdown","source":["pytorch model은 보통 class로 정의\n","- nn과 같은 module을 꼭 import 해야 함 ★ : GPU, CPU 간 꼬이는 것 방지"],"metadata":{"id":"m3jeaflV2tT_"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    # input, hidden layer1/2, output의 차원을 먼저 알아야 함\n","    def __init__(self, in_dim: int, h1_dim: int, h2_dim: int, out_dim: int):\n","        # nn.Module 가져오기\n","        super().__init__()\n","        # MLP layer를 각각 정의할 때 제일 처음 linear layer를 지나야 함\n","        self.linear1 = nn.Linear(in_dim, h1_dim)\n","        self.linear2 = nn.Linear(h1_dim, h2_dim)\n","        self.linear3 = nn.Linear(h2_dim, out_dim)\n","        self.relu = F.relu\n","    \n","    def forward(self, input):\n","        # 위에서 input shape : torch.Size([100, 1, 28, 28]) → 1×28×28\n","        x = torch.flatten(input, start_dim=1)\n","        x = self.relu(self.linear1(x))\n","        x = self.relu(self.linear2(x))\n","        out = self.linear3(x)\n","        # output에 sigmoid(binary)나 softmax(multi) 씌워서 출력해도 되고 출력 후 나중에 작업해도 됨\n","        # out = F.softmax(out)\n","        return out"],"metadata":{"id":"tafPABGv2tW6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 모델 선언 및 손실 함수, 최적화(optimizer) 정의, Tensorboard Logger 정의"],"metadata":{"id":"F90LNgLP2ta5"}},{"cell_type":"markdown","source":["아래 셀 값들 조절해서 다른 모델 만들어 볼 수 있음"],"metadata":{"id":"16u_coLyhzc8"}},{"cell_type":"code","source":["# define model\n","# input shape : 28*28\n","# hidden layer : 임의로 일단 128, 64로 정함 -> 수정 가능 (0.5배, 2배, 3배 등)\n","# output : label 0~9까지 있으니 10\n","model = MLP(28*28, 128, 64, 10)\n","\n","# define loss\n","loss_function = nn.CrossEntropyLoss()\n","\n","# define optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","max_epoch = 15\n","\n","# define tensorboard logger\n","writer = SummaryWriter()\n","log_interval = 100"],"metadata":{"id":"4CHBOnbp2teA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["아래 코드 돌릴 때는 새로 생성되었던 runs 파일 삭제하고 다시 돌려야 함"],"metadata":{"id":"Krq7LWk3gIxz"}},{"cell_type":"code","source":["%load_ext tensorboard\n","# runs라는 폴더를 만들고 그 안에 logging을 쌓을 예정\n","%tensorboard --logdir runs/\n","\n","train_step = 0\n","for epoch in range(1, max_epoch+1):\n","    \n","    # valid step\n","    # ★TIP : train step보다 먼저 해 보면 랜덤 input에 대해 얼마나 막장인지 미리 결과 확인해 볼 수 있으니 좋음\n","    # 단, validation data에 대해서는 optimizer를 하면 안 됨 -> cheating이니까\n","    # pytorch에서는 'torch.no_grad()'만 해도 optimizer 안 한 것으로 바꿔줌\n","    with torch.no_grad():\n","        val_loss = 0.0\n","        val_corrects = 0\n","\n","        # enumerate 안에 tqdm 해 주면 결과를 더 예쁘게 볼 수 있음\n","        # https://tqdm.github.io/docs/tqdm/\n","        # - position : Specify the line offset to print this bar (starting from 0) -> Useful to manage multiple bars at once\n","        #              ipynb은 자꾸 밑으로 내려가는데(?) 'position=0'으로 해야 가만히 있음 \n","        # - leave=True : keeps all traces of the progressbar upon termination of iteration\n","        # - desc : 진행바 앞에 text 출력\n","        for val_batch_idx, (val_images, val_labels) in enumerate(\n","            tqdm(val_dataloader, position=0, leave=True, desc=\"validation\")\n","            ):\n","            # forward\n","            val_outputs = model(val_images)\n","            _, val_preds = torch.max(val_outputs, 1)\n","\n","            # loss & acc\n","            # val_outputs.shape[0] : batch size\n","            val_loss += loss_function(val_outputs, val_labels) / val_outputs.shape[0]\n","            val_corrects += torch.sum(val_preds == val_labels.data) / val_outputs.shape[0]\n","            \n","    # valid step logging\n","    val_epoch_loss = val_loss / len(val_dataloader)\n","    val_epoch_acc = val_corrects / len(val_dataloader)\n","\n","    print(\n","        f\"{epoch} epoch, {train_step} step: val_loss: {val_epoch_loss}, val_acc: {val_epoch_acc}\"\n","    )\n","\n","    # tensorboard에 write\n","    # val_step 말고 train_step으로 해야 서로 같은 시점에서 비교하기 좋음\n","    writer.add_scalar('Loss/val', val_epoch_loss, train_step)\n","    writer.add_scalar('Acc/val', val_epoch_acc, train_step)\n","    writer.add_images('Images/val', val_images, train_step)\n","\n","    # ===============================================================\n","\n","    # train step\n","    for batch_idx, (images, labels) in enumerate(\n","        tqdm(train_dataloader, position=0, leave=True, desc='training')\n","        ):\n","\n","        current_loss = 0.0\n","        current_corrects = 0  # 몇 개나 맞췄는지\n","        \n","        # Forward ===================================================\n","        # get predictions\n","        outputs = model(images)\n","        _, preds = torch.max(outputs, 1)  # 1 : dimension\n","        # print(outputs)\n","        # print(preds)\n","\n","        # get loss\n","        # Cross Entropy : loss_function(input, target)\n","        loss = loss_function(outputs, labels)\n","        \n","        # Backpropagation ===========================================\n","        # optimizer 초기화 (zero화) : garbage 값이 있을 수도 있으니 처리해주기\n","        optimizer.zero_grad()\n","\n","        # Perform backward pass\n","        # Chain rule 자동 계산해 줌\n","        loss.backward()\n","\n","        # Perform Optimization\n","        optimizer.step()\n","\n","        current_loss += loss.item()\n","        current_corrects += torch.sum(preds == labels.data)\n","\n","        # 일정 이상 돌면 = 100번(log_interval)마다 정확도 평균 계산\n","        if train_step % log_interval == 0:\n","            train_loss = current_loss / log_interval\n","            train_acc = current_corrects / log_interval\n","\n","            print(\n","                f'{train_step}: train_loss: {train_loss}, train_acc: {train_acc}'\n","            )\n","\n","            # tensorboard에 write\n","            writer.add_scalar('Loss/train', train_loss, train_step)\n","            writer.add_scalar('Acc/train', train_acc, train_step)\n","            writer.add_images('Images/train', images, train_step)\n","            writer.add_graph(model, images)\n","\n","            # loss 초기화\n","            current_loss = 0\n","            current_corrects= 0\n","\n","        train_step += 1\n","        # break\n","    # break"],"metadata":{"id":"2Ferpwec7a-F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## save model\n","\n","- ckpt : checkpoint"],"metadata":{"id":"0wLLsasjiVx7"}},{"cell_type":"code","source":["os.makedirs('./logs/models', exist_ok=True)\n","torch.save(model, './logs/models/mlp.ckpt')"],"metadata":{"id":"VLw2mu7a8Mze"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["load model\n","\n","- eval(expression) : expression을 문자열로 받아 실행"],"metadata":{"id":"CrGdVCUda1aH"}},{"cell_type":"code","source":["loaded_model = torch.load('./logs/models/mlp.ckpt')\n","loaded_model.eval()\n","print(loaded_model)  # 잘 load 되었는지 확인"],"metadata":{"id":"YxhfY-u8a1dn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## softmax"],"metadata":{"id":"71UCgFtffPRF"}},{"cell_type":"code","source":["def softmax(x, axis=0):\n","    'numpy softmax'\n","    max = np.max(x, axis=axis, keepdims=True)\n","    e_x = np.exp(x - max)\n","    sum = np.sum(e_x, axis=axis, keepdims=True)\n","    f_x = e_x / sum\n","    return f_x"],"metadata":{"id":"RmiXm9w7bABH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## test model"],"metadata":{"id":"liGHGIJWfSOK"}},{"cell_type":"markdown","source":["torch.tensor\n","- It has an additional \"computational graph\" layer\n","- To convert a `torch.tensor` to `np.ndarray`, you must explicitly remove the computational graph of the tensor using the ★ `detach()` command\n","\n","★ 분석을 할 때는 마지막에 다 numpy로 바꿔줘야 scikit learn 등 여러 가지 tool에 쉽고 유연하게 적용이 가능하기 때문에 아래와 같이 대부분 마지막에는 numpy로 바꿔주고 끝냄"],"metadata":{"id":"pd3tepWWdTgx"}},{"cell_type":"code","source":["test_batch_size = 100\n","test_dataset = FashionMNIST(data_root, download=True, train=False, transform=transforms.ToTensor())\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=1)\n","\n","test_labels_list = []\n","test_preds_list = []\n","test_outputs_list = []\n","\n","for i, (test_images, test_labels) in enumerate(tqdm(test_dataloader, position=0, leave=True, desc='testing')):\n","    # forward\n","    test_outputs = loaded_model(test_images)\n","    _, test_preds = torch.max(test_outputs, 1)\n","\n","    final_outs = softmax(test_outputs.detach().numpy(), axis=1)\n","    test_outputs_list.extend(final_outs)\n","    test_preds_list.extend(test_preds.detach().numpy())\n","    test_labels_list.extend(test_labels.detach().numpy())\n","\n","# accuracy를 구하기 위해 numpy array로 변경\n","test_preds_list = np.array(test_preds_list)\n","test_labels_list = np.array(test_labels_list)\n","\n","print(f'\\nacc: {np.mean(test_preds_list == test_labels_list)*100}%')"],"metadata":{"id":"jZEUnSE1cGVV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ROC curve\n","\n","- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html"],"metadata":{"id":"ZmfIQKhWfD_o"}},{"cell_type":"code","source":["from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score\n","\n","fpr = {}\n","tpr = {}\n","thresh = {}  # threshold\n","n_class = 10\n","\n","for i in range(n_class):\n","    # [:, i] : batch size는 유지하고 각 class마다 확인\n","    fpr[i], tpr[i], thresh[i] = roc_curve(test_labels_list, np.array(test_outputs_list)[:, i], pos_label=i)"],"metadata":{"id":"91RPOZE_fXZm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["전체 test case에 대한 값"],"metadata":{"id":"uFxCXLUDisQF"}},{"cell_type":"code","source":["print(fpr)"],"metadata":{"id":"8H8phF44igCg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(n_class):\n","    plt.plot(fpr[i], tpr[i], linestyle='--', label=f'Class {i} vs Rest')\n","plt.title('Multi-class ROC Curve')\n","plt.xlabel('False Position Rate')\n","plt.ylabel('True Positive Rate')\n","plt.legend(loc='best')\n","plt.show()"],"metadata":{"id":"_ZjbBLVuilIQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n","\n","- ovo : Stands for One-vs-one. Computes the average AUC of all possible pairwise combinations of classes.\n","- macro : Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account."],"metadata":{"id":"qXg4j3kRjhSO"}},{"cell_type":"code","source":["print('auc_score ', roc_auc_score(test_labels_list, test_outputs_list, multi_class='ovo', average='macro'))"],"metadata":{"id":"ikzNIkLBivPx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"WuhuVp-SjaPp"},"execution_count":null,"outputs":[]}]}
