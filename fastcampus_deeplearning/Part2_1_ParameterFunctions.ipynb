{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1d70868",
   "metadata": {},
   "source": [
    "# Parameter Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c40f6a",
   "metadata": {},
   "source": [
    "# 1. Affine Functions with 1 Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f7ece4",
   "metadata": {},
   "source": [
    "## 1.1 Affine Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ea707c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e47f738",
   "metadata": {},
   "source": [
    "tf.constant\n",
    "- vector가 아닌 matrix 형태로 생성됨\n",
    "- ★ tensorflow는 기본적으로 floating point 32bit를 기준으로 작동함\n",
    "  - 숫자 작성할 때, floating point로 인지하게 하기 위해 반드시 .(dot) 붙이는 습관 들이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "651fccde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[10.]])\n",
    "print(x.shape)\n",
    "\n",
    "x = tf.constant([[10.],[20.]])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd882c6",
   "metadata": {},
   "source": [
    "### 1.1.1 input setting\n",
    "- ★ x값을 matrix 형태로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81aed34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[10.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a11ce52",
   "metadata": {},
   "source": [
    "### 1.1.2 Affine Function\n",
    "- ★ units=1 : Affine Function (Dense Layer) 하나 생성\n",
    "- ★ Dense(units=1)에서 weight, bias 값을 설정하지 않으면, 따로 생성되지 않음 (값이 없는 상태)\n",
    "  - W, B = dense.get_weights() 로 weight, bias 값 확인해 보려고 하면 Error 발생\n",
    "- activation\n",
    "  - Dense(units=1, activation='linear') → Default\n",
    "    - ★ sigmoid, tanh, relu에서는 output을 위한 추가 보정이 필요하지만, linear는 z 값 자체가 output이 됨\n",
    "  - Dense(units=1, activation='sigmoid')\n",
    "  - Dense(units=1, activation='tanh')\n",
    "  - Dense(units=1, activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3962bfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = Dense(units=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9500da20",
   "metadata": {},
   "source": [
    "### 1.1.3 forward propagation + parameter initialization\n",
    "- x 값을 forward propagation 통과시키기\n",
    "- Dense(units=1)에서 weight, bias 값을 설정하지 않았기 때문에\n",
    "  - ★ dense(x)와 같이 x값이 실제로 통과될 때 weight, bias 값 최초 생성됨 : parameter initialization\n",
    "- input 'x'에 대해 affine function의 'weight', 'bias' 값 대입해서 y값 연산\n",
    "  - z = x * w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81601faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-1.1352742]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_tf = dense(x)\n",
    "print(y_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234df73c",
   "metadata": {},
   "source": [
    "### 1.1.4 Get weight, bias\n",
    "- dense 안에 들어있는 weight와 bias 값 확인해 보기\n",
    "- matrix 형태로 출력됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db11d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11352742]] [0.]\n"
     ]
    }
   ],
   "source": [
    "W, B = dense.get_weights()\n",
    "print(W, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f2ede",
   "metadata": {},
   "source": [
    "### 1.1.5 forward propagation (manual)\n",
    "\n",
    "- tensorflow - linear algebra 함수를 이용해서 직접 계산해 보고, dense(x)의 계산 결과인 'y_tf'와 일치하는지 확인하기 위함\n",
    "  - dense(x) : z = x * weight + bias\n",
    "- matmul : X, weight가 matrix 형태이기 때문에 matrix multiplication 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7198a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_man = tf.linalg.matmul(x, W) + B\n",
    "print(y_man)\n",
    "print(y_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81badc8",
   "metadata": {},
   "source": [
    "### 1.1.6 Print Results\n",
    "- x : 1×1 matrix\n",
    "- W (weight) : 1×1 matrix\n",
    "- B (bias) : vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ed12d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Input/Weight/Bias =====\n",
      "x: (1, 1)\n",
      "[[10.]]\n",
      "\n",
      "W: (1, 1)\n",
      "[[-0.11352742]]\n",
      "\n",
      "B: (1,)\n",
      "[0.]\n",
      "\n",
      "===== Outputs =====\n",
      "y (Tensorflow): (1, 1)\n",
      "(1, 1)\n",
      "\n",
      "y (Manual): (1, 1)\n",
      "(1, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('===== Input/Weight/Bias =====')\n",
    "print('x: {}\\n{}\\n'.format(x.shape, x.numpy()))\n",
    "print('W: {}\\n{}\\n'.format(W.shape, W))\n",
    "print('B: {}\\n{}\\n'.format(B.shape, B))\n",
    "\n",
    "print('===== Outputs =====')\n",
    "print('y (Tensorflow): {}\\n{}\\n'.format(y_tf.shape, y_tf.shape))\n",
    "print('y (Manual): {}\\n{}\\n'.format(y_man.shape, y_man.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480047c2",
   "metadata": {},
   "source": [
    "## 1.2 Params Initialization\n",
    "\n",
    "- linear regression, logistic regression를 배울 때 많이 사용되는 방법\n",
    "  - 우리가 원하는 weight, bias 값의 방향을 설정하기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee401aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.initializers import Constant   # 랜덤 값이 아닌 원하는 값으로 initialization 하기 위함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba3f151",
   "metadata": {},
   "source": [
    "### 1.2.1 input setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0b4341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[10.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ccfbce",
   "metadata": {},
   "source": [
    "### 1.2.2 weight/bias setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0eb6e5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(10.0, shape=(), dtype=float32) tf.Tensor(20.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "w, b = tf.constant(10.), tf.constant(20.)\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5dad1a",
   "metadata": {},
   "source": [
    "### 1.2.3 w_init/b_init setting\n",
    "- layer 안에 있는 weight, bias를 초기화 시켜주기 위한 object\n",
    "- 실제 tensor 값을 가지고 있는 게 아님"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1856277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.initializers.initializers_v2.Constant object at 0x000001838B68C6D0> <keras.initializers.initializers_v2.Constant object at 0x000001838B606D30>\n"
     ]
    }
   ],
   "source": [
    "w_init, b_init = Constant(w), Constant(b)\n",
    "print(w_init, b_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8638084b",
   "metadata": {},
   "source": [
    "### 1.2.4 Affine Function\n",
    "\n",
    "- y_tf : 120. 나옴\n",
    "  - x (input) = 10, weight = 10, bias = 20\n",
    "  - 10 * 10 + 20 = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01283331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[120.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "dense = Dense(units=1,\n",
    "              activation='linear',\n",
    "              kernel_initializer=w_init,\n",
    "              bias_initializer=b_init)\n",
    "\n",
    "y_tf = dense(x)\n",
    "print(y_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb22eb3a",
   "metadata": {},
   "source": [
    "weight, bias 값 확인해 보니, 내가 원했던 10.과 20.으로 각각 초기화 되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01cdcf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: (1, 1)\n",
      "[[10.]]\n",
      "\n",
      "B: (1,)\n",
      "[20.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "W, B = dense.get_weights()\n",
    "\n",
    "print('W: {}\\n{}\\n'.format(W.shape, W))\n",
    "print('B: {}\\n{}\\n'.format(B.shape, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e931ca",
   "metadata": {},
   "source": [
    "# 2. Affine Functions with n Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78b2040",
   "metadata": {},
   "source": [
    "## 2.1 Affine Functions with n Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89296805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dd5e04",
   "metadata": {},
   "source": [
    "### 2.1.1 input setting\n",
    "- uniform distribution에서 가져오기\n",
    "- minval=0, maxval=10 : 0~10까지 숫자 중 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "214ee84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10) \n",
      " tf.Tensor(\n",
      "[[0.36864758 2.795496   4.4873357  3.2905495  9.087748   6.357126\n",
      "  8.873041   6.0415316  8.092084   5.4093957 ]], shape=(1, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.uniform(shape=(1,10), minval=0, maxval=10)\n",
    "print(x.shape, '\\n', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b0c946",
   "metadata": {},
   "source": [
    "### 2.1.2 Affine Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0aff9852",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = Dense(units=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f48f6",
   "metadata": {},
   "source": [
    "### 2.1.3 forward propagation\n",
    "\n",
    "- 위 Dense(units=1) 에서 weight, bias 값 따로 설정 안 해 줬으니\n",
    "  - dense(x)를 통과하면서 weight, bias 초기화 생성됨\n",
    "- x 값이 10개가 있으니, ★ weight 10개, bias 1개를 초기화 시켜줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e386ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tf = dense(x)\n",
    "\n",
    "W, B = dense.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae6c0e",
   "metadata": {},
   "source": [
    "★ z = x 값의 transpose * W + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ee9e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_man = tf.linalg.matmul(x, W) + B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202121d9",
   "metadata": {},
   "source": [
    "### 2.1.4 Print Results\n",
    "\n",
    "- ★ x 값이 10개가 있으니 weight는 10개 값을 가지고 있는 column vector로 생성됨\n",
    "- bias는 scalar 값 1개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19d2c7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Input/Weight/Bias =====\n",
      "x: (1, 10)\n",
      "[[0.36864758 2.795496   4.4873357  3.2905495  9.087748   6.357126\n",
      "  8.873041   6.0415316  8.092084   5.4093957 ]]\n",
      "\n",
      "W: (10, 1)\n",
      "[[ 0.34909564]\n",
      " [ 0.159612  ]\n",
      " [ 0.55341536]\n",
      " [ 0.14207762]\n",
      " [ 0.02079248]\n",
      " [-0.6888896 ]\n",
      " [-0.20484728]\n",
      " [-0.3683192 ]\n",
      " [ 0.13245553]\n",
      " [-0.11672592]]\n",
      "\n",
      "B: (1,)\n",
      "[0.]\n",
      "\n",
      "===== Outputs =====\n",
      "y (Tensorflow): (1, 1)\n",
      "(1, 1)\n",
      "\n",
      "y (Manual): (1, 1)\n",
      "(1, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('===== Input/Weight/Bias =====')\n",
    "print('x: {}\\n{}\\n'.format(x.shape, x.numpy()))\n",
    "print('W: {}\\n{}\\n'.format(W.shape, W))\n",
    "print('B: {}\\n{}\\n'.format(B.shape, B))\n",
    "\n",
    "print('===== Outputs =====')\n",
    "print('y (Tensorflow): {}\\n{}\\n'.format(y_tf.shape, y_tf.shape))\n",
    "print('y (Manual): {}\\n{}\\n'.format(y_man.shape, y_man.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174b77c1",
   "metadata": {},
   "source": [
    "# 3. Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3135a4ac",
   "metadata": {},
   "source": [
    "## 3.1 Activation Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ed32f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.math import exp, maximum\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "# input setting\n",
    "x = tf.random.normal(shape=(1,5))\n",
    "\n",
    "# activation function\n",
    "sigmoid = Activation('sigmoid')\n",
    "tanh = Activation('tanh')\n",
    "relu = Activation('relu')\n",
    "\n",
    "# forward propagation - Tensorflow\n",
    "y_sigmoid_tf = sigmoid(x)\n",
    "y_tanh_tf = tanh(x)\n",
    "y_relu_tf = relu(x)\n",
    "\n",
    "# forward propagation - manual\n",
    "y_sigmoid_man = 1 / (1 + exp(-x))\n",
    "y_tanh_man = (exp(x)-exp(-x)) / (exp(x)+exp(-x))\n",
    "y_relu_man = maximum(x,0)\n",
    "\n",
    "# print results\n",
    "print('x: {}\\n{}'.format(x.shape, x.numpy()))\n",
    "\n",
    "print('Sigmoid(Tensorflow): {}\\n{}'.format(y_sigmoid_tf.shape, y_sigmoid_tf.numpy()))\n",
    "print('Sigmoid(manual): {}\\n{}'.format(y_sigmoid_man.shape, y_sigmoid_man.numpy()))\n",
    "\n",
    "print('Tanh(Tensorflow): {}\\n{}'.format(y_tanh_tf.shape, y_tanh_tf.numpy()))\n",
    "print('Tanh(manual): {}\\n{}'.format(y_tanh_man.shape, y_tanh_man.numpy()))\n",
    "\n",
    "print('Relu(Tensorflow): {}\\n{}'.format(y_relu_tf.shape, y_relu_tf.numpy()))\n",
    "print('Relu(manual): {}\\n{}'.format(y_relu_man.shape, y_relu_man.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe8f8cf",
   "metadata": {},
   "source": [
    "## 3.2 Activation in Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d79c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.math import exp, maximum\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e912cdc6",
   "metadata": {},
   "source": [
    "### 3.2.1 Input Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f556786",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal(shape=(1,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042eb6b0",
   "metadata": {},
   "source": [
    "### 3.2.2 Artificial Neurons\n",
    "\n",
    "- Dense Layer에서 affine function을 만들고 그 뒤에 바로 activation function을 통과하도록 구현\n",
    "- affine function : z = x^T * w + b\n",
    "  - 현재 input 'x'는 5개 값이 있으니 affine function에서는 weight 5개, bias 1개 생성\n",
    "- activation function : sigmoid(z), tanh(z), relu(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d5c4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_sigmoid = Dense(units=1, activation='sigmoid')\n",
    "dense_tanh = Dense(units=1, activation='tanh')\n",
    "dense_relu = Dense(units=1, activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed637b72",
   "metadata": {},
   "source": [
    "### 3.2.3 Forward Propagation : Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ac9c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "y_sigmoid = dense_sigmoid(x)\n",
    "y_tanh = dense_tanh(x)\n",
    "y_relu = dense_relu(x)\n",
    "\n",
    "# Manual\n",
    "W, B = dense_sigmoid.get_weights()\n",
    "z = tf.linalg.matmul(x, W) + B\n",
    "a = 1 / (1 + exp(-z))\n",
    "\n",
    "# Tensorflow vs Manual : 결과값 같은지 비교 & 검증\n",
    "print('Activation value (Tensorflow): {}\\n{}'.format(y_sigmoid.shape, y_sigmoid.numpy()))\n",
    "print('Activation value (manual): {}\\n{}'.format(a.shape, a.numpy()))\n",
    "\n",
    "# Print Results\n",
    "print('AN with Sigmoid: {}\\n{}'.format(y_sigmoid.shape, y_sigmoid.numpy()))\n",
    "print('AN with Tanh: {}\\n{}'.format(y_sigmoid.shape, y_tanh.numpy()))\n",
    "print('AN with ReLU: {}\\n{}'.format(y_sigmoid.shape, y_relu.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522a8ba8",
   "metadata": {},
   "source": [
    "# 4. Artificial Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922bc7f5",
   "metadata": {},
   "source": [
    "## 4.1 Artificial Neurons\n",
    "\n",
    "- 1~3에서 배운 내용 정리 : 구현부터 검증까지\n",
    "- 평소 혼자 분석할 때도 이렇게 tensorflow를 사용할 때와 manually 작업했을 때 결과가 같게 나오는지 ★ 검증하기\n",
    "  - 실제로 내가 배운, 알고 있는 내용이 제대로 구현되고 있는지 확인\n",
    "  - 이렇게 이해를 하고 있어야 추가적인 연구 개발이 가능함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2491b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: sigmoid\n",
      "\n",
      "y_tf: (1, 1)\n",
      "[[0.4292091]]\n",
      "\n",
      "y_man: (1, 1)\n",
      "[[0.42920908]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.math import exp, maximum\n",
    "\n",
    "activation = 'sigmoid'\n",
    "#activation = 'tanh'\n",
    "#activation = 'relu'\n",
    "\n",
    "x = tf.random.uniform(shape=(1,10)) # input setting\n",
    "\n",
    "dense = Dense(units=1, activation=activation)  # affine + activation function\n",
    "\n",
    "y_tf = dense(x)\n",
    "W, B = dense.get_weights()\n",
    "\n",
    "# calculate activation vaue manually\n",
    "y_man = tf.linalg.matmul(x, W) + B\n",
    "\n",
    "if activation == 'sigmoid':\n",
    "    y_man = 1/(1+exp(-y_man))\n",
    "elif activation == 'tanh':\n",
    "    y_man = (exp(y_man) - exp(-y_man)) / (exp(y_man) + exp(-y_man))\n",
    "elif activation == 'relu':\n",
    "    y_man = maximum(x, 0)\n",
    "\n",
    "print('Activation: {}\\n'.format(activation))\n",
    "print('y_tf: {}\\n{}\\n'.format(y_tf.shape, y_tf.numpy()))\n",
    "print('y_man: {}\\n{}\\n'.format(y_man.shape, y_man.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12d242b",
   "metadata": {},
   "source": [
    "# 5. Minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ef2da",
   "metadata": {},
   "source": [
    "## 5.1 Shapes of Dense Layers\n",
    "\n",
    "- minibatch size 'N'은 절대 weight, bias 크기에 영향을 미치지 않음\n",
    "- feature size에 따라서만 weight 변화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16fea283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x:  (8, 10)\n",
      "Shape of W:  (10, 1)\n",
      "Shape of B:  (1,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "N, n_feature = 8, 10                        # set input params\n",
    "x = tf.random.normal(shape=(N, n_feature))  # generate minibatch\n",
    "\n",
    "dense = Dense(units=1, activation='relu')   # implement an Artifical Neurons\n",
    "y = dense(x)                                # forward propagation\n",
    "\n",
    "W, B = dense.get_weights()                  # get weight/bias\n",
    "\n",
    "# print results\n",
    "print('Shape of x: ', x.shape)            # matrix 형태\n",
    "print('Shape of W: ', W.shape)\n",
    "print('Shape of B: ', B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00447cb",
   "metadata": {},
   "source": [
    "## 5.2 Output Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8085d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output(Tensorflow): \n",
      " [[0.42787078]\n",
      " [0.52452624]\n",
      " [0.54379654]\n",
      " [0.43149492]\n",
      " [0.34651136]\n",
      " [0.79414344]\n",
      " [0.71111906]\n",
      " [0.5163546 ]]\n",
      "Output(manual)    : \n",
      " [[0.42787075]\n",
      " [0.5245262 ]\n",
      " [0.54379654]\n",
      " [0.43149492]\n",
      " [0.34651136]\n",
      " [0.7941434 ]\n",
      " [0.71111906]\n",
      " [0.5163546 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "N, n_feature = 8, 10                         # set input params\n",
    "x = tf.random.normal(shape=(N, n_feature))   # generate minibatch\n",
    "\n",
    "dense = Dense(units=1, activation='sigmoid') # implement an Artifical Neurons\n",
    "y_tf = dense(x)                              # forward propagation(Tensorflow)\n",
    "\n",
    "W, B = dense.get_weights()                   # get weight/bias\n",
    "\n",
    "y_man = tf.linalg.matmul(x,W) + B            # forward propagation(manual)\n",
    "y_man = 1/(1+tf.math.exp(-y_man))\n",
    "\n",
    "# print results\n",
    "print('Output(Tensorflow): \\n', y_tf.numpy())\n",
    "print('Output(manual)    : \\n', y_man.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f800f839",
   "metadata": {},
   "source": [
    "★ math.equal로 비교하는 방법은 비추\n",
    "- floating point가 짤리기도 하기 때문에, 사실 같은데 다른 것처럼 False 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5bb5df13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[False]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]], shape=(8, 1), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "print(tf.math.equal(y_tf, y_man))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b4fbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bbd644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4688ba2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0584773d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f6055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bef299a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017bcf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5305d17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e72aedc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
