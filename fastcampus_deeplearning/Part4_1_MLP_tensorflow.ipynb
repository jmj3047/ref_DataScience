{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02.MLP_tensorflow.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMIWkfWwjU8/UOsezfWFFY1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Tensorflow\n","- `Tensorflow`가 예전에는 쓰기 어려운 모델이었음 (코딩할 줄 아는 사람들만 사용)\n","- 그래서 `pytorch`가 많이 쓰이다 보니, `Tensorflow`에서도 쉽게 사용할 수 있는 `Keras` 만듦\n","- `Tensorflow 2.0`에서는 `keras`와 합쳐진 `tf.keras.Model`이나 `Sequential` 많이 사용\n","- `Tensorflow`에서 train step, test step을 사용하는 class 구조는 `pytorch lightening`과 비슷\n","  - `pytorch lightening` : `pytorch`를 더 쉽게 사용하기 위한 library"],"metadata":{"id":"8gmIaEsDowHC"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"bfadOzkbuIB4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import sys\n","sys.path.append('/content/drive/MyDrive/#fastcampus')\n","drive_project_root = '/content/drive/MyDrive/#fastcampus'\n","# !pip install -r '/content/drive/MyDrive/#fastcampus/requirements.txt'"],"metadata":{"id":"4AAPJkUPpFvb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datetime import datetime\n","\n","import numpy as np\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf"],"metadata":{"id":"wmIGtqQXpfJr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GPU 확인"],"metadata":{"id":"G8_oGfQKp2cT"}},{"cell_type":"code","source":["tf.config.list_physical_devices()"],"metadata":{"id":"NMr3t8-QpzX0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"L76O6F6_p3Ud"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["https://www.tensorflow.org/</br>\n","- https://www.tensorflow.org/overview/?hl=ko</br>\n","- 튜토리얼 : https://www.tensorflow.org/tutorials?hl=ko\n","- API > Tensorflow : 각 함수에 대한 설명\n","  - 구글에 'Tensorflow API 한글' 검색하면 번역본도 볼 수 있음\n","\n","초보자용 vs 전문가용\n","- 수업에서는 전문가용으로 할 예정\n","- 초보자용에서 사용하는 Sequential 버전(순차적으로 build 하는 방법)에는 한계가 있기 때문\n","- 실제 현업/연구에서는 Sequential 거의 안 씀"],"metadata":{"id":"eV6g35xTqKhF"}},{"cell_type":"markdown","source":["## define gpu\n","- https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy\n","- This strategy is typically used for training on one machine with multiple GPUs.\n","- 아래 코드 결과 보면 GPU 0번 잡아서 가져옴"],"metadata":{"id":"2lfmx_jAp5ac"}},{"cell_type":"code","source":["mirrored_strategy = tf.distribute.MirroredStrategy()"],"metadata":{"id":"N0sq47Enq1fU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## data, data loader 정의\n","\n","- 사실 tensorflow에서는 data loader를 정의를 안 하기도 함\n","- dataset으로 그냥 처리 가능\n","- 단, 여기서는 pytorch 방식과 비교하기 위해 사용함"],"metadata":{"id":"TQmPE2zdsfCx"}},{"cell_type":"code","source":["with mirrored_strategy.scope():\n","\n","    # dataset 정의 =====================================================================\n","    fashion_mnist = tf.keras.datasets.fashion_mnist\n","    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n","\n","    # normalization\n","    x_train = x_train / 255.0\n","    x_test = x_test / 255.0\n","\n","    # train/val splits\n","    train_size = int(len(x_train)*0.9)\n","    val_size = len(x_train) - train_size\n","\n","    # train, test dataset 정의\n","    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size=1024)\n","    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).shuffle(buffer_size=1024)\n","\n","    # train dataset을 train과 validation으로 나누기\n","    # tensorflow에서는 아래와 같이 take, skip 사용해서 데이터 많이 나눔\n","    train_dataset = dataset.take(train_size)   # train은 dataset에서 train_size만큼 take하고\n","    val_dataset = dataset.skip(train_size)     # val은 전체 dataset에서 train_size만큼 skip하고 남은 것\n","\n","    # 검증\n","    print(f'train total : {len(dataset)} (train : {len(train_dataset)}, validation : {len(val_dataset)})')\n","    print(f'test : {len(test_dataset)}')\n","\n","    # dataloader 정의 ==================================================================\n","    train_batch_size = 100\n","    val_batch_size = 10\n","    test_batch_size = 100\n","\n","    # drop_remainder=True : memory size가 안 맞으면 error 나는 것 방지 (pytorch는 이런 것 자동으로 처리함 = tensorflow와 차이점)\n","    train_dataloader = train_dataset.batch(train_batch_size, drop_remainder=True)\n","    val_dataloader = val_dataset.batch(val_batch_size, drop_remainder=True)\n","    test_dataloader = test_dataset.batch(test_batch_size, drop_remainder=True)"],"metadata":{"id":"_asj7gDkq1iE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_example = next(iter(train_dataloader))\n","print(sample_example)"],"metadata":{"id":"Zy_V0pfIu9tq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## plot figure"],"metadata":{"id":"qqD1A_X9q1lC"}},{"cell_type":"code","source":["plt.figure(figsize=(10,10))\n","for c in range(16):\n","    plt.subplot(4, 4, c+1)\n","    plt.imshow(x_train[c].reshape(28,28), cmap='gray')\n","plt.show()"],"metadata":{"id":"qZwUyCthq1oE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## make model"],"metadata":{"id":"L7L9eTkdq1qI"}},{"cell_type":"code","source":["class MLP(tf.keras.Model):\n","    def __init__(self, input_dim: int, h1_dim: int, h2_dim: int, out_dim: int):\n","        super().__init__()\n","\n","        # tensorflow는 pytorch와 다르게 flatten을 하지 않아도 되지만\n","        # pytorch와 비슷한 구조로 코딩하기 위해 여기서는 썼음\n","        self.flatten = tf.keras.layers.Flatten()\n","\n","        # tf nn module vs keras module\n","        # 1) nn module : tensorflow 1.0에서 사용, 기능이 조금 더 많음\n","        # 2) keras : tensorflow 2.0에서 사용\n","        self.linear1 = tf.keras.layers.Dense(input_dim=input_dim, units=h1_dim)\n","        # self.linear2 = tf.keras.layers.Dense(input_dim=h1_dim, units=h2_dim)\n","        # -> 이렇게 써도 되지만, pytorh보다 keras는 flexibility가 있어서 input_dim 생략해도 알아서 인지함\n","        self.linear2 = tf.keras.layers.Dense(units=h2_dim)\n","        self.linear3 = tf.keras.layers.Dense(units=out_dim)\n","        self.relu = tf.nn.relu\n","    \n","    # tensorflow에서는 'training=Fasle' 구문 꼭 넣기를 권장함\n","    # 나중에 regularization에서 drop out 할 때 이 부분을 조절할 수 있어야 함\n","    # - 학습일 때는 켜고, evaluation 때는 끄고\n","    def call(self, input, training=False):\n","        x = self.flatten(input)\n","        x = self.relu(self.linear1(x))\n","        x = self.relu(self.linear2(x))\n","        out = self.linear3(x)\n","        out = tf.nn.softmax(out)  # output을 확률값으로 바꿈\n","        return out\n","    \n","    # GradientTape() 구현\n","    # 따로 구현해도 되지만 class 안에 이렇게 넣어주면 나중에 더 코드가 깔끔해짐\n","    def train_step(self, data):\n","        # pass\n","        images, labels = data\n","        \n","        with tf.GradientTape() as tape:\n","            outputs = self(images, training=True)\n","            preds = tf.argmax(outputs, 1)\n","\n","            # 위에서 out이 softmax 안 거친 경우, 여기 넣을 때 softmax 처리 해줘야 함\n","            loss = self.compiled_loss(\n","                labels, outputs\n","            )\n","\n","        # compute gradients\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","\n","        # update weights\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","        \n","        # update the metrics\n","        self.compiled_metrics.update_state(labels, preds)\n","\n","        # return a dict mapping metrics names to current values\n","        logs = {m.name: m.result() for m in self.metrics}\n","        logs.update({\"loss\": loss})\n","        return logs\n","    \n","    def test_step(self, data):\n","        # pass\n","        images, labels = data\n","        outputs = self(images, training=False)\n","        preds = tf.argmax(outputs, 1)\n","        loss = self.compiled_loss(\n","            labels, outputs\n","        )\n","\n","        # update the metrics\n","        self.compiled_metrics.update_state(labels, preds)\n","\n","        # return a dict mapping metrics names to current values\n","        logs = {m.name: m.result() for m in self.metrics}\n","        logs.update({\"test_loss\": loss})\n","        return logs"],"metadata":{"id":"wn2XnFMFq1tW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## define model"],"metadata":{"id":"04O0P4wLPmC2"}},{"cell_type":"code","source":["n_class = 10\n","max_epoch = 50\n","\n","with mirrored_strategy.scope():\n","    # channel : rgb가 없고 gray니까 1\n","    model = MLP(28*28*1, 128, 64, n_class)  # *args\n","    model_name = type(model).__name__       # MLP\n","\n","    # define loss\n","    loss_function = tf.losses.SparseCategoricalCrossentropy()\n","\n","    # define optimizer\n","    lr = 1e-3\n","    optimizer = tf.optimizers.Adam(learning_rate=lr)\n","\n","    model.compile(\n","        loss = loss_function,\n","        optimizer = optimizer,\n","        metrics = [tf.keras.metrics.Accuracy()],\n","    )\n","\n","    # model build\n","    # 이 부분 생략해도 되지만 build를 해 놓으면 나중에 debugging하기 좋음 -> 권장\n","    # batch 1 : 임의로 설정\n","    model.build((1, 28*28*1))\n","\n","# 만약 build 안 하고 summary 하면 build, fit을 하거나 input shape를 넣으라고 경고 뜸\n","# fit은 학습이기 때문에 무거운 감이 있고 빠르게 하기 위해 build 선호\n","model.summary()"],"metadata":{"id":"P1kMS7uKq1wU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## define logging & callbacks"],"metadata":{"id":"rapd69GTRAuU"}},{"cell_type":"code","source":["log_interval = 100\n","run_name = f'{datetime.now()}-{model.name}'\n","\n","run_dirname = 'dnn-tutorial-fashion-mnist-runs-tf'\n","\n","# 경로에 'run'이라는 폴더를 만들고, run_dirname에 run_name 생성\n","log_dir = os.path.join(drive_project_root, \"runs\", run_dirname, run_name)"],"metadata":{"id":"qc5A36plRsaE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tb_callback = tf.keras.callbacks.TensorBoard(\n","    log_dir, update_freq=log_interval\n",")"],"metadata":{"id":"lSbWZOHRSsGW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["경로 잘 찾고 있는지 확인"],"metadata":{"id":"YGTecT2yVqp8"}},{"cell_type":"code","source":["! ls /content/drive/MyDrive/\\#fastcampus/runs/"],"metadata":{"id":"1u3ORwmUVijX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tensorboard load하기 : load extension\n","%load_ext tensorboard\n","\n","# 경로 지정 : terminal 문법이기 때문에 #을 # 그대로 인지하려면 앞에 '\\' 써줘야 함\n","%tensorboard --logdir /content/drive/MyDrive/\\#fastcampus/runs/\n","\n","model.fit(\n","    train_dataloader,\n","    validation_data=val_dataloader,\n","    epochs=max_epoch,\n","    callbacks=[tb_callback] # callback은 여러 개를 넣을 수 있기 때문에 list 형태로 지정함\n",")"],"metadata":{"id":"HubFa3A2Szm9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## model testing"],"metadata":{"id":"F9Tx9Zu5Tg5Y"}},{"cell_type":"code","source":["model.evaluate(test_dataloader)"],"metadata":{"id":"83w4bBslWGNw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["auc curve"],"metadata":{"id":"XnbF1YQ_WM1p"}},{"cell_type":"code","source":["test_labels_list = []\n","test_preds_list = []\n","test_outputs_list = []\n","\n","for i, (test_images, test_labels) in enumerate(tqdm(test_dataloader, position=0, leave=True, desc='testing')):\n","    with mirrored_strategy.scope():\n","        test_outputs = model(test_images)\n","    test_preds = tf.argmax(test_outputs, 1)\n","\n","    final_outs = test_outputs.numpy()\n","    test_outputs_list.extend(final_outs)\n","\n","    test_preds_list.extend(test_preds.numpy())\n","    test_labels_list.extend(test_labels.numpy())\n","\n","test_preds_list = np.array(test_preds_list)\n","test_labels_list = np.array(test_labels_list)\n","\n","test_accuracy = np.mean(test_preds_list == test_labels_list)\n","print(f'\\nacc: {test_accuracy*100}%')"],"metadata":{"id":"5K99rt_2WJ1O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["roc curve"],"metadata":{"id":"Ib1UnftTXUkw"}},{"cell_type":"code","source":["from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score\n","\n","fpr = {}\n","tpr = {}\n","thresh = {}\n","n_class = 10\n","\n","for i in range(n_class):\n","    fpr[i], tpr[i], thresh[i] = roc_curve(test_labels_list, np.array(test_outputs_list)[:, i], pos_label=i)"],"metadata":{"id":"1-guDbjmXhR3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fpr"],"metadata":{"id":"ncHDtbpCX8oJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["plot"],"metadata":{"id":"a32IhqbCYOrO"}},{"cell_type":"code","source":["for i in range(n_class):\n","    plt.plot(fpr[i], tpr[i], linestyle=\"--\", label=f\"Class {i} vs Rest\")\n","\n","plt.title(\"Multi-class ROC Curve\")\n","plt.xlabel(\"Flase Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.legend(loc=\"best\")\n","plt.show()"],"metadata":{"id":"r7tdD3yhYR9Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["auc score\n","- multi class이기 때문에 multi_class, average option 안 넣어주면 error 발생"],"metadata":{"id":"uglGAHtqY6rq"}},{"cell_type":"code","source":["auc_score = roc_auc_score(test_labels_list, test_outputs_list, multi_class=\"ovo\", average=\"macro\")"],"metadata":{"id":"vfISE8sBYoeY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'auc score : {auc_score*100}')"],"metadata":{"id":"Z01rKFr4Y2Uv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"2pWod1MIZILf"},"execution_count":null,"outputs":[]}]}