{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part6_2_이미지처리실습2.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPaxLTBYIms90AMdQaDIXwN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import sys\n","from glob import glob\n","\n","import math\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import cv2\n","from tensorflow import keras\n","from tensorflow.keras import optimizers\n","import tensorflow as tf\n","import tensorflow.keras.losses as losses\n","\n","import albumentations as A\n","\n","drive_project_root = 'drive/MyDrive/data/'\n","sys.path.append(drive_project_root)\n","\n","image_root = 'drive/MyDrive/data/images/'\n","anno_root = 'drive/MyDrive/data/annotations/'\n","\n","image_dir = image_root\n","bbox_dir = anno_root + 'xmls/'    # bounding box\n","seg_dir = anno_root + 'trimaps/'  # segmentation map\n","\n","os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n","\n","!ls"],"metadata":{"id":"OhVP0HFXC0ek"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install git+https://github.com/albumentations-team/albumentations.git"],"metadata":{"id":"wgI_MP69NddE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip uninstall opencv-python"],"metadata":{"id":"yl48uXYUNdff"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install opencv-python"],"metadata":{"id":"ug8t6xzSNdiZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. 사진 확인\n","- 1 : 사진에서 개, 고양이에 해당하는 부분 (object)\n","- 2 : 배경\n","- 3 : 둘 중 어디에도 속하지 않는 곳 (주로 object의 테두리)"],"metadata":{"id":"qei5El41NnI_"}},{"cell_type":"code","source":["image_files = glob(image_root+'*.jpg')\n","image_path = image_files[134]\n","\n","seg_path = image_path.replace('images', 'annotations/trimaps')\n","seg_path = seg_path.replace('jpg', 'png')\n","\n","image = cv2.imread(image_path)\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","seg_map = cv2.imread(seg_path, cv2.IMREAD_GRAYSCALE)  # grayscale로 읽기\n","\n","plt.figure(figsize=(10, 10))\n","plt.subplot(1, 2, 1)\n","plt.imshow(image)\n","plt.subplot(1, 2, 2)\n","plt.imshow(seg_map)\n","plt.show()"],"metadata":{"id":"FjAZqskzC0hI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["목표 : 사진에서 개, 고양이 부분만 예측하는 segmentation model 생성\n","- seg map에서 1이 아닌 값은 모두 0으로 바꾸기"],"metadata":{"id":"159iVZyeMyrP"}},{"cell_type":"code","source":["seg_map[seg_map != 1] = 0\n","\n","plt.figure(figsize=(10, 10))\n","plt.subplot(1, 2, 1)\n","plt.imshow(image)\n","plt.subplot(1, 2, 2)\n","plt.imshow(seg_map)\n","plt.show()"],"metadata":{"id":"vWHG85E7Mytu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","# 2. Loss 함수의 구현 : IoU / Dice coefficient\n","\n","- segmentation model 평가할 때 사용\n","- segmentation은 keras, tensorflow에서 공식적으로 제공하는 loss가 없어 직접 구현 필요"],"metadata":{"id":"9GR7thtcCrpJ"}},{"cell_type":"markdown","source":["### IoU\n","- 어차피 사람이 만든 bounding box도 정확하지 않아서 IoU 값이 0.7만 돼도 모델 성능이 좋다고 판단\n","- 0.9 이상이면 사람보다 잘한다고 판단\n","\n","### Dice\n","- F1 Score와 비슷한 개념"],"metadata":{"id":"3hY8lNbSCz2Z"}},{"cell_type":"markdown","source":["문제 : 두 mask가 주어졌을 때 겹치는 부분을 구하려면 각 pixel에 대해서 model이 제대로 예측을 했는지 알아야 함 (true/false)\n","- 방법 : [특정 threshold를 기준으로 틀리면 0, 맞으면 1] + [step function 사용]\n","- 한계 : step function은 미분 불가능해서 이 개념을 모델에 바로 사용하기는 어려움\n","- 해결책\n","  - 연속적인 함수로 만들기 위해, label의 0/1을 각 pixel이 특정 class에 속할 확률값으로 해석\n","  - 기존의 binary crossentropy를 사용할 때처럼 모델이 각 pixel을 얼마나 잘 분류했는지 알 수 있게 됨\n","  - 기존 '맞다/틀리다'에서 '일부분만 맞췄다'는 개념이 추가됨\n","  - = continuous한 함수가 됨\n","- 결론 : 학습에 iou, dice를 사용할 수 있게 됨"],"metadata":{"id":"8YSuqMXNRglu"}},{"cell_type":"markdown","source":["element wise 곱 + reduce_sum 예시\n","- y_true = [0, 1, 1, 0]\n","- y_pred = [0.5, 1, 0.2, 0.3]\n","- y_true \\* y_pred = [0 * 0.5, 1 * 1, 1 * 0.2, 0 * 0.3] = [0, 1, 0.2, 0]\n","- reduce_sum = 0 + 1 + 0.2 + 0 = 1.2\n","- 결론 : 두 개는 1.2 pixel이 겹친다"],"metadata":{"id":"Yk_QkpYhR-AK"}},{"cell_type":"markdown","source":["score & loss\n","- score : 서로 일치할수록 (성능이 좋을수록) 1, 나쁠수록 0에 가까운 값 갖게 됨\n","- loss : gradient descent를 하기 때문에 성능이 나쁠수록 loss가 커져야 함, 좋을수록 0에 가까운 값"],"metadata":{"id":"D14Dq2YRU8D5"}},{"cell_type":"code","source":["def iou(y_true, y_pred):\n","    \n","    # y_true가 전부 '0'으로 되어 있다면 union이 항상 '0'이 되니 마지막에 union으로 나눌 때 에러 발생\n","    # 이를 방지하기 위해 smooth 사용\n","    smooth = 1.\n","\n","    # Flatten\n","    # input은 2차원의 이미지인데 더 편리하게 계산하기 위해 보통 1차원 vector로 flatten함\n","    y_true = tf.reshape(y_true, [-1])\n","    y_pred = tf.reshape(y_pred, [-1])\n","\n","    # 두 tensor를 element-wise 곱셈을 하고 합을 구하면 총 몇 pixel이 겹치는지 알 수 있음\n","    intersection = tf.reduce_sum(y_true * y_pred)\n","\n","    # union : 라벨과 예측값의 pixel 개수 모두 더하고 교집합 부분 빼면 됨\n","    # sum으로 pixel 개수를 구할 수 있는 건 input 값이 0, 1 값으로만 이루어져있기 때문\n","    # but, y_pred는 0, 1로만 이루어지지 않음 = 'pixel 개수'라고 표현하는 게 맞지는 않음\n","    # -> 하지만 연속적인 함수로 만들어 주기 위해 어쩔 수 없는 부분\n","    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n","    \n","    score = intersection / (union + smooth)\n","    return score\n","\n","# dice coefficient : 2 * |X ∩ Y| / (|X| + |Y|)\n","def dice_coef(y_true, y_pred):\n","\n","    smooth = 1.\n","\n","    y_true = tf.reshape(y_true, [-1])\n","    y_pred = tf.reshape(y_pred, [-1])\n","\n","    intersection = tf.reduce_sum(y_true * y_pred)\n","    \n","    score = (2. * intersection) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n","\n","    return score\n","\n","def dice_loss(y_true, y_pred):\n","    loss = 1 - dice_coef(y_true, y_pred)\n","    return loss\n","\n","# binary crossentropy\n","# - 보통 loss 값 계산할 때 위처럼 dice loss를 단독으로 사용하지 않음\n","def bce_dice_loss(y_true, y_pred):\n","    # 보통 1:1 비율로 사용하는데 상황에 따라 10:1, 0.9:1 등으로 적용하기도 함\n","    loss = 1. * losses.binary_crossentropy(y_true, y_pred) + 1. * dice_loss(y_true, y_pred)\n","    return loss"],"metadata":{"id":"1EZ6GEfqMywY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 성능 확인"],"metadata":{"id":"Zze1tGelMyyy"}},{"cell_type":"code","source":["# 분모에 smooth라고 안전장치로 1을 더했기 때문에 이 코드의 결과가 1이 나오지는 않음\n","print('같은 값이 들어온 경우 (dice_coef): ', dice_coef(seg_map.astype('float32'), seg_map.astype('float32')).numpy())\n","print('같은 값이 들어온 경우 (iou)      : ', iou(seg_map.astype('float32'), seg_map.astype('float32')).numpy())\n","\n","# prediction으로 모두 0이 들어간 경우 -> 분자인 intersection이 전부 0이니 계산 결과 0 나옴\n","zeros = np.zeros_like(seg_map)\n","print('prediction이 0인 경우 (dice_coef): ', dice_coef(seg_map.astype('float32'), zeros.astype('float32')).numpy())\n","print('prediction이 0인 경우 (iou)      : ', iou(seg_map.astype('float32'), zeros.astype('float32')).numpy())\n","\n","# prediction으로 모두 1이 들어간 경우\n","# dice : 0.3949031 -> intersection을 2배 하니 가중치가 더 많이 들어가서 iou보다 큰 값 가짐\n","# iou : 0.24603069 -> 전체의 24.6% 정도만 겹친다\n","ones = np.ones_like(seg_map)\n","print('prediction이 1인 경우 (dice_coef): ', dice_coef(seg_map.astype('float32'), ones.astype('float32')).numpy())\n","print('prediction이 1인 경우 (iou)      : ', iou(seg_map.astype('float32'), ones.astype('float32')).numpy())"],"metadata":{"id":"Rlt6dXc-My1W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","# 3. Data Loader의 구현\n","\n","segmentation task\n","- 임의의 사진에 대해 개/고양이를 예측하고자 함 (품종 구분X)\n","- 사진에서 0(배경), 1(개/고양이), 2(테두리) 값 중 1의 부분을 예측"],"metadata":{"id":"sZ8AWUDtC0lB"}},{"cell_type":"code","source":["csv_path = drive_project_root+'kfolds.csv'\n","df = pd.read_csv(csv_path)\n","print(df[:10])\n","\n","idx = random.choice(range(len(df)))  # random하게 하나의 index 선택해서 image 출력하기 위함\n","\n","file_name = df.loc[idx].file_name\n","img_path = f'{image_root}{file_name}.jpg'\n","mask_path = f'{anno_root}trimaps/{file_name}.png'  # segmentation mask\n","\n","print('img_path: ', img_path)\n","print('mask_path: ', mask_path)\n","\n","img = cv2.imread(img_path)  # image 읽어오기\n","img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # mask는 grayscale로 저장되어 있음 -> 옵션 'IMREAD_GRAYSCALE' 입력 필요\n","\n","# img와 mask는 shape이 같아야 함\n","# 다만 img는 채널이 3개, mask는 채널이 1개\n","assert img.shape[:2] == mask.shape[:2]  # 가로, 세로 길이 일치여부 확인\n","\n","# image 시각화\n","plt.figure(figsize=(10, 10))\n","plt.subplot(1,2,1)\n","plt.imshow(img)\n","plt.subplot(1,2,2)\n","plt.imshow(mask)\n","plt.show()"],"metadata":{"id":"g0ms5kxEcFBS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.1 Augmentation"],"metadata":{"id":"aaWh3huaC0nr"}},{"cell_type":"code","source":["class Augmentation:\n","    def __init__(self, size, mode='train'):\n","        if mode == 'train':\n","            # Declare an augmentation pipeline\n","            self.transform = A.Compose([\n","                A.HorizontalFlip(p=0.5),  # 좌우반전\n","                A.ShiftScaleRotate(       # 상하좌우 이동 (shift) + 확대/축소 (scale) + 회전 (rotate)\n","                    p=0.5,\n","                    shift_limit=0.05,  # 이미지의 가로 길이가 최대 몇% 넘어가도 되는지\n","                    scale_limit=0.05,  # 최대 몇% 확대/축소할지\n","                    rotate_limit=15,   # 최대 몇 도 회전할지\n","                ),\n","                A.CoarseDropout(       # 이미지에 구멍을 뚫기\n","                    p=0.5,\n","                    max_holes=8,                 # 최대 구멍 개수\n","                    max_height=int(0.1 * size),  # 가로 최대 길이 : 이미지의 10%\n","                    max_width=int(0.1 * size),\n","                ),\n","                A.RandomBrightnessContrast(p=0.2),\n","            ])\n","\n","    def __call__(self, **kwargs):\n","        if self.transform:    # train mode인 경우\n","            augmented = self.transform(**kwargs)\n","            img = augmented['image']\n","            mask = augmented['mask']   # mask도 image와 동일한 rotate, scale 등을 적용하기 위해 추가\n","            return img, mask"],"metadata":{"id":"mkQH-dPuYOJB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.2 Data Generator"],"metadata":{"id":"VVEpYhMpYOQ3"}},{"cell_type":"code","source":["class DataGenerator(keras.utils.Sequence):\n","    def __init__(self, batch_size, csv_path, image_size,\n","                 fold, mode='train', shuffle=True):\n","        \n","        self.batch_size = batch_size\n","        self.image_size = image_size\n","        self.fold = fold\n","        self.mode = mode\n","        self.shuffle = shuffle\n","\n","        self.df = pd.read_csv(csv_path)\n","        \n","        if self.mode == 'train':\n","            self.df = self.df[self.df['fold'] != self.fold]\n","        elif self.mode == 'val':\n","            self.df = self.df[self.df['fold'] == self.fold]\n","        \n","        #### Remove invalid files\n","        #### https://github.com/tensorflow/models/issues/3134\n","        invalid_filenames = [\n","            'Egyptian_Mau_14',\n","            'Egyptian_Mau_139',\n","            'Egyptian_Mau_145',\n","            'Egyptian_Mau_156',\n","            'Egyptian_Mau_167',\n","            'Egyptian_Mau_177',\n","            'Egyptian_Mau_186',\n","            'Egyptian_Mau_191',\n","            'Abyssinian_5',\n","            'Abyssinian_34',\n","            'chihuahua_121',\n","            'beagle_116'\n","        ]\n","\n","        self.df = self.df[~self.df['file_name'].isin(invalid_filenames)]\n","        self.transform = Augmentation(image_size, mode)\n","\n","        self.on_epoch_end()\n","            \n","    def __len__(self):\n","        return math.ceil(len(self.df) / self.batch_size)\n","        \n","    def __getitem__(self, idx):\n","        strt = idx * self.batch_size\n","        fin = (idx + 1) * self.batch_size\n","        data = self.df.iloc[strt:fin]\n","        \n","        batch_x, batch_y = self.get_data(data)\n","        return np.array(batch_x), np.array(batch_y)\n","        \n","    # 이때 label == mask\n","    def get_data(self, data):\n","        batch_x = []\n","        batch_y = []\n","        \n","        for _, r in data.iterrows():\n","            file_name = r['file_name']\n","            \n","            image = cv2.imread(f'{image_root}{file_name}.jpg')\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","            image = cv2.resize(image, (self.image_size, self.image_size))\n","\n","            mask = cv2.imread(f'{anno_root}trimaps/{file_name}.png', cv2.IMREAD_GRAYSCALE)\n","            mask = cv2.resize(mask, (self.image_size, self.image_size))  # mask도 imge에 맞게 resize 필요\n","\n","            # 0, 1, 2 중 1(개/고양이)에 해당하는 값들만 예측하는 task\n","            # -> 1이 아닌 값들은 모두 날려줘야 함\n","            mask[mask != 1] = 0\n","\n","            # ★ 예측 시 주의 : image에 적용되는 operation들이 mask에도 적용되어야 함\n","            # i.e. image가 15도 회전되었다면 mask도 이에 맞춰 15도 회전 필요\n","            # -> 방법 : image와 mask를 함께 transform에 넘겨주기\n","            if self.mode == 'train': \n","                # augmentation 중에는 image가 uint8인 경우에만 적용되는 것들이 있음\n","                # -> image를 먼저 uint8로 변환하기\n","                image = image.astype('uint8')\n","                image, mask = self.transform(image=image, mask=mask)\n","\n","            image = image.astype('float32')\n","            image = image / 255.\n","            mask = mask.astype('float32')\n","            \n","            # label = int(r['species']) - 1\n","            \n","            batch_x.append(image)\n","            batch_y.append(mask)\n","        \n","        return batch_x, batch_y\n","\n","    def on_epoch_end(self):\n","        if self.shuffle:\n","            self.df = self.df.sample(frac=1).reset_index(drop=True)\n","\n","train_generator = DataGenerator(\n","    fold=1,\n","    mode='train',\n","    csv_path=csv_path,\n","    batch_size=128,\n","    image_size=256,\n","    shuffle=True\n",")\n","\n","valid_generator = DataGenerator(\n","    fold=1,\n","    mode='val',\n","    csv_path=csv_path,\n","    batch_size=128,\n","    image_size=128,\n","    shuffle=True\n",")"],"metadata":{"id":"VVAO4y9VYOTT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["시각화를 통해 모델 생성이 잘 되었는지 확인"],"metadata":{"id":"hXztyDihYOVo"}},{"cell_type":"code","source":["for batch in train_generator:\n","    X, y = batch\n","    plt.figure(figsize=(10, 10))\n","\n","    for i in range(3):\n","        plt.subplot(3, 2, i*2 + 1)\n","        plt.imshow(X[i])   # image\n","        plt.axis('off')\n","        plt.subplot(3, 2, i*2 + 2)\n","        plt.imshow(y[i], cmap='gray')   # mask\n","        plt.axis('off')\n","        plt.tight_layout()\n","    break"],"metadata":{"id":"ro4OeIuOYOYV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","# 4. UNet\n","\n","- 목적\n","  - biomedical 분야에서 image segmentation을 목적으로 제안된 end to end 방식의 fully convolutional network 모델\n","  - 의료 영상에 적용하기 위해 만들어졌지만 일반적인 이미지들에 대해서도 좋은 성능을 보이기 때문에 지금까지도 널리 쓰이는 model architecture\n","- U처럼 생긴 네트워크의 구성 때문에 'UNet'이라고 불리게 됨\n","- 구조\n","  - 1) Contraction path (encoder) : image의 context 학습\n","    - convolution layer, maxpooling layer의 stack으로 이루어짐\n","  - 2) Expanding path (decoder) : image localization 학습 (위치 정보)\n","    - transpose convolution 사용\n","  - 즉, 처음부터 끝까지 convolution layer로만 이루어짐\n","  - = Dense layer 없음\n","    - input image size에 영향 받지 않게 됨\n","    - 물론 그렇다고 모든 image size를 input으로 받을 수는 없음\n","    - encoder에서 down sampling이 maxpooling 때문에 한 번씩 발생할 때마다 가로와 세로 길이가 딱 절반씩 감소하는데 현재 구조에서는 총 4번 down sampling이 발생함 -> input image size가 최소 16 * 16은 되어야 함 (16 \\* 16보다 작으면 error 발생)\n","- output : image와 같은 크기의 segmentation mask\n","- encoder가 image context를 학습하는 방법\n","  - Max Pooling (Down Sampling)\n","    - feature map의 크기를 줄여 최종적으로 network의 parameter 수를 감소시키는 것\n","    - = 중요한 정보만 유지하고 나머지는 버림\n","    - 중요한 정보 : image의 context를 잘 설명하는 값 (i.e. 최대 pixel)\n","- decoder (Up Sampling)\n","  - image segmentation을 수행하면 image의 context 뿐만 아니라 localization도 함께 수행되어야 하기 때문에 잃어버린 위치에 대한 정보를 학습하는 역할\n","  - Transposed Convolution (Deep Convolution)\n","    - Up Sampling을 위해 가장 자주 사용되는 방법\n","    - 학습 가능한 parameter들로 up sampling을 수행하는 layer\n"],"metadata":{"id":"tHLwgrj4YOa4"}},{"cell_type":"markdown","source":["### 4.1 Training with UNet\n","\n","- keras에서 segmentation model을 따로 제공하지 않기 때문에 쓰고 싶은 모델이 있으면 그 모델이 구현되어 있는 유명한 repo를 가져와서 수정해서 사용하는 방식이 일반적\n","- [UNet 구현 Repo](https://github.com/karolzak/keras-unet)\n","  - UNet은 예전에 나온 논문이기 때문에 요즘 default로 많이 쓰이는 dropout, batch normalization 등이 적용 안 되어 있음\n","  - 그런데 이 repository에서는 이런 부분까지 적용할 수 있게 해 놓음 (Customizable U-Net) -> 이 부분 가져다 쓰기"],"metadata":{"id":"WKraGDzynfH9"}},{"cell_type":"code","source":["pip install git+https://github.com/karolzak/keras-unet"],"metadata":{"id":"JNcuoSG186HG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras_unet.models import custom_unet\n","\n","model = custom_unet(\n","    input_shape=(128, 128, 3),\n","    # input_shape=(32, 32, 3),  # OOM error 때문에 input shape 줄여봄\n","    use_batch_norm=True,\n","    upsample_mode='deconv', # simple : 학습이 일어나지 않는 것 -> deconv를 쓰는 게 일반적\n","    dropout_type='spatial', # spatial을 권장\n","    # use_dropout_on_upsampling=False, # upsampling할 때 dropout 사용할 지\n","    use_attention=True,  # encoder와 decoder 사이 skip-connection을 지나갈 때 attention을 사용할 지\n","    num_classes=1,\n","    filters=64,          # (default: 16)\n","    dropout=0.2,\n","    num_layers=4, # encoder에 있는 layer 개수 (convolution에 몇 개 block을 만들지) - 깊다고 항상 좋은 건 아님\n","    output_activation='sigmoid' # 이진분류\n",")\n","\n","model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[iou])\n","model.summary()"],"metadata":{"id":"990lQA0LnfKl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- custom_unet이 인자로 받는 것들의 각각의 의미"],"metadata":{"id":"PhDSCFkRnfM8"}},{"cell_type":"code","source":["help(custom_unet)"],"metadata":{"id":"RExy-X0-nfP1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 10번째 epoch에서 val_iou가 0.73인데 이 정도면 어느정도 모양은 다 알아볼 수 있다고 생각할 수 있음\n","- 0.85 이상이면 정말 잘한다\n","- 0.90 이상이면 사람보다 잘한다"],"metadata":{"id":"r1llgDvOM0dV"}},{"cell_type":"markdown","source":["에러발생 : OOM (out of memory)\n","- You can fix this by making your model smaller or reducing your batch size\n","  - 이미지 input_shape을 줄이거나 batch_size 줄일 것\n","- If you have multiple GPUS at hand, kindly select a GPU which is not as busy as this one\n","  - `export CUDA_VISIBLE_DEVICES=1`\n","  - check the available GPUs using `nvidia-smi `"],"metadata":{"id":"wR4AbNPeSIKv"}},{"cell_type":"code","source":["!export CUDA_VISIBLE_DEVICES=1"],"metadata":{"id":"88cQgenWSj9N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi "],"metadata":{"id":"NvJAltN_ShyI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(\n","    train_generator,\n","    validation_data=valid_generator,\n","    epochs=10,\n","    verbose=1\n",")"],"metadata":{"id":"HOEU4BrCnfSt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.2 시각화\n","\n","- IoU : 우상향 -> 단순히 epoch 수만 더 늘려도 성능이 올라갈 수 있음"],"metadata":{"id":"f8jB4QoNMquJ"}},{"cell_type":"code","source":["history = history.history\n","\n","plt.figure(figsize=(10, 5))\n","plt.subplot(1,2,1)\n","plt.plot(history['loss'], label='train')\n","plt.plot(history['val_loss'], label='val')\n","plt.legend()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.title('Loss')\n","\n","plt.subplot(1,2,2)\n","plt.plot(history['iou'], label='train')\n","plt.plot(history['val_iou'], label='val')\n","plt.legend()\n","plt.xlabel('epoch')\n","plt.ylabel('iou')\n","plt.title('IoU')\n","plt.show()"],"metadata":{"id":"d67gunDknfVu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for images, masks in valid_generator:\n","    break"],"metadata":{"id":"IDec-BD3YOdw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preds = model.predict(images)"],"metadata":{"id":"_U1YidSuM9kb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","idx = random.choice(range(len(preds)))\n","plt.figure(figsize=(10,10))\n","plt.subplot(1,2,1)\n","plt.imshow(images[idx,...])\n","plt.subplot(1,2,2)\n","plt.imshow(images[idx,...,0], cmap='gray')"],"metadata":{"id":"QX_ARVcmM9nJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","# 5. TensorBoard visualization\n","\n","- Google에서 open-source로 제공하는 학습/모델 시각화 tool\n","- 장점\n","  - 학습 도중에도 loss/accuracy를 그래프로 확인 가능\n","  - 기존처럼 학습이 완료된 후 epoch에 따른 loss, iou 변화를 따로 그래프로 시각화하지 않아도 됨\n","  - 학습이 진행됨에 따라 특정 이미지에 대해 모델의 예측 결과가 어떻게 달라지는 지도 확인 가능 -> 매우 유용\n","- keras에서는 tensorboard logging을 callback으로 제공 -> 기존과 학습 code는 동일하게 하고 callback 함수만 추가 필요"],"metadata":{"id":"sBwmZc-ANKh2"}},{"cell_type":"code","source":["from tensorflow.keras.callbacks import TensorBoard\n","from tensorflow.keras.callbacks import Callback"],"metadata":{"id":"n-8Y96ydTmAX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.1 custom callback 만들기\n","\n","- https://www.tensorflow.org/tensorboard/image_summaries\n","  - plot_to_image, image_grid 함수 코드 그대로 복사 + 붙여넣기"],"metadata":{"id":"3sN-t9UaNLM3"}},{"cell_type":"code","source":["# 임의의 figure에 대해 변환하는 함수라 추가 수정 필요 없음\n","def plot_to_image(figure):\n","    \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n","    returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n","    # Save the plot to a PNG in memory.\n","    buf = io.BytesIO()\n","    plt.savefig(buf, format='png')\n","    # Closing the figure prevents it from being displayed directly inside\n","    # the notebook.\n","    plt.close(figure)\n","    buf.seek(0)\n","    # Convert PNG buffer to TF image\n","    image = tf.image.decode_png(buf.getvalue(), channels=4)\n","    # Add the batch dimension\n","    image = tf.expand_dims(image, 0)\n","    return image\n","\n","# 전체 수정 필요\n","def image_grid(img, mask, preds):\n","\n","    figure = plt.figure(figsize=(10, 10))\n","\n","    # 처음 3개의 이미지에 대해서만 시각화하고자 함\n","    for i in range(3):\n","        plt.subplot(3, 3, 3*i + 1)\n","        plt.imshow(img[i])\n","        plt.axis('off')\n","        plt.title('img')\n","\n","        plt.subplot(3, 3, 3*i + 2)  # 오른쪽에 위치 (정답 mask)\n","        plt.imshow(mask[i], camp='gray')\n","        plt.axis('off')\n","        plt.title('gt')\n","\n","        plt.subplot(3, 3, 3*i + 3)  # 예측 결과\n","        plt.imshow(preds[i, ..., 0], camp='gray')  # i번째 이미지, 마지막 채널은 single 채널이라 의미가 없으니 없애주기 -> gray\n","        plt.axis('off')\n","        plt.title('pred')\n","\n","    return figure"],"metadata":{"id":"4o1-QHtTTfkE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 매 epoch이 끝날 때마다 사용자가 지정해 준 이미지에 대한 모델의 예측 결과를 tensorboard에 logging 하는 callback 함수\n","class TrainHistory(Callback):\n","    def __init__(self, data, log_dir=drive_project_root+'weights'):\n","        self.img, self.mask = data    # 입력 데이터 -> data generator에서 가져오기\n","        self.write = tf.summary.create_file_writer(log_dir)\n","        \n","    # image logging 주기 정하기\n","    # 매 batch 마다 logging 하면 학습 느려짐 -> 매 epoch이 \"끝날 때마다\" logging하기\n","    def on_epoch_end(self, epoch, logs=None):\n","        pred = self.model.predict(self.img)   # 예측 결과\n","        \n","        figure = image_grid(self.img, self.mask, pred)  # matplotlib로 plot 해주는 함수\n","\n","        with self.writer.as_default():\n","            tf.summary.image(\n","                'plot',\n","                plot_to_image(figure),  # tensorboard에 logging이 가능한 image 형태로 변환\n","                epoch                   # 몇 epoch에 대한 결과인지\n","            )"],"metadata":{"id":"OIglJ7m6Tfg1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["callbacks = [\n","    TensorBoard(drive_project_root+'weights/'),  # 저장위치\n","    TrainHistory(\n","        log_dir=drive_project_root+'weights/',\n","        data=valid_generator[0][:3]  # 첫 번째 batch에 있는 처음 3개의 이미지 = 고정된 이미지에 대한 결과가 어떻게 변화하는지 확인하기 위함\n","    )\n","]"],"metadata":{"id":"q7XevDcAQ988"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(\n","    train_generator,\n","    validation_data=valid_generator,\n","    epochs=10,\n","    verbose=1,\n","    callbacks=callbacks\n",")"],"metadata":{"id":"y-oRJFThNLQ_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.2 Tensorboard 띄우기\n","\n","- 상단 'IMAGES' 들어가서 step 막대기 조절하며, epoch이 지날 때마다 (학습이 진행됨에 따라) 예측 변화 정도 확인해 보기"],"metadata":{"id":"Ykf6hBE2NLg-"}},{"cell_type":"code","source":["%load_ext tensorboard\n","%tensorboard --logidr drive/MyDrive/data/weights/"],"metadata":{"id":"OZ9Dr0AXbhYq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","# 6. Learning Rate Schedule\n","\n","- optimizer에서 learning rate를 설정하지 않으면 default인 0.001이 적용됨\n","- schedule : 학습이 진행됨에 따라 learning rate를 변환하는 것"],"metadata":{"id":"8qhXMcy-bhbj"}},{"cell_type":"code","source":["# 한 epoch을 학습하는 동안 몇 번의 weight update를 하는지\n","# 전체 데이터 개수 / batch_size\n","len(train_generator)"],"metadata":{"id":"itSOT4kAbhec"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.1 Exponential Decay : 10 epoch에 대한 시각화\n","- decay_steps = 46 : exponential 이라는 이름과는 맞지 않게 linear 하게 감소함\n","- decay_steps = 3 : 0.96이 더 자주 곱해지니 learning rate가 더 빠르게 감소함\n","  - 여기서는 보여주기 위해 3으로 설정했지만, 일반적으로 3으로 설정하는 건 너무 작은 편"],"metadata":{"id":"cT4UFQH_KK26"}},{"cell_type":"code","source":["def decayed_learning_rate(step):\n","    initial_learning_rate = 0.001\n","\n","    # 위에서 len(train_generator) = 46이었는데 decay_steps도 46으로 설정하면\n","    # 한 epoch이 지날 때마다 learning rate도 4% 씩 줄이라는 의미 (0.96)\n","    decay_rate = 0.96\n","    # decay_steps = 46 \n","    decay_steps = 3\n","\n","    return initial_learning_rate * decay_rate ** (step / decay_steps)\n","\n","# 한 번 학습을 진행할 때마다 46 step이 있음\n","# 10 epoch동안 학습하면 46 * 10\n","lrs = [decayed_learning_rate(i) for i in range(1, 46 * 10)]\n","plt.plot(lrs)"],"metadata":{"id":"NEmP8ohgbhgy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.2 Optimizer에 적용하기"],"metadata":{"id":"yYLOEIPPbhjQ"}},{"cell_type":"code","source":["lr_schedule = optimizers.schedules.ExponentialDecay(\n","    0.001,  # initial learning rate\n","    decay_steps=3,\n","    decay_rate=0.96,  # 3 step마다 0.96 곱하기\n",")\n","optimizer = optimizers.Adam(lr_schedule)"],"metadata":{"id":"ZXbGyc8Qbhl7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.3 Modeling"],"metadata":{"id":"dC0hqXzsbhom"}},{"cell_type":"code","source":["model = custom_unet(\n","    input_shape=(128, 128, 3),\n","    use_batch_norm=True,\n","    num_classes=1,\n","    filters=64,\n","    dropout=0.2,\n","    use_attention=True,\n","    output_activation='sigmoid'\n",")\n","\n","model.compile(optimizer=optimizer, loss=bce_dice_loss, metrics=[iou])\n","model.summary()"],"metadata":{"id":"CLPspVQIbhrT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(\n","    train_generator,\n","    validation_data=valid_generator,\n","    epochs=10,\n","    verbose=1\n",")"],"metadata":{"id":"JjBCpx1hLavv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.4 visualization : train data"],"metadata":{"id":"eROD0sEiOBF_"}},{"cell_type":"code","source":["plt.figure(figsize=(10, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(train_losses[:10], label='w/o scheduling')\n","plt.plot(history.history['loss'], label='scheduling')\n","plt.legend()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.title(\"Train Loss\")\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(train_ious[:10], label='w/o scheduling')\n","plt.plot(history.history['iou'], label='scheduling')\n","plt.legend()\n","plt.xlabel('epoch')\n","plt.ylabel('IoU')\n","plt.title(\"Train IoU\")\n","plt.show()"],"metadata":{"id":"QUCWi_fbLayM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.5 visualization : validation data"],"metadata":{"id":"PxWB0yprOGPd"}},{"cell_type":"code","source":["plt.figure(figsize=(15, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(valid_losses[:10], label='w/o scheduling')\n","plt.plot(history.history['val_loss'], label='scheduling')\n","plt.legend()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.title(\"Valid Loss\")\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(valid_ious[:10], label='w/o scheduling')\n","plt.plot(history.history['val_iou'], label='scheduling')\n","plt.legend()\n","plt.xlabel('epoch')\n","plt.ylabel('IoU')\n","plt.title(\"Valid IoU\")\n","plt.show()"],"metadata":{"id":"ZhjhdAoiLa0x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","# 7. Multiclass segmentation\n","\n","- 3가지 부분(배경, 개/고양이, 테두리)을 모두 예측하는 모델 만들기\n","- Augmentation은 binary와 같이 써도 되고, DataGenerator 부분만 일부 수정 필요"],"metadata":{"id":"lJJru73BLa3K"}},{"cell_type":"code","source":["class DataGenerator(keras.utils.Sequence):\n","    def __init__(self, batch_size, csv_path, image_size,\n","                 fold, mode='train', shuffle=True):\n","        \n","        self.batch_size = batch_size\n","        self.image_size = image_size\n","        self.fold = fold\n","        self.mode = mode\n","        self.shuffle = shuffle\n","\n","        self.df = pd.read_csv(csv_path)\n","        \n","        if self.mode == 'train':\n","            self.df = self.df[self.df['fold'] != self.fold]\n","        elif self.mode == 'val':\n","            self.df = self.df[self.df['fold'] == self.fold]\n","        \n","        #### Remove invalid files\n","        #### https://github.com/tensorflow/models/issues/3134\n","        invalid_filenames = [\n","            'Egyptian_Mau_14',\n","            'Egyptian_Mau_139',\n","            'Egyptian_Mau_145',\n","            'Egyptian_Mau_156',\n","            'Egyptian_Mau_167',\n","            'Egyptian_Mau_177',\n","            'Egyptian_Mau_186',\n","            'Egyptian_Mau_191',\n","            'Abyssinian_5',\n","            'Abyssinian_34',\n","            'chihuahua_121',\n","            'beagle_116'\n","        ]\n","\n","        self.df = self.df[~self.df['file_name'].isin(invalid_filenames)]\n","        self.transform = Augmentation(image_size, mode)\n","\n","        self.on_epoch_end()\n","            \n","    def __len__(self):\n","        return math.ceil(len(self.df) / self.batch_size)\n","        \n","    def __getitem__(self, idx):\n","        strt = idx * self.batch_size\n","        fin = (idx + 1) * self.batch_size\n","        data = self.df.iloc[strt:fin]\n","        \n","        batch_x, batch_y = self.get_data(data)\n","        return np.array(batch_x), np.array(batch_y)\n","        \n","    # 이때 label == mask\n","    def get_data(self, data):\n","        batch_x = []\n","        batch_y = []\n","        \n","        for _, r in data.iterrows():\n","            file_name = r['file_name']\n","            \n","            image = cv2.imread(f'{image_root}{file_name}.jpg')\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","            image = cv2.resize(image, (self.image_size, self.image_size))\n","\n","            mask = cv2.imread(f'{anno_root}trimaps/{file_name}.png', cv2.IMREAD_GRAYSCALE)\n","            mask = cv2.resize(mask, (self.image_size, self.image_size))\n","\n","            # mask[mask != 1] = 0    -> 개/고양이 부분이 아닌 다른 부분을 모두 0으로 바꿀 필요 없음\n","            # 단, 현재 class는 1, 2, 3인데 keras에서는 0, 1, 2 순이 되어야 하기 때문에 변경 필요\n","            mask -= 1                 # 개/고양이(0), 배경(1), 테두리(2)\n","\n","            if self.mode == 'train': \n","                image = image.astype('uint8')\n","                image, mask = self.transform(image=image, mask=mask)\n","\n","            image = image.astype('float32')\n","            image = image / 255.\n","            mask = mask.astype('float32')\n","            \n","            # label = int(r['species']) - 1\n","            \n","            batch_x.append(image)\n","            batch_y.append(mask)\n","        \n","        return batch_x, batch_y\n","\n","    def on_epoch_end(self):\n","        if self.shuffle:\n","            self.df = self.df.sample(frac=1).reset_index(drop=True)\n","\n","train_generator = DataGenerator(\n","    fold=1,\n","    mode='train',\n","    csv_path=csv_path,\n","    batch_size=128,\n","    image_size=256,\n","    shuffle=True\n",")\n","\n","valid_generator = DataGenerator(\n","    fold=1,\n","    mode='val',\n","    csv_path=csv_path,\n","    batch_size=128,\n","    image_size=128,\n","    shuffle=True\n",")"],"metadata":{"id":"CX7lMB5pLa5s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = custom_unet(\n","    input_shape=(128, 128, 3),\n","    use_batch_norm=True,\n","    num_classes=3,   ### 수정\n","    filters=64,\n","    dropout=0.2,\n","    use_attention=True,\n","    output_activation='softmax' ### 수정\n",")\n","\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')  # loss 부분 수정 필요\n","model.summary()"],"metadata":{"id":"P5XhoQaZLa8I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(\n","    train_generator,\n","    validation_data=valid_generator,\n","    epochs=20,\n","    verbose=1\n",")"],"metadata":{"id":"W4WHQWaXLa-3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = history.history\n","plt.figure(figsize=(10, 5))\n","plt.plot(history['loss'], label='train')\n","plt.plot(history['val_loss'], label='val')\n","plt.legend()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.title('Loss')\n","plt.show()"],"metadata":{"id":"Es51GMSXP6q-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 모델이 잘 예측하고 있는지 시각화"],"metadata":{"id":"UFyhuUIsP6tw"}},{"cell_type":"code","source":["for images, masks in valid_generator:\n","    break\n","\n","preds = model.predict(images)\n","pred.shape   # 채널 : 3"],"metadata":{"id":"xySZ5Qv_P6w1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 3개의 class 중 확률이 가장 높은 1개 값으로 분류해서 출력하기 (나머지는 버리기)\n","- 현재는 각 class(0, 1, 2)에 속할 확률이 모두 표시되어 있음"],"metadata":{"id":"yFzUc0_eRW_9"}},{"cell_type":"code","source":["pred = np.argmax(preds[idx], axis=-1) \n","np.unique(pred)   # 0, 1, 2 값만 나옴"],"metadata":{"id":"r36PVycQRV_k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = random.choice(range(len(preds)))\n","plt.figure(figsize=(10, 10))\n","plt.subplot(1, 3, 1)\n","plt.imshow(images[idx, ...])  # 원본\n","plt.subplot(1, 3, 2)\n","plt.imshow(masks[idx])   # mask\n","plt.subplot(1, 3, 3)\n","plt.imshow(preds)        # 예측값"],"metadata":{"id":"G8zV7acCQiMg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"dmFSKrU7QiPJ"},"execution_count":null,"outputs":[]}]}
