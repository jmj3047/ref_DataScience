{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b594a080",
   "metadata": {},
   "source": [
    "# 개념\n",
    "\n",
    "### Deep Learning architecture는 무엇인가요? ★ (면접 interview)\n",
    "- correlation filter들을 묶어서 filter bank를 만들고 그 filter bank들의 cascade 구조가 Deep Learning architecture 입니다\n",
    "  - cascade 구조 : layer들이 연결된 것\n",
    " \n",
    "### Dense Layer\n",
    "- neuron들의 집합\n",
    "- 앞 neuron들과 연결\n",
    "\n",
    "### Mxnet\n",
    "- GPU를 여러 개 돌리는데 특화되어 있음\n",
    "- keras를 쓰는 사람들이 GPU 여러 개 돌려야 하면 Mxnet 많이 사용함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5942acdb",
   "metadata": {},
   "source": [
    "# 1. Dense Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0650f06e",
   "metadata": {},
   "source": [
    "## 1.1 Shapes of Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9359fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdb4fe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Input/Weight/Bias =====\n",
      "X:  (1, 10)\n",
      "W:  (10, 3)\n",
      "B:  (3,)\n",
      "y:  (1, 3)\n"
     ]
    }
   ],
   "source": [
    "N, n_feature = 1, 10\n",
    "X = tf.random.normal(shape=(N, n_feature))  # input setting\n",
    "\n",
    "n_neuron = 3\n",
    "dense = Dense(units=n_neuron, activation='sigmoid')  # 1개의 dense layer 안에 3개의 neuron 들어 있음\n",
    "y = dense(X)\n",
    "\n",
    "W, B = dense.get_weights()\n",
    "\n",
    "print('===== Input/Weight/Bias =====')\n",
    "print('X: ', X.shape)\n",
    "print('W: ', W.shape)\n",
    "print('B: ', B.shape)\n",
    "print('y: ', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bfbbdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Input/Weight/Bias =====\n",
      "X:  (8, 10)\n",
      "W:  (10, 3)\n",
      "B:  (3,)\n",
      "y:  (8, 3)\n"
     ]
    }
   ],
   "source": [
    "N, n_feature = 8, 10\n",
    "X = tf.random.normal(shape=(N, n_feature))  # input setting\n",
    "\n",
    "n_neuron = 3\n",
    "dense = Dense(units=n_neuron, activation='sigmoid')  # 1개의 dense layer 안에 3개의 neuron 들어 있음\n",
    "y = dense(X)\n",
    "\n",
    "W, B = dense.get_weights()\n",
    "\n",
    "print('===== Input/Weight/Bias =====')\n",
    "print('X: ', X.shape)\n",
    "print('W: ', W.shape)\n",
    "print('B: ', B.shape)\n",
    "print('y: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e85ee1f",
   "metadata": {},
   "source": [
    "## 1.2 Output Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5cb5e1",
   "metadata": {},
   "source": [
    "- 연산 검증\n",
    "- tensorflow와 matrix multiplication, dot products 결과가 모두 같게 나오는 지 확인\n",
    "\n",
    "### 1.2.1 원래 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9637c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.math import exp\n",
    "from tensorflow.linalg import matmul\n",
    "\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef2353e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y (tensorflow): \n",
      " [[0.38559175 0.4296191  0.4367892 ]\n",
      " [0.88488495 0.25752503 0.11596313]\n",
      " [0.5542643  0.47863567 0.38486636]\n",
      " [0.4652395  0.5958901  0.36504048]]\n"
     ]
    }
   ],
   "source": [
    "N, n_feature = 4, 10\n",
    "X = tf.random.normal(shape=(N, n_feature))  # input setting\n",
    "\n",
    "n_neuron = 3\n",
    "dense = Dense(units=n_neuron, activation='sigmoid')  # 1개의 dense layer 안에 3개의 neuron 들어 있음\n",
    "y_tf = dense(X)\n",
    "\n",
    "W, B = dense.get_weights()\n",
    "print('y (tensorflow): \\n', y_tf.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b3a812",
   "metadata": {},
   "source": [
    "### 1.2.2 Calculate with matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07939e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y (manual with matrix multiplication): \n",
      " [[0.38559175 0.4296191  0.43678918]\n",
      " [0.8848849  0.257525   0.11596316]\n",
      " [0.5542643  0.47863567 0.38486633]\n",
      " [0.46523947 0.5958901  0.36504045]]\n"
     ]
    }
   ],
   "source": [
    "z = matmul(X, W) + B\n",
    "y_man = 1/(1+exp(-z))\n",
    "\n",
    "print('y (manual with matrix multiplication): \\n', y_man.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61c1bbc",
   "metadata": {},
   "source": [
    "### 1.2.3 : Calculate with dot products\n",
    "\n",
    "- tensor는 'y_man_vec[0,0] = 10' 과 같은 연산 불가능\n",
    "  - gradient 구할 수 없게 되기 때문\n",
    "  - 그래서 numpy를 사용해야 함\n",
    "- 먼저 틀 만들기 : y_man_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3ad2e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "y (manual with dot roducts): \n",
      " [[0.38559174 0.4296191  0.43678921]\n",
      " [0.88488491 0.25752506 0.11596314]\n",
      " [0.55426431 0.47863564 0.38486636]\n",
      " [0.46523948 0.5958901  0.36504048]]\n"
     ]
    }
   ],
   "source": [
    "y_man_vec = np.zeros(shape=(N, n_neuron))\n",
    "print(y_man_vec)\n",
    "\n",
    "for x_idx in range(N):\n",
    "    x = X[x_idx]\n",
    "    \n",
    "    for nu_idx in range(n_neuron):\n",
    "        w, b = W[:, nu_idx], B[nu_idx]\n",
    "        \n",
    "        z = tf.reduce_sum(x*w) + b\n",
    "        a = 1/(1 + np.exp(-z))\n",
    "        y_man_vec[x_idx, nu_idx] = a\n",
    "\n",
    "print('y (manual with dot roducts): \\n', y_man_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447fdfc1",
   "metadata": {},
   "source": [
    "# 2. Cascaded Dense Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36e67a",
   "metadata": {},
   "source": [
    "## 2.1 Shapes of Cascaded Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2c13e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (4, 10)\n",
      "\n",
      "W1:  (10, 3)\n",
      "B1:  (3,)\n",
      "A1: (4, 3)\n",
      "\n",
      "W2:  (3, 5)\n",
      "B2:  (5,)\n",
      "y: (4, 5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# N : minibatch size\n",
    "N, n_feature = 4, 10\n",
    "X = tf.random.normal(shape=(N, n_feature))\n",
    "\n",
    "n_neurons = [3, 5]\n",
    "dense1 = Dense(units=n_neurons[0], activation='sigmoid')   # 첫 번째 dense layer\n",
    "dense2 = Dense(units=n_neurons[1], activation='sigmoid')   # 두 번째 dense layer\n",
    "\n",
    "# Forward Propagation\n",
    "A1 = dense1(X)\n",
    "y = dense2(A1)\n",
    "\n",
    "# get weight/bias\n",
    "W1, B1 = dense1.get_weights()\n",
    "W2, B2 = dense2.get_weights()\n",
    "\n",
    "print('X: {}\\n'.format(X.shape))\n",
    "\n",
    "print('W1: ', W1.shape)\n",
    "print('B1: ', B1.shape)\n",
    "print('A1: {}\\n'.format(A1.shape))  # minibatch × neuron\n",
    "\n",
    "print('W2: ', W2.shape)\n",
    "print('B2: ', B2.shape)\n",
    "print('y: {}\\n'.format(y.shape))    # minibatch × neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9406781d",
   "metadata": {},
   "source": [
    "## 2.2 Dense Layers with Python List\n",
    "\n",
    "- TensorFlow는 매우 flexible 해서 구현할 수 있는 방법이 다양함\n",
    "  - 기존에 알고 있는 python 문법 그대로 이용해도 됨\n",
    "- dense layer가 여러 개가 있으면, list와 for문 사용해서 효율적으로 코드 짜기 가능\n",
    "  - dense1 = Dense(units=n_neurons[0], activation='sigmoid')   # 첫 번째 dense layer\n",
    "  - dense2 = Dense(units=n_neurons[1], activation='sigmoid')   # 두 번째 dense layer\n",
    "  - 이렇게 10개 만들면 매우 비효율적인 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f71ec66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  (4, 10)\n",
      "After dense layer  1   (4, 10)\n",
      "After dense layer  2   (4, 20)\n",
      "After dense layer  3   (4, 30)\n",
      "After dense layer  4   (4, 40)\n",
      "After dense layer  5   (4, 50)\n",
      "After dense layer  6   (4, 60)\n",
      "After dense layer  7   (4, 70)\n",
      "After dense layer  8   (4, 80)\n",
      "After dense layer  9   (4, 90)\n",
      "After dense layer  10   (4, 100)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# N : minibatch size\n",
    "N, n_feature = 4, 10\n",
    "X = tf.random.normal(shape=(N, n_feature))\n",
    "\n",
    "n_neurons = [10,20,30,40,50,60,70,80,90,100]\n",
    "\n",
    "# dense_layers 안에 10개의 dense layer object 만들기\n",
    "dense_layers = list()\n",
    "for n_neuron in n_neurons:\n",
    "    dense = Dense(units=n_neuron, activation='relu')\n",
    "    dense_layers.append(dense)\n",
    "\n",
    "print('Input: ', X.shape)\n",
    "for idx, dense in enumerate(dense_layers):\n",
    "    X = dense(X)\n",
    "    print('After dense layer ', idx+1, ' ', X.shape)\n",
    "\n",
    "y = X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e552e5bb",
   "metadata": {},
   "source": [
    "## 2.3 Output Calculations\n",
    "\n",
    "- Tensorflow에서 각 Layer를 통과하면서 Tensorflow의 연산 과정 살펴보기\n",
    "- tf.identity(X) : original X값 복사해서 저장하기\n",
    "  - X_cp = X 라고 하면 두 변수의 주소값이 같아서, X 값이 바뀌면 X_cp 값도 바뀜 → 따로 copy 해 둔 의미가 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa4ffd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y (Tensorflow): \n",
      " [[0.33424628 0.4058136  0.48894918 0.4621372  0.57884294]\n",
      " [0.36388016 0.38423467 0.4965263  0.46321887 0.5773694 ]\n",
      " [0.36786014 0.35406166 0.4990071  0.4631284  0.5875218 ]\n",
      " [0.34541464 0.3835951  0.49541283 0.45196107 0.5769063 ]]\n",
      "y (Manual): \n",
      " [[0.3342463  0.4058136  0.4889492  0.4621372  0.57884294]\n",
      " [0.36388016 0.38423467 0.49652627 0.46321884 0.5773694 ]\n",
      " [0.36786014 0.35406163 0.4990071  0.46312836 0.5875218 ]\n",
      " [0.34541464 0.3835951  0.49541286 0.45196107 0.5769063 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.math import exp\n",
    "from tensorflow.linalg import matmul\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# N : minibatch size\n",
    "N, n_feature = 4, 10\n",
    "X = tf.random.normal(shape=(N, n_feature))\n",
    "X_cp = tf.identity(X)\n",
    "\n",
    "n_neurons = [3,4,5]\n",
    "\n",
    "dense_layers = list()\n",
    "for n_neuron in n_neurons:\n",
    "    dense = Dense(units=n_neuron, activation='sigmoid')\n",
    "    dense_layers.append(dense)\n",
    "\n",
    "# forward propagation (Tensorflow)\n",
    "W, B = list(), list()\n",
    "for idx, dense in enumerate(dense_layers):\n",
    "    X = dense(X)\n",
    "    \n",
    "    w, b = dense.get_weights()    \n",
    "    W.append(w)\n",
    "    B.append(b)\n",
    "\n",
    "print('y (Tensorflow): \\n', X.numpy())\n",
    "\n",
    "# forward propagation (Manual)\n",
    "for layer_idx in range(len(n_neurons)):\n",
    "    w, b = W[layer_idx], B[layer_idx]\n",
    "    \n",
    "    X_cp = matmul(X_cp, w) + b\n",
    "    X_cp = 1/(1+exp(-X_cp))\n",
    "    \n",
    "print('y (Manual): \\n', X_cp.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d3e409",
   "metadata": {},
   "source": [
    "# 3. Model Implementation\n",
    "\n",
    "Sequential vs Model-subclassing\n",
    "- 둘 다 가능\n",
    "  - 아래와 같이 간단한 모델 생성에서는 차이가 없어 보임\n",
    "  - forward propagation 방법만 살짝 달라짐\n",
    "- ★ Sequential보다 Model-subclassing 방법에 더 익숙해져야 함\n",
    "  - 그래야 더 복잡한 모델을 만들 수 있고 내가 원하는 값을 더 flexible하게 분석하기도 좋음\n",
    "- 나중에는 이 두 개를 섞어 사용하기도 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282d4769",
   "metadata": {},
   "source": [
    "## 3.1 Model Implementation with Sequential Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc399fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Sequential() : layer들을 포함하고 있는 object 생성 (빈 frame 같은 것)\n",
    "model = Sequential()\n",
    "model.add(Dense(units=10, activation='sigmoid'))  # 한 layer 안에 neuron 10개 있음\n",
    "model.add(Dense(units=20, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c9b32e",
   "metadata": {},
   "source": [
    "아래 두 가지 코드가 근본적으로는 같음\n",
    "- 그렇지만 tensorflow에서 제공하는 다양한 기능들을 이후에 사용하려면 '방법2'를 써야 좋음\n",
    "\n",
    "```python\n",
    "# 방법1\n",
    "model = list()\n",
    "for n_neuron in n_neurons:\n",
    "    model.append(Dense(units=n_neuron, activation='sigmoid'))\n",
    "    \n",
    "# 방법2\n",
    "model = Sequential()\n",
    "for n_neuron in n_neurons:\n",
    "    model.add(Dense(units=n_neuron, activation='sigmoid'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76b7d87",
   "metadata": {},
   "source": [
    "## 3.2 Model Implementation with Model-subclassing\n",
    "\n",
    "- TensorFlow에서 제공하는 'Model'을 상속 받아서 새로운 model 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae3acfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class TestModel(Model):\n",
    "    def __init__(self):\n",
    "        super(TestModel, self).__init__()   # 이 부분은 무조건 작성해야 함\n",
    "        \n",
    "        # 설계도 만드는 역할\n",
    "        self.dense1 = Dense(units=10, activation='sigmoid')\n",
    "        self.dense2 = Dense(units=20, activation='sigmoid')\n",
    "        \n",
    "# 모델 생성\n",
    "model = TestModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1880f6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.core.dense.Dense object at 0x000001C3C37EFBE0>\n",
      "<keras.layers.core.dense.Dense object at 0x000001C3C3821670>\n"
     ]
    }
   ],
   "source": [
    "print(model.dense1)\n",
    "print(model.dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053bc5a1",
   "metadata": {},
   "source": [
    "## 3.3 Forward Propagation of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c07871fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42f07005",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.random.normal(shape=(4,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e390e13",
   "metadata": {},
   "source": [
    "### 3.3.1 sequential method\n",
    "\n",
    "- 초보자를 위해 tensorflow에서 제공하는 method이기 때문에, 매우 간단함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5d90d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=10, activation='sigmoid'))\n",
    "model.add(Dense(units=20, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41949eb0",
   "metadata": {},
   "source": [
    "model(X) : X를 input으로 dense layer를 알아서 차근차근 지나감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2444ff99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2717339  0.5779645  0.60417205 0.60902226 0.61334085 0.45653743\n",
      "  0.6171141  0.47270983 0.507848   0.3616433  0.6196947  0.40714997\n",
      "  0.4430326  0.48904005 0.5354475  0.38162935 0.44112837 0.61389124\n",
      "  0.3919639  0.4449017 ]\n",
      " [0.32069117 0.5001586  0.6161568  0.6949515  0.668616   0.51909095\n",
      "  0.62964934 0.4325876  0.49256212 0.41801545 0.5936141  0.4002023\n",
      "  0.5416438  0.45583063 0.49197537 0.46065876 0.42061245 0.5914731\n",
      "  0.33512124 0.3663872 ]\n",
      " [0.36624497 0.49199957 0.55439436 0.6422566  0.5915066  0.4484036\n",
      "  0.6504155  0.4440811  0.49958375 0.55257857 0.61358964 0.38130254\n",
      "  0.48231816 0.59819376 0.4823711  0.5699385  0.42495924 0.5741213\n",
      "  0.28030485 0.40708756]\n",
      " [0.2942353  0.57161605 0.57103866 0.60114574 0.5808535  0.39764562\n",
      "  0.6613636  0.46452296 0.52804756 0.42583257 0.6437923  0.3974021\n",
      "  0.43033716 0.5687699  0.527315   0.45365173 0.46001095 0.5879725\n",
      "  0.34410328 0.43245268]]\n"
     ]
    }
   ],
   "source": [
    "y = model(X)\n",
    "print(y.numpy())  # 4 × 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f822e5ae",
   "metadata": {},
   "source": [
    "### 3.3.2 model-subclassing method\n",
    "\n",
    "<b>기본 code</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d759b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(Model):\n",
    "    def __init__(self):\n",
    "        super(TestModel, self).__init__()   # 이 부분은 무조건 작성해야 함\n",
    "        \n",
    "        self.dense1 = Dense(units=10, activation='sigmoid')\n",
    "        self.dense2 = Dense(units=20, activation='sigmoid')\n",
    "        \n",
    "    # call : special method (python 문법)\n",
    "    def call(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "model = TestModel()\n",
    "y = model(X)         # call이 special method이기 때문에 이렇게만 써도 call 함수를 자동으로 연결해 줌 (python 문법)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91d3e64",
   "metadata": {},
   "source": [
    "<b>neuron 여러 개 입력할 때 for 문 활용해서 더 효율적인 코드 작성하는 방법</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de49eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(Model):\n",
    "    def __init__(self, n_neurons):\n",
    "        super(TestModel, self).__init__()   # 이 부분은 무조건 작성해야 함\n",
    "        \n",
    "        self.n_neurons = n_neurons\n",
    "               \n",
    "        self.dense_layers = list() \n",
    "        for n_neuron in self.n_neurons:\n",
    "            self.dense_layers.append(Dense(units=n_neuron, activation='sigmoid'))\n",
    "        \n",
    "    def call(self, x):\n",
    "        for dense in self.dense_layers:\n",
    "            x = dense(x)\n",
    "        return x\n",
    "\n",
    "n_neurons = [3,4,5]\n",
    "model = TestModel(n_neurons)\n",
    "y = model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cf5de4",
   "metadata": {},
   "source": [
    "<b>sequential 모델과 model-subclassing을 같이 엮어서 쓰는 방법</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2a71f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(Model):\n",
    "    def __init__(self):\n",
    "        super(TestModel, self).__init__()   # 이 부분은 무조건 작성해야 함\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(units=10, activation='sigmoid'))\n",
    "        self.model.add(Dense(units=20, activation='sigmoid'))\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "model = TestModel()\n",
    "y = model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43071f3",
   "metadata": {},
   "source": [
    "## 3.4 Layers in Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c003b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9b91c4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[<keras.layers.core.dense.Dense object at 0x000001C3C3C067F0>, <keras.layers.core.dense.Dense object at 0x000001C3C3AE0040>]\n"
     ]
    }
   ],
   "source": [
    "X = tf.random.normal(shape=(4,10))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=10, activation='sigmoid'))\n",
    "model.add(Dense(units=20, activation='sigmoid'))\n",
    "\n",
    "y = model(X)  # forward propagation\n",
    "\n",
    "print(type(model.layers))\n",
    "print(model.layers)    # 두 개의 object가 들어있음 (dense layer 2개)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065f3e5d",
   "metadata": {},
   "source": [
    "★ dense layer에 어떤 기능들이 있는지 살펴보기\n",
    "- get_weights 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "844b8790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_TF_MODULE_IGNORED_PROPERTIES\n",
      "__call__\n",
      "__class__\n",
      "__delattr__\n",
      "__dict__\n",
      "__dir__\n",
      "__doc__\n",
      "__eq__\n",
      "__format__\n",
      "__ge__\n",
      "__getattribute__\n",
      "__getstate__\n",
      "__gt__\n",
      "__hash__\n",
      "__init__\n",
      "__init_subclass__\n",
      "__le__\n",
      "__lt__\n",
      "__module__\n",
      "__ne__\n",
      "__new__\n",
      "__reduce__\n",
      "__reduce_ex__\n",
      "__repr__\n",
      "__setattr__\n",
      "__setstate__\n",
      "__sizeof__\n",
      "__str__\n",
      "__subclasshook__\n",
      "__weakref__\n",
      "_activity_regularizer\n",
      "_add_trackable\n",
      "_add_trackable_child\n",
      "_add_variable_with_custom_getter\n",
      "_auto_track_sub_layers\n",
      "_autocast\n",
      "_autographed_call\n",
      "_build_input_shape\n",
      "_call_accepts_kwargs\n",
      "_call_arg_was_passed\n",
      "_call_fn_arg_defaults\n",
      "_call_fn_arg_positions\n",
      "_call_fn_args\n",
      "_call_full_argspec\n",
      "_callable_losses\n",
      "_cast_single_input\n",
      "_checkpoint_dependencies\n",
      "_clear_losses\n",
      "_compute_dtype\n",
      "_compute_dtype_object\n",
      "_dedup_weights\n",
      "_default_training_arg\n",
      "_deferred_dependencies\n",
      "_delete_tracking\n",
      "_deserialization_dependencies\n",
      "_deserialize_from_proto\n",
      "_dtype\n",
      "_dtype_policy\n",
      "_dynamic\n",
      "_eager_losses\n",
      "_expects_mask_arg\n",
      "_expects_training_arg\n",
      "_flatten\n",
      "_flatten_layers\n",
      "_flatten_modules\n",
      "_functional_construction_call\n",
      "_gather_children_attribute\n",
      "_gather_saveables_for_checkpoint\n",
      "_get_call_arg_value\n",
      "_get_cell_name\n",
      "_get_existing_metric\n",
      "_get_input_masks\n",
      "_get_legacy_saved_model_children\n",
      "_get_node_attribute_at_index\n",
      "_get_save_spec\n",
      "_get_trainable_state\n",
      "_handle_activity_regularization\n",
      "_handle_deferred_dependencies\n",
      "_handle_weight_regularization\n",
      "_inbound_nodes\n",
      "_inbound_nodes_value\n",
      "_infer_output_signature\n",
      "_init_call_fn_args\n",
      "_init_set_name\n",
      "_initial_weights\n",
      "_input_spec\n",
      "_instrument_layer_creation\n",
      "_instrumented_keras_api\n",
      "_instrumented_keras_layer_class\n",
      "_instrumented_keras_model_class\n",
      "_is_layer\n",
      "_keras_api_names\n",
      "_keras_api_names_v1\n",
      "_keras_tensor_symbolic_call\n",
      "_list_extra_dependencies_for_serialization\n",
      "_list_functions_for_serialization\n",
      "_lookup_dependency\n",
      "_losses\n",
      "_map_resources\n",
      "_maybe_build\n",
      "_maybe_cast_inputs\n",
      "_maybe_create_attribute\n",
      "_maybe_initialize_trackable\n",
      "_metrics\n",
      "_metrics_lock\n",
      "_must_restore_from_config\n",
      "_name\n",
      "_name_based_attribute_restore\n",
      "_name_based_restores\n",
      "_name_scope\n",
      "_no_dependency\n",
      "_non_trainable_weights\n",
      "_obj_reference_counts\n",
      "_obj_reference_counts_dict\n",
      "_object_identifier\n",
      "_outbound_nodes\n",
      "_outbound_nodes_value\n",
      "_outer_name_scope\n",
      "_preload_simple_restoration\n",
      "_preserve_input_structure_in_config\n",
      "_restore_from_checkpoint_position\n",
      "_saved_model_arg_spec\n",
      "_saved_model_inputs_spec\n",
      "_self_name_based_restores\n",
      "_self_saveable_object_factories\n",
      "_self_setattr_tracking\n",
      "_self_tracked_trackables\n",
      "_self_unconditional_checkpoint_dependencies\n",
      "_self_unconditional_deferred_dependencies\n",
      "_self_unconditional_dependency_names\n",
      "_self_update_uid\n",
      "_serialize_to_proto\n",
      "_set_call_arg_value\n",
      "_set_connectivity_metadata\n",
      "_set_dtype_policy\n",
      "_set_mask_keras_history_checked\n",
      "_set_mask_metadata\n",
      "_set_save_spec\n",
      "_set_trainable_state\n",
      "_set_training_mode\n",
      "_setattr_tracking\n",
      "_should_cast_single_input\n",
      "_single_restoration_from_checkpoint_position\n",
      "_split_out_first_arg\n",
      "_stateful\n",
      "_supports_masking\n",
      "_symbolic_call\n",
      "_tf_api_names\n",
      "_tf_api_names_v1\n",
      "_thread_local\n",
      "_track_trackable\n",
      "_trackable_children\n",
      "_trackable_saved_model_saver\n",
      "_tracking_metadata\n",
      "_trainable\n",
      "_trainable_weights\n",
      "_unconditional_checkpoint_dependencies\n",
      "_unconditional_dependency_names\n",
      "_update_uid\n",
      "_updates\n",
      "_use_input_spec_as_call_signature\n",
      "activation\n",
      "activity_regularizer\n",
      "add_loss\n",
      "add_metric\n",
      "add_update\n",
      "add_variable\n",
      "add_weight\n",
      "apply\n",
      "bias\n",
      "bias_constraint\n",
      "bias_initializer\n",
      "bias_regularizer\n",
      "build\n",
      "built\n",
      "call\n",
      "compute_dtype\n",
      "compute_mask\n",
      "compute_output_shape\n",
      "compute_output_signature\n",
      "count_params\n",
      "dtype\n",
      "dtype_policy\n",
      "dynamic\n",
      "finalize_state\n",
      "from_config\n",
      "get_config\n",
      "get_input_at\n",
      "get_input_mask_at\n",
      "get_input_shape_at\n",
      "get_losses_for\n",
      "get_output_at\n",
      "get_output_mask_at\n",
      "get_output_shape_at\n",
      "get_updates_for\n",
      "get_weights\n",
      "inbound_nodes\n",
      "input\n",
      "input_mask\n",
      "input_shape\n",
      "input_spec\n",
      "kernel\n",
      "kernel_constraint\n",
      "kernel_initializer\n",
      "kernel_regularizer\n",
      "losses\n",
      "metrics\n",
      "name\n",
      "name_scope\n",
      "non_trainable_variables\n",
      "non_trainable_weights\n",
      "outbound_nodes\n",
      "output\n",
      "output_mask\n",
      "output_shape\n",
      "set_weights\n",
      "stateful\n",
      "submodules\n",
      "supports_masking\n",
      "trainable\n",
      "trainable_variables\n",
      "trainable_weights\n",
      "units\n",
      "updates\n",
      "use_bias\n",
      "variable_dtype\n",
      "variables\n",
      "weights\n",
      "with_name_scope\n"
     ]
    }
   ],
   "source": [
    "dense1 = model.layers[0]\n",
    "for tmp in dir(dense1):\n",
    "    print(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97186e8d",
   "metadata": {},
   "source": [
    "이런 식으로 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "26bc2bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10) (10,)\n",
      "(10, 20) (20,)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    w, b = layer.get_weights()\n",
    "    print(w.shape, b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bf11aa",
   "metadata": {},
   "source": [
    "## 3.5 Trainable Variables in Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d0c91190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "X = tf.random.normal(shape=(4,10))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=10, activation='sigmoid'))\n",
    "model.add(Dense(units=20, activation='sigmoid'))\n",
    "\n",
    "y = model(X)\n",
    "\n",
    "# list type\n",
    "# 안에 weight, bias 등 기타 학습 대상이 되는 variable들이 들어 있음 (총 4개)\n",
    "print(type(model.trainable_variables))\n",
    "print(len(model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc203273",
   "metadata": {},
   "source": [
    "하나씩 접근해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0ae633f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n"
     ]
    }
   ],
   "source": [
    "for train_var in model.trainable_variables:\n",
    "    print(type(train_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5943cb",
   "metadata": {},
   "source": [
    "아래 값은 weight, bias를 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "08c39e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(10,)\n",
      "(10, 20)\n",
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "for train_var in model.trainable_variables:\n",
    "    print(train_var.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14ef082",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
