{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7CUNw8dNBwA4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/content/drive/MyDrive/#fastcampus\")\n",
    "drive_project_root = \"/content/drive/MyDrive/#fastcampus\"\n",
    "!pip install -r \"/content/drive/MyDrive/#fastcampus/requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBxZXWDEHbkh"
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from typing import List\n",
    "from typing import Dict\n",
    "from typing import Tuple\n",
    "\n",
    "import io\n",
    "import re\n",
    "import unicodedata\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as ticker\n",
    "from omegaconf import OmegaConf\n",
    "from omegaconf import DictConfig\n",
    "import hydra\n",
    "from hydra.core.config_store import ConfigStore\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlfSWYBuJvXi"
   },
   "outputs": [],
   "source": [
    "from config_utils_tf import flatten_dict\n",
    "from config_utils_tf import register_config\n",
    "from config_utils_tf import get_optimizer_element\n",
    "from config_utils_tf import get_callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdeNqGVRIJzv"
   },
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mR6QoAFuIMY0"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aknbjV1NLAGD"
   },
   "source": [
    "## Data 다운로드 및 전처리."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYTBI0gDCA75"
   },
   "outputs": [],
   "source": [
    "# Download the file\n",
    "data_root = os.path.join(drive_project_root, \"data\", \"anki_spa_eng\")\n",
    "if not os.path.exists(data_root):\n",
    "    os.mkdir(data_root)\n",
    "\n",
    "data_path = os.path.join(data_root, \"spa-eng.zip\")\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    data_path,\n",
    "    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True,\n",
    "    cache_dir=data_root\n",
    ")\n",
    "\n",
    "path_to_file = os.path.join(\n",
    "    os.path.dirname(path_to_zip),\n",
    "    \"datasets\",\n",
    "    \"spa-eng\",\n",
    "    \"spa.txt\",\n",
    ")\n",
    "print(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LeYe7E2KDGBR"
   },
   "outputs": [],
   "source": [
    "# 전처리\n",
    "def unicode_to_ascii(s):\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\")\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    # ascii 로 변환 및 소문자로 변환\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # 단어와 단어 뒤에 오는 구두점(.) 사이에 공백을 생성\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # (a-z, A-Z, [?.!,¿] 을 제외한 모든 것을 공백으로 대체)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "\n",
    "    # 모델의 앞뒤에 start, end 토큰 추가.\n",
    "    w = \"<start> \" + w + \" <end>\"\n",
    "    return w\n",
    "\n",
    "\n",
    "def create_dataset(path: str, num_examples: Optional[int]=None):\n",
    "    lines = io.open(path, encoding=\"UTF-8\").read().strip().split(\"\\n\")\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split(\"\\t\")] for l in lines[:num_examples]]\n",
    "\n",
    "    return zip(*word_pairs)\n",
    "\n",
    "\n",
    "en, sp = create_dataset(path_to_file)\n",
    "print(en[-1])\n",
    "print(sp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7hP5kWUFte-"
   },
   "outputs": [],
   "source": [
    "# Tokenizer 정의, 최종적으로 쓸 데이터 정리.\n",
    "\n",
    "def tokenize(lang):\n",
    "    lang_tokenizer = Tokenizer(filters=\"\")\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = pad_sequences(tensor, padding=\"post\")\n",
    "\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "def load_dataset(path, num_examples=None):\n",
    "    tar_lang, src_lang = create_dataset(path, num_examples) # en, sp\n",
    "\n",
    "    src_tensor, src_tokenizer = tokenize(src_lang)\n",
    "    tar_tensor, tar_tokenizer = tokenize(tar_lang)\n",
    "\n",
    "    return src_tensor, tar_tensor, src_tokenizer, tar_tokenizer\n",
    "\n",
    "\n",
    "# 언어 데이터셋을 불러오기.\n",
    "num_examples = 30000\n",
    "src_tensor, tar_tensor, src_tokenizer, tar_tokenizer = load_dataset(\n",
    "    path_to_file, num_examples\n",
    ")\n",
    "\n",
    "max_tar_len, max_src_len = tar_tensor.shape[1], src_tensor.shape[1]\n",
    "\n",
    "src_vocab_size = len(src_tokenizer.word_index) + 1\n",
    "tar_vocab_size = len(tar_tokenizer.word_index) + 1\n",
    "\n",
    "print(src_vocab_size, tar_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JNJuccAHjmY"
   },
   "outputs": [],
   "source": [
    "print(tar_tensor[-1])\n",
    "print(tar_tokenizer.word_index)\n",
    "for i in tar_tensor[-1]:\n",
    "    if i == 0:\n",
    "        break\n",
    "    print(i, tar_tokenizer.index_word[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LwF7hrxLzfP"
   },
   "source": [
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiV5XAEDImlt"
   },
   "outputs": [],
   "source": [
    "class GRUEncoder(tf.keras.Model):\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.enc_emb = tf.keras.layers.Embedding(\n",
    "            cfg.data.src.vocab_size,\n",
    "            cfg.model.enc.embed_size\n",
    "        )\n",
    "        self.enc_gru = tf.keras.layers.GRU(\n",
    "            cfg.model.enc.rnn.units,\n",
    "            return_state=True,\n",
    "            return_sequences=True,\n",
    "            recurrent_initializer=\"glorot_uniform\"\n",
    "        )\n",
    "    \n",
    "    def call(self, src_tokens, state=None, training=False):\n",
    "        embed_enc = self.enc_emb(src_tokens)\n",
    "        enc_outputs, enc_states = self.enc_gru(\n",
    "            embed_enc, initial_state=state\n",
    "        )\n",
    "        return enc_outputs, enc_states\n",
    "\n",
    "\n",
    "class GRUDecoder(tf.keras.Model):\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.dec_emb = tf.keras.layers.Embedding(\n",
    "            cfg.data.tar.vocab_size,\n",
    "            cfg.model.dec.embed_size\n",
    "        )\n",
    "        self.dec_gru = tf.keras.layers.GRU(\n",
    "            cfg.model.dec.rnn.units,\n",
    "            return_state=True,\n",
    "            return_sequences=True,\n",
    "            recurrent_initializer=\"glorot_uniform\"\n",
    "        )\n",
    "        self.fc = tf.keras.layers.Dense(cfg.data.tar.vocab_size)\n",
    "    \n",
    "    def call(self, tar_tokens, state=None, training=False):\n",
    "        embed_dec = self.dec_emb(tar_tokens)\n",
    "        dec_outputs, dec_states = self.dec_gru(\n",
    "            embed_dec, initial_state=state\n",
    "        )\n",
    "        final_outputs = self.fc(dec_outputs)\n",
    "        return final_outputs, dec_states, None # None는 추후 attention 등 추가시 인터페이스 통일 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVFGsb4UgXo1"
   },
   "outputs": [],
   "source": [
    "# Attention 모델 정의\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.fc1 = tf.keras.layers.Dense(cfg.model.attention.latent_dim)\n",
    "        self.fc2 = tf.keras.layers.Dense(cfg.model.attention.latent_dim)\n",
    "        self.fc_score = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, query, value):\n",
    "        # query = hidden, value = enc_outputs\n",
    "        query_with_time_axis = tf.expand_dims(query, 1) # [B, L, hidden_dim] # 보통 L=1이다.\n",
    "\n",
    "        score = self.fc_score(\n",
    "            tf.nn.tanh(\n",
    "                self.fc1(query_with_time_axis) + self.fc2(value)\n",
    "            )\n",
    "        ) # [B, L, hidden_dim] -> [B, L, 1]\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1) # [B, L, 1]\n",
    "\n",
    "        context_vector = attention_weights * value # [B, hidden]\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "class AttentionalGRUDecoder(tf.keras.Model):\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.dec_emb = tf.keras.layers.Embedding(\n",
    "            cfg.data.tar.vocab_size,\n",
    "            cfg.model.dec.embed_size\n",
    "        )\n",
    "        self.dec_gru = tf.keras.layers.GRU(\n",
    "            cfg.model.dec.rnn.units,\n",
    "            return_state=True,\n",
    "            return_sequences=True,\n",
    "            recurrent_initializer=\"glorot_uniform\"\n",
    "        )\n",
    "        self.attention = BahdanauAttention(cfg)\n",
    "        self.fc = tf.keras.layers.Dense(cfg.data.tar.vocab_size)\n",
    "\n",
    "\n",
    "    def call(self, tar_tokens, hidden, enc_output):\n",
    "        # enc_output: [B, L, hidden_dim]\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        x = self.dec_emb(tar_tokens)\n",
    "\n",
    "        # embedding된 tar_token과 context_vector를 concat으로 합친다.\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        dec_outputs, dec_states = self.dec_gru(x)\n",
    "        dec_outputs = tf.reshape(dec_outputs, (-1, dec_outputs.shape[2])) # [B * 1, embedding_dim + hidden_dim]\n",
    "        \n",
    "        final_outputs = self.fc(dec_outputs) # [B, Vocab Size]\n",
    "        return final_outputs, dec_states, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SopEf-gWLq_d"
   },
   "source": [
    "## Configuration 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tmuu8gaLpwf"
   },
   "outputs": [],
   "source": [
    "# data configuration\n",
    "data_anki_spa_eng_cfg = {\n",
    "    \"name\": \"anki_spa_eng_cfg\",\n",
    "    \"src\": {\n",
    "        \"vocab_size\": src_vocab_size,\n",
    "        \"max_len\": max_src_len,\n",
    "    },\n",
    "    \"tar\": {\n",
    "        \"vocab_size\": tar_vocab_size,\n",
    "        \"max_len\": max_tar_len,\n",
    "    },\n",
    "    \"train_val_test_split_ratio\": [0.8, 0.1, 0.1],\n",
    "    \"train_val_shuffle\": True,\n",
    "}\n",
    "\n",
    "# model configuration\n",
    "model_translate_rnn_seq2seq_cfg = {\n",
    "    \"name\": \"RNNSeq2Seq\",\n",
    "    \"enc\": {\n",
    "        \"embed_size\": 256,\n",
    "        \"rnn\": {\n",
    "            \"units\": 1024,\n",
    "        },\n",
    "    },\n",
    "    \"dec\": {\n",
    "        \"embed_size\": 256,\n",
    "        \"rnn\": {\n",
    "            \"units\": 1024,\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "model_translate_attention_based_seq2seq_cfg = {\n",
    "    \"name\": \"AttentionBasedSeq2Seq\",\n",
    "    \"enc\": {\n",
    "        \"embed_size\": 256,\n",
    "        \"rnn\": {\n",
    "            \"units\": 1024,\n",
    "        },\n",
    "    },\n",
    "    \"dec\": {\n",
    "        \"embed_size\": 256,\n",
    "        \"rnn\": {\n",
    "            \"units\": 1024,\n",
    "        },\n",
    "    },\n",
    "    \"attention\": {\n",
    "        \"latent_dim\": 1024,\n",
    "    }\n",
    "}\n",
    "\n",
    "# optimizer_configs\n",
    "adam_warmup_lr_sch_opt_cfg = {\n",
    "    \"optimizer\": {\n",
    "        \"name\": \"Adam\",\n",
    "        \"other_kwargs\": {},\n",
    "    },\n",
    "    \"lr_scheduler\": {\n",
    "        \"name\": \"LinearWarmupLRSchedule\",\n",
    "        \"kwargs\": {\n",
    "            \"lr_peak\": 1e-3,\n",
    "            \"warmup_end_steps\": 1500,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "radam_no_lr_sch_opt_cfg = {\n",
    "    \"optimizer\": {\n",
    "        \"name\": \"RectifiedAdam\",\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"other_kwargs\": {},\n",
    "    },\n",
    "    \"lr_scheduler\": None\n",
    "}\n",
    "\n",
    "# train_cfg\n",
    "train_cfg: dict = {\n",
    "    \"train_batch_size\": 128,\n",
    "    \"val_batch_size\": 32,\n",
    "    \"test_batch_size\": 32,\n",
    "    \"max_epochs\": 50,\n",
    "    \"distribute_strategy\": \"MirroredStrategy\",\n",
    "    \"teacher_forcing_ratio\": 0.5,\n",
    "}\n",
    "\n",
    "_merged_cfg_presets = {\n",
    "    \"rnn_translate_spa_eng_radam\": {\n",
    "        \"data\": data_anki_spa_eng_cfg,\n",
    "        \"model\": model_translate_rnn_seq2seq_cfg,\n",
    "        \"opt\": radam_no_lr_sch_opt_cfg ,\n",
    "        \"train\": train_cfg\n",
    "    },\n",
    "    \"attention_based_translate_spa_eng_radam\": {\n",
    "        \"data\": data_anki_spa_eng_cfg,\n",
    "        \"model\": model_translate_attention_based_seq2seq_cfg,\n",
    "        \"opt\": radam_no_lr_sch_opt_cfg ,\n",
    "        \"train\": train_cfg\n",
    "    },\n",
    "}\n",
    "\n",
    "### hydra composition ###\n",
    "# clear hydra instance \n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "\n",
    "# register preset configs\n",
    "register_config(_merged_cfg_presets)\n",
    "\n",
    "# initialization\n",
    "hydra.initialize(config_path=None)\n",
    "\n",
    "# using_config_key = \"rnn_translate_spa_eng_radam\"\n",
    "using_config_key = \"attention_based_translate_spa_eng_radam\"\n",
    "cfg = hydra.compose(using_config_key)\n",
    "\n",
    "# define & override log_cfg\n",
    "model_name = cfg.model.name\n",
    "run_dirname = \"dnn-tutorial-spa-eng-translate-runs-tf\"\n",
    "run_name = f\"{datetime.now().isoformat(timespec='seconds')}-{using_config_key}-{model_name}\"\n",
    "log_dir = os.path.join(drive_project_root, \"runs\", run_dirname, run_name)\n",
    "\n",
    "log_cfg = {\n",
    "    \"run_name\": run_name,\n",
    "    \"checkpoint_filepath\": os.path.join(log_dir, \"model\"),\n",
    "    \"tensorboard_log_dir\": log_dir,\n",
    "    \"callbacks\": {\n",
    "        \"TensorBoard\": {\n",
    "            \"log_dir\": log_dir,\n",
    "            \"update_freq\": 50,\n",
    "        },\n",
    "        \"EarlyStopping\": {\n",
    "            \"patience\": 30,\n",
    "            \"verbose\": True,\n",
    "        }\n",
    "    },\n",
    "    \"wandb\": {\n",
    "        \"project\": \"dnn-tutorial-spa-eng-translate-runs-tf\",\n",
    "        \"name\": run_name,\n",
    "        \"tags\": [\"dnn-tutorial-spa-eng-translate-runs-tf\"],\n",
    "        \"reinit\": True,\n",
    "        \"sync_tensorboard\": True\n",
    "    },\n",
    "}\n",
    "\n",
    "# unlock struct of config & set log config\n",
    "OmegaConf.set_struct(cfg, False)\n",
    "cfg.log = log_cfg\n",
    "\n",
    "# relock config\n",
    "OmegaConf.set_struct(cfg, True)\n",
    "print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "# save yaml\n",
    "# with open(os.path.join(log_dir, \"config.yaml\")) as f:\n",
    "# with open(\"config.yaml\", \"w\") as f:\n",
    "#     OmegaConf.save(cfg, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yARForQbUcbx"
   },
   "outputs": [],
   "source": [
    "def get_distribute_strategy(strategy_name: str, **kwargs):\n",
    "    return getattr(tf.distribute, strategy_name)(**kwargs)\n",
    "\n",
    "distribute_strategy = get_distribute_strategy(cfg.train.distribute_strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X89D1QCGJXTN"
   },
   "outputs": [],
   "source": [
    "# dataset batchify 및 train/val/test splits\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_tensor, tar_tensor))\n",
    "total_n = len(src_tensor)\n",
    "\n",
    "print(cfg.data.train_val_test_split_ratio)\n",
    "train_size = int(total_n * cfg.data.train_val_test_split_ratio[0])\n",
    "val_size = int(total_n * cfg.data.train_val_test_split_ratio[1])\n",
    "test_size = total_n - (train_size + val_size)\n",
    "\n",
    "# split (train, val) (test) dataset\n",
    "test_dataset = dataset.skip(train_size + val_size)\n",
    "train_val_dataset = dataset.take(train_size + val_size)\n",
    "\n",
    "if cfg.data.train_val_shuffle:\n",
    "    train_val_dataset = train_val_dataset.shuffle(buffer_size=1024)\n",
    "\n",
    "train_dataset = train_val_dataset.take(train_size)\n",
    "val_dataset = train_val_dataset.skip(train_size)\n",
    "\n",
    "train_n, val_n, test_n = len(train_dataset), len(val_dataset), len(test_dataset)\n",
    "print(train_n, val_n, test_n)\n",
    "assert train_n + val_n + test_n == total_n\n",
    "\n",
    "# batchfy (dataloader)\n",
    "train_batch_size = cfg.train.train_batch_size\n",
    "val_batch_size = cfg.train.val_batch_size\n",
    "test_batch_size = cfg.train.test_batch_size\n",
    "\n",
    "train_dataloader = train_dataset.batch(train_batch_size, drop_remainder=True)\n",
    "val_dataloader = val_dataset.batch(val_batch_size, drop_remainder=True)\n",
    "test_dataloader = test_dataset.batch(test_batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9KyfbS4RzQ7"
   },
   "outputs": [],
   "source": [
    "# 모델 정의 \n",
    "def get_seq2seq_model(cfg: DictConfig):\n",
    "    if cfg.model.name == \"RNNSeq2Seq\":\n",
    "        encoder = GRUEncoder(cfg)\n",
    "        decoder = GRUDecoder(cfg)\n",
    "        return encoder, decoder\n",
    "    elif cfg.model.name == \"AttentionBasedSeq2Seq\":\n",
    "        encoder = GRUEncoder(cfg)\n",
    "        decoder = AttentionalGRUDecoder(cfg)\n",
    "        return encoder, decoder\n",
    "    else:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXowW1xyR8yI"
   },
   "outputs": [],
   "source": [
    "# loss 정의\n",
    "def loss_function(\n",
    "    real,\n",
    "    pred,\n",
    "    loss_object=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=\"none\"\n",
    "    )\n",
    "):\n",
    "    # delete [pad] loss part with masks. \n",
    "    mask = tf.math.logical_not(\n",
    "        tf.math.equal(real, 0)\n",
    "    )\n",
    "    _loss = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=_loss.dtype)\n",
    "    _loss *= mask\n",
    "\n",
    "    return tf.reduce_mean(_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ijO-VITS80n"
   },
   "outputs": [],
   "source": [
    "# get model\n",
    "encoder, decoder = get_seq2seq_model(cfg)\n",
    "\n",
    "# get optimizer \n",
    "optimizer, scheduler = get_optimizer_element(\n",
    "    cfg.opt.optimizer, cfg.opt.lr_scheduler\n",
    ")\n",
    "\n",
    "# checkpoints\n",
    "checkpoint_prefix = cfg.log.checkpoint_filepath\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    optimizer=optimizer,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eo3jhwhHVOqu"
   },
   "outputs": [],
   "source": [
    "# wandb setup\n",
    "wandb.init(\n",
    "    config=flatten_dict(cfg),\n",
    "    **cfg.log.wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBtJIDQ-TWHP"
   },
   "source": [
    "## Define Custom Train/Eval Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zu-skjUCTVYT"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def _step(src, tar, enc_hidden, teacher_forcing_ratio=0.5):\n",
    "\n",
    "    if cfg.model.name == \"RNNSeq2Seq\":\n",
    "        return _rnn_step(src, tar, enc_hidden, teacher_forcing_ratio)\n",
    "    elif cfg.model.name == \"AttentionBasedSeq2Seq\":\n",
    "        return _attentional_rnn_step(src, tar, enc_hidden, teacher_forcing_ratio)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "@tf.function\n",
    "def _attentional_rnn_step(src, tar, enc_hidden, teacher_forcing_ratio=0.5):\n",
    "    enc_output, enc_hidden = encoder(src, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    # add start token\n",
    "    dec_input = tf.expand_dims(\n",
    "        [tar_tokenizer.word_index[\"<start>\"]] * src.shape[0], # multiply with batch_size\n",
    "        1\n",
    "    ) # [B, 1]\n",
    "\n",
    "    outputs = []\n",
    "    loss = 0\n",
    "    # sequence 길이만큼 루프 ! (autoregressive or teacher-forcing)\n",
    "    for t in range(1, tar.shape[1]):\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "        outputs.append(predictions) # [B, Vocab_SZ]\n",
    "\n",
    "        final_outs = tf.argmax(predictions, 1) # [B]\n",
    "        ground_truth = tar[:, t] # [B]\n",
    "\n",
    "        loss += loss_function(ground_truth, predictions)\n",
    "\n",
    "        if random.random() < teacher_forcing_ratio: # teacher forcing case\n",
    "            dec_input = tf.expand_dims(ground_truth, 1)\n",
    "        else: # no teacher\n",
    "            dec_input = tf.expand_dims(final_outs, 1)\n",
    "    return loss, outputs\n",
    "\n",
    "@tf.function\n",
    "def _rnn_step(src, tar, enc_hidden, teacher_forcing_ratio=0.5):\n",
    "    \n",
    "    enc_output, enc_hidden = encoder(src, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    # add start token\n",
    "    dec_input = tf.expand_dims(\n",
    "        [tar_tokenizer.word_index[\"<start>\"]] * src.shape[0], # multiply with batch_size\n",
    "        1\n",
    "    ) # [B, 1]\n",
    "\n",
    "    outputs = []\n",
    "    loss = 0\n",
    "    # sequence 길이만큼 루프 ! (autoregressive or teacher-forcing)\n",
    "    for t in range(1, tar.shape[1]):\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden) # [B, 1, V_SZ]\n",
    "\n",
    "        outputs.append(predictions[:, 0]) # [B, V_SZ]\n",
    "        final_outs = tf.argmax(predictions, 2) # [B, 1]\n",
    "\n",
    "        ground_truth = tf.expand_dims(tar[:, t], 1) # [B, 1]\n",
    "\n",
    "        loss += loss_function(ground_truth, predictions)\n",
    "\n",
    "        if random.random() < teacher_forcing_ratio: # teacher forcing case\n",
    "            dec_input = ground_truth\n",
    "        else: # no teacher\n",
    "            dec_input = final_outs\n",
    "    \n",
    "    return loss, outputs\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(src, tar, enc_hidden, teacher_forcing_ratio=0.5):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, outputs = _step(src, tar, enc_hidden, teacher_forcing_ratio)\n",
    "    \n",
    "    batch_loss = (loss / int(tar.shape[1])) # divide with seq_len\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss, outputs\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def eval_step(src, tar, enc_hidden):\n",
    "    loss, outputs = _step(src, tar, enc_hidden, 0.0)\n",
    "    batch_loss = (loss / int(tar.shape[1])) # divide with seq_len\n",
    "    return batch_loss, outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnINJm_6WRKg"
   },
   "source": [
    "## Training/Eval Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIXfPYDeWqTH"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/drive/MyDrive/\\#fastcampus/runs/\n",
    "\n",
    "val_dataloader = iter(val_dataloader)\n",
    "steps_per_epoch = train_n // cfg.train.train_batch_size\n",
    "\n",
    "# tensorboard summary writer\n",
    "tb_writer = tf.summary.create_file_writer(\n",
    "    cfg.log.tensorboard_log_dir\n",
    ")\n",
    "\n",
    "# custom loop\n",
    "step = 0\n",
    "for epoch in range(cfg.train.max_epochs): # epoch iter\n",
    "    start = time.time()\n",
    "    total_epoch_loss = 0\n",
    "\n",
    "    for (batch, (cur_src, cur_tar)) in enumerate(\n",
    "        train_dataloader.take(steps_per_epoch)\n",
    "    ): # batch iter\n",
    "        enc_hidden = tf.zeros((\n",
    "            cfg.train.train_batch_size,\n",
    "            cfg.model.enc.rnn.units\n",
    "        ))\n",
    "        batch_loss, outputs = train_step(cur_src, cur_tar, enc_hidden)\n",
    "        total_epoch_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0 or steps_per_epoch == batch:\n",
    "            print(\"Epoch {} Batch {} Train Loss {:.4f}\".format(\n",
    "                epoch + 1,\n",
    "                batch,\n",
    "                batch_loss.numpy()\n",
    "            ))\n",
    "        \n",
    "        step += 1\n",
    "    \n",
    "    # save model per 2 epoch\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    \n",
    "    train_loss = total_epoch_loss / steps_per_epoch\n",
    "    print(\"Epoch {} Train Loss {:.4f}\".format(epoch + 1, train_loss))\n",
    "\n",
    "    with tb_writer.as_default():\n",
    "        tf.summary.scalar(\"train_loss\", train_loss, step=step)\n",
    "    \n",
    "    \n",
    "    # validation step\n",
    "    enc_hidden = tf.zeros((\n",
    "        cfg.train.val_batch_size,\n",
    "        cfg.model.enc.rnn.units\n",
    "    ))\n",
    "    cur_src, cur_tar = next(val_dataloader)\n",
    "    val_loss, outputs = eval_step(cur_src, cur_tar, enc_hidden)\n",
    "    print(\"Epoch {} Val Loss {:.4f}\".format(epoch + 1, val_loss))\n",
    "    \n",
    "    # token -> text & logging\n",
    "    preds = tf.stack(outputs, axis=1)\n",
    "    preds = tf.argmax(preds, axis=2)\n",
    "    preds = [p.numpy() for p in preds]\n",
    "\n",
    "    src_texts = src_tokenizer.sequences_to_texts(cur_src.numpy())\n",
    "    tar_texts = tar_tokenizer.sequences_to_texts(cur_tar.numpy())\n",
    "    pred_texts = tar_tokenizer.sequences_to_texts(preds)\n",
    "\n",
    "    with tb_writer.as_default():\n",
    "        tf.summary.scalar(\"val_loss\", val_loss, step=step)\n",
    "        tf.summary.text(\"val_src_text\", src_texts[0], step=step)\n",
    "        tf.summary.text(\"val_tar_text\", tar_texts[0], step=step)\n",
    "        tf.summary.text(\"val_pred_text\", pred_texts[0], step=step)\n",
    "    \n",
    "    print(f\"Time taken for 1 epoch {time.time() - start} sec\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VX-BaGMMn9Z7"
   },
   "source": [
    "## Evaluation Code Examples (Attentional RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hYtrRlan8qO"
   },
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    sentence,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    src_tokenizer,\n",
    "    tar_tokenizer,\n",
    "    max_src_len,\n",
    "    max_tar_len,\n",
    "):\n",
    "    # preprocessing sentence\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [src_tokenizer.word_index[i] for i in sentence.split(\" \")]\n",
    "    inputs = pad_sequences([inputs], maxlen=max_src_len, padding=\"post\")\n",
    "\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = \"\"\n",
    "\n",
    "    # encoder forward\n",
    "    hidden = [tf.zeros((1, encoder.cfg.model.enc.rnn.units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    # autoregressive inference of decoder\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([tar_tokenizer.word_index[\"<start>\"]], 0) # start token\n",
    "\n",
    "    attention_plot = np.zeros((max_tar_len, max_src_len))\n",
    "    for t in range(max_tar_len):\n",
    "        predictions, dec_hidden, attention_weights = decoder(\n",
    "            dec_input, dec_hidden, enc_out\n",
    "        )\n",
    "\n",
    "        # for plotting of attention weights.\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += tar_tokenizer.index_word[predicted_id] + \" \"\n",
    "\n",
    "        if tar_tokenizer.index_word[predicted_id] == \"<end>\":\n",
    "            break\n",
    "        \n",
    "        # predicted 된 id 를 모델에 다시 넣기 위해.. (autoregressive)\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    \n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "def plot_attention(attention, sentence: List, predicted_sentence: List):\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    ax.matshow(attention, cmap=\"viridis\")\n",
    "\n",
    "    fontdict = {\"fontsize\": 16}\n",
    "\n",
    "    ax.set_xticklabels([\"\"] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([\"\"] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QBIYHrTl8Xs"
   },
   "outputs": [],
   "source": [
    "# checkpoint restore\n",
    "checkpoint.restore(tf.train.latest_checkpoint(cfg.log.checkpoint_filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJ6PqFi2qx6l"
   },
   "outputs": [],
   "source": [
    "result, sentence, attention_plot = evaluate(\n",
    "    u\"Esta es mi vida.\", encoder, decoder, src_tokenizer, tar_tokenizer, max_src_len, max_tar_len\n",
    ")\n",
    "\n",
    "attention_plot = attention_plot[:len(result.split(\" \")), :len(sentence.split(\" \"))]\n",
    "plot_attention(attention_plot, sentence.split(\" \"), result.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILzmiyAyrVDR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMeTgCiJfpH5OFIydOhkgBZ",
   "collapsed_sections": [],
   "name": "06-2. Attentional RNN Seq2seq TF.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1p3dDvm1_3oOGjxKSMVLAUgss95EMZ8sf",
     "timestamp": 1636642636450
    },
    {
     "file_id": "1-Kuw2KqbixGHAmKXAQv1_FygXgZlXxE0",
     "timestamp": 1636634164801
    },
    {
     "file_id": "14UhxSABuj-M683k4TilUDhQIo39HFK-t",
     "timestamp": 1634019312693
    },
    {
     "file_id": "1CBRS5ajK2WVXMEN97owUmh6yhmljiILI",
     "timestamp": 1634015371559
    },
    {
     "file_id": "1bRDRTJmGm80o_sWIacmyt-pUnHfqOKqE",
     "timestamp": 1634012243448
    },
    {
     "file_id": "1apCHsYMGF0a1zGJeJe5WhAGyUTbehmi7",
     "timestamp": 1633794411943
    },
    {
     "file_id": "1Yq_LYjN0_deqoBFaD_ZJFfoes_VYtBVN",
     "timestamp": 1633790134053
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
