{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7CUNw8dNBwA4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/content/drive/MyDrive/#fastcampus\")\n",
    "drive_project_root = \"/content/drive/MyDrive/#fastcampus\"\n",
    "!pip install -r \"/content/drive/MyDrive/#fastcampus/requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBxZXWDEHbkh"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from omegaconf import OmegaConf\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdeNqGVRIJzv"
   },
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mR6QoAFuIMY0"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zawdEF0vIO-9"
   },
   "outputs": [],
   "source": [
    "# define gpus strategy\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZ_NevzbKGBk"
   },
   "source": [
    "## 데이터 및 데이터로더 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X89D1QCGJXTN"
   },
   "outputs": [],
   "source": [
    "with mirrored_strategy.scope():\n",
    "    # 데이터 셋 정의 \n",
    "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    \n",
    "    # normalization\n",
    "    \n",
    "    # turn off for efficient_net\n",
    "    # x_train = x_train / 255.0\n",
    "    # x_test = x_test / 255.0\n",
    "\n",
    "    # train/val splits\n",
    "    train_size = int(len(x_train) * 0.9)\n",
    "    val_size = len(x_train) - train_size\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size=1024)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).shuffle(buffer_size=1024)\n",
    "\n",
    "    train_dataset = dataset.take(train_size)\n",
    "    val_dataset = dataset.skip(train_size)\n",
    "    print(len(train_dataset), len(val_dataset), len(dataset), len(test_dataset))\n",
    "    \n",
    "    # dataloader 정의\n",
    "    train_batch_size = 100\n",
    "    val_batch_size = 10\n",
    "    test_batch_size = 100\n",
    "\n",
    "    train_dataloader = train_dataset.batch(train_batch_size, drop_remainder=True)\n",
    "    val_dataloader = val_dataset.batch(val_batch_size, drop_remainder=True)\n",
    "    test_dataloader = test_dataset.batch(test_batch_size, drop_remainder=True)\n",
    "\n",
    "sample_example = next(iter(train_dataloader))\n",
    "print(sample_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGRtSVzNJmck"
   },
   "outputs": [],
   "source": [
    "# plot figure\n",
    "plt.figure(figsize=(10, 10))\n",
    "for c in range(16):\n",
    "    plt.subplot(4, 4, c+1)\n",
    "    plt.imshow(x_train[c].reshape(28, 28), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LwF7hrxLzfP"
   },
   "source": [
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54FXO6o1J654"
   },
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self, input_dim: int, h1_dim: int, h2_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.linear1 = tf.keras.layers.Dense(input_dim=input_dim, units=h1_dim)\n",
    "        self.linear2 = tf.keras.layers.Dense(units=h2_dim)\n",
    "        self.linear3 = tf.keras.layers.Dense(units=out_dim)\n",
    "        self.relu = tf.nn.relu\n",
    "    \n",
    "    def call(self, input, training=False):\n",
    "        x = self.flatten(input)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        out = self.linear3(x)\n",
    "        out = tf.nn.softmax(out)\n",
    "        return out\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        images, labels = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self(images, training=True)\n",
    "            preds = tf.argmax(outputs, 1)\n",
    "\n",
    "            loss = self.compiled_loss(\n",
    "                labels, outputs\n",
    "            )\n",
    "        \n",
    "        # compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # update the metrics\n",
    "        self.compiled_metrics.update_state(labels, preds)\n",
    "\n",
    "        # return a dict mapping metrics names to current values\n",
    "        logs = {m.name: m.result() for m in self.metrics}\n",
    "        logs.update({\"loss\": loss})\n",
    "        return logs\n",
    "\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        images, labels = data\n",
    "        outputs = self(images, training=False)\n",
    "        preds = tf.argmax(outputs, 1)\n",
    "        loss = self.compiled_loss(\n",
    "            labels, outputs\n",
    "        )\n",
    "\n",
    "        # update the metrics\n",
    "        self.compiled_metrics.update_state(labels, preds)\n",
    "\n",
    "        # return a dict mapping metrics names to current values\n",
    "        logs = {m.name: m.result() for m in self.metrics}       \n",
    "        logs.update({\"test_loss\": loss})\n",
    "        return logs      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJI8kHykffHy"
   },
   "outputs": [],
   "source": [
    "class MLPWithDropout(tf.keras.Model):\n",
    "    def __init__(self, input_dim: int, h1_dim: int, h2_dim: int, out_dim: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.linear1 = tf.keras.layers.Dense(input_dim=input_dim, units=h1_dim)\n",
    "        self.linear2 = tf.keras.layers.Dense(units=h2_dim)\n",
    "        self.linear3 = tf.keras.layers.Dense(units=out_dim)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_prob)\n",
    "        self.relu = tf.nn.relu\n",
    "    \n",
    "    def call(self, input, training=False):\n",
    "        x = self.flatten(input)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.dropout(x, training=training)\n",
    "        out = self.linear3(x)\n",
    "        out = tf.nn.softmax(out)\n",
    "        return out\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        images, labels = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self(images, training=True)\n",
    "            preds = tf.argmax(outputs, 1)\n",
    "\n",
    "            loss = self.compiled_loss(\n",
    "                labels, outputs\n",
    "            )\n",
    "        \n",
    "        # compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # update the metrics\n",
    "        self.compiled_metrics.update_state(labels, preds)\n",
    "\n",
    "        # return a dict mapping metrics names to current values\n",
    "        logs = {m.name: m.result() for m in self.metrics}\n",
    "        return logs\n",
    "\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        images, labels = data\n",
    "        outputs = self(images, training=False)\n",
    "        preds = tf.argmax(outputs, 1)\n",
    "        loss = self.compiled_loss(\n",
    "            labels, outputs\n",
    "        )\n",
    "\n",
    "        # update the metrics\n",
    "        self.compiled_metrics.update_state(labels, preds)\n",
    "\n",
    "        # return a dict mapping metrics names to current values\n",
    "        logs = {m.name: m.result() for m in self.metrics}\n",
    "        return logs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNXZzmTwvUll"
   },
   "outputs": [],
   "source": [
    "_cnn_cfg: dict = {\n",
    "    \"layer_1\": {\n",
    "        \"conv2d_filters\": 32,\n",
    "        \"conv2d_kernel_size\": [3, 3],\n",
    "        \"conv2d_strides\": [1, 1],\n",
    "        \"conv2d_padding\": \"same\",\n",
    "        \"maxpool2d_pool_size\": [2, 2],\n",
    "        \"maxpool2d_strides\": [2, 2],\n",
    "        \"maxpool2d_padding\": \"valid\",\n",
    "    },\n",
    "    \"layer_2\": {\n",
    "        \"conv2d_filters\": 64,\n",
    "        \"conv2d_kernel_size\": [3, 3],\n",
    "        \"conv2d_strides\": [1, 1],\n",
    "        \"conv2d_padding\": \"valid\",\n",
    "        \"maxpool2d_pool_size\": [2, 2],\n",
    "        \"maxpool2d_strides\": [1, 1],\n",
    "        \"maxpool2d_padding\": \"valid\",\n",
    "    },\n",
    "    \"fc_1\": {\"units\": 512},\n",
    "    \"fc_2\": {\"units\": 128},\n",
    "    \"fc_3\": {\"units\": 10},\n",
    "    \"dropout_prob\": 0.25,\n",
    "}\n",
    "_cnn_cfg = OmegaConf.create(_cnn_cfg)\n",
    "print(OmegaConf.to_yaml(_cnn_cfg))\n",
    "\n",
    "class ConvBatchNormMaxPool(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv2d_filters,\n",
    "        conv2d_kernel_size,\n",
    "        conv2d_strides,\n",
    "        conv2d_padding,\n",
    "        maxpool2d_pool_size,\n",
    "        maxpool2d_strides,\n",
    "        maxpool2d_padding,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv2d = tf.keras.layers.Conv2D(\n",
    "            filters=conv2d_filters,\n",
    "            kernel_size=conv2d_kernel_size,\n",
    "            strides=conv2d_strides,\n",
    "            padding=conv2d_padding,\n",
    "        )\n",
    "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "        self.maxpool2d = tf.keras.layers.MaxPool2D(\n",
    "            pool_size = maxpool2d_pool_size,\n",
    "            strides=maxpool2d_strides,\n",
    "            padding=maxpool2d_padding,\n",
    "        )\n",
    "    \n",
    "    def call(self, input):\n",
    "        \"\"\"Conv2D --> BatchNormalization --> Activation --> Maxpooling \"\"\"\n",
    "        x = self.conv2d(input)\n",
    "        x = self.batchnorm(x)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        out = self.maxpool2d(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CNN(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, cfg: DictConfig = _cnn_cfg\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layer1 = ConvBatchNormMaxPool(\n",
    "            cfg.layer_1.conv2d_filters,\n",
    "            cfg.layer_1.conv2d_kernel_size,\n",
    "            cfg.layer_1.conv2d_strides,\n",
    "            cfg.layer_1.conv2d_padding,\n",
    "            cfg.layer_1.maxpool2d_pool_size,\n",
    "            cfg.layer_1.maxpool2d_strides,\n",
    "            cfg.layer_1.maxpool2d_padding,\n",
    "        )\n",
    "        self.layer2 = ConvBatchNormMaxPool(\n",
    "            cfg.layer_2.conv2d_filters,\n",
    "            cfg.layer_2.conv2d_kernel_size,\n",
    "            cfg.layer_2.conv2d_strides,\n",
    "            cfg.layer_2.conv2d_padding,\n",
    "            cfg.layer_2.maxpool2d_pool_size,\n",
    "            cfg.layer_2.maxpool2d_strides,\n",
    "            cfg.layer_2.maxpool2d_padding,\n",
    "        )\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(cfg.fc_1.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(cfg.fc_2.units)\n",
    "        self.fc3 = tf.keras.layers.Dense(cfg.fc_3.units)\n",
    "        self.dropout = tf.keras.layers.Dropout(cfg.dropout_prob)\n",
    "        \n",
    "    def call(self, input, training=False):\n",
    "        input = tf.expand_dims(input, -1)  # [B X 28 X 28 X 1]\n",
    "        x = self.layer1(input)\n",
    "        x = self.layer2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.fc2(x)\n",
    "        out = self.fc3(x)\n",
    "        out = tf.nn.softmax(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "    def train_step(self, data):\n",
    "        images, labels = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # images = [B X 28 X 28]\n",
    "            outputs = self(images, training=True)\n",
    "            preds = tf.argmax(outputs, 1)\n",
    "\n",
    "            loss = self.compiled_loss(\n",
    "                labels, outputs\n",
    "            )\n",
    "        \n",
    "        # compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # update the metrics\n",
    "        self.compiled_metrics.update_state(labels, preds)\n",
    "\n",
    "        # return a dict mapping metrics names to current values\n",
    "        logs = {m.name: m.result() for m in self.metrics}\n",
    "        logs.update({\"loss\": loss})\n",
    "        return logs\n",
    "\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        images, labels = data\n",
    "        outputs = self(images, training=False)\n",
    "        preds = tf.argmax(outputs, 1)\n",
    "        loss = self.compiled_loss(\n",
    "            labels, outputs\n",
    "        )\n",
    "\n",
    "        # update the metrics\n",
    "        self.compiled_metrics.update_state(labels, preds)\n",
    "\n",
    "        # return a dict mapping metrics names to current values\n",
    "        logs = {m.name: m.result() for m in self.metrics}       \n",
    "        logs.update({\"test_loss\": loss})\n",
    "        return logs      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-c9u4e_6-kj"
   },
   "outputs": [],
   "source": [
    "_efficient_finetune_cfg: dict = {\n",
    "    \"efficient_net_model_name\": \"EfficientNetB0\",\n",
    "    \"classes\": 10,\n",
    "    \"efficient_net_weight_trainable\": True,\n",
    "    \"kwargs\": {\n",
    "        \"include_top\": False,\n",
    "        \"weights\": 'imagenet',\n",
    "    }\n",
    "}\n",
    "_efficient_finetune_cfg = OmegaConf.create(_efficient_finetune_cfg)\n",
    "print(OmegaConf.to_yaml(_efficient_finetune_cfg))\n",
    "\n",
    "class EfficientNetFinetune(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, cfg: DictConfig = _efficient_finetune_cfg\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.efficientnet = getattr(\n",
    "            tf.keras.applications.efficientnet,\n",
    "            cfg.efficient_net_model_name\n",
    "        )(**cfg.kwargs)\n",
    "        self.efficientnet.trainable = cfg.efficient_net_weight_trainable\n",
    "\n",
    "        self.resize = tf.keras.layers.Resizing(224, 224)\n",
    "        self.avgpool = tf.keras.layers.GlobalAveragePooling2D(name=\"avg_pool\")\n",
    "        self.out_dense = tf.keras.layers.Dense(units=cfg.classes)\n",
    "\n",
    "        \n",
    "    def call(self, input, training=False):\n",
    "        input = tf.expand_dims(input, -1) # [B, 28, 28, 1]\n",
    "        x = self.resize(input) # [B, 224, 224, 1]\n",
    "        x = tf.stack([x, x, x], axis=-2) # [B, 224, 224, 3]\n",
    "        x = self.efficientnet(x, training=training)\n",
    "\n",
    "        # build top\n",
    "        x = self.avgpool(x)\n",
    "        out = self.out_dense(x)\n",
    "        out = tf.nn.softmax(out)\n",
    "        return out\n",
    "\n",
    "    def train_step(self, data):\n",
    "        images, labels = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # images = [B X 28 X 28]\n",
    "            outputs = self(images, training=True)\n",
    "            preds = tf.argmax(outputs, 1)\n",
    "\n",
    "            loss = self.compiled_loss(\n",
    "                labels, outputs\n",
    "            )\n",
    "        \n",
    "        # compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # update the metrics\n",
    "        self.compiled_metrics.update_state(labels, preds)\n",
    "\n",
    "        # return a dict mapping metrics names to current values\n",
    "        logs = {m.name: m.result() for m in self.metrics}\n",
    "        logs.update({\"loss\": loss})\n",
    "        return logs\n",
    "\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        images, labels = data\n",
    "        outputs = self(images, training=False)\n",
    "        preds = tf.argmax(outputs, 1)\n",
    "        loss = self.compiled_loss(\n",
    "            labels, outputs\n",
    "        )\n",
    "\n",
    "        # update the metrics\n",
    "        self.compiled_metrics.update_state(labels, preds)\n",
    "\n",
    "        # return a dict mapping metrics names to current values\n",
    "        logs = {m.name: m.result() for m in self.metrics}       \n",
    "        logs.update({\"test_loss\": loss})\n",
    "        return logs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSSFQy9Cwo9C"
   },
   "outputs": [],
   "source": [
    "# learning rate scheduler 구현! \n",
    "class LinearWarmupLRScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr_peak: float,\n",
    "        warmup_end_steps: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lr_peak = lr_peak\n",
    "        self.warmup_end_steps = warmup_end_steps\n",
    "        \n",
    "    def __call__(self, step):\n",
    "\n",
    "        step_float = tf.cast(step, tf.float32)\n",
    "        warmup_end_steps = tf.cast(self.warmup_end_steps, tf.float32)\n",
    "        lr_peak = tf.cast(self.lr_peak, tf.float32)\n",
    "\n",
    "        return tf.cond(\n",
    "            step_float < warmup_end_steps, \n",
    "            lambda: lr_peak * (tf.math.maximum(step_float, 1) / warmup_end_steps) , # if Ture\n",
    "            lambda: lr_peak, # else\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9KyfbS4RzQ7"
   },
   "outputs": [],
   "source": [
    "# 모델 정의 \n",
    "n_class = 10\n",
    "max_epoch = 50\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    # model = MLP(28*28*1, 128, 64, n_class)\n",
    "    # model = MLPWithDropout(28*28*1, 128, 64, n_class, dropout_prob=0.3)\n",
    "    # model = CNN()\n",
    "    model = EfficientNetFinetune()\n",
    "    model_name = type(model).__name__\n",
    "\n",
    "    # define loss\n",
    "    loss_function = tf.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "    # define optimizer\n",
    "    lr = 1e-3\n",
    "    scheduler = None\n",
    "    # scheduler = LinearWarmupLRScheduler(lr, 1500)\n",
    "    scheduler_name = type(scheduler).__name__ if scheduler is not None else \"no_scheduler\"\n",
    "    if scheduler is None:\n",
    "        scheduler = lr\n",
    "\n",
    "    # optimizer = tf.optimizers.Adam(learning_rate=scheduler)\n",
    "    # optimizer = tf.optimizers.SGD(learning_rate=scheduler)\n",
    "    optimizer = tfa.optimizers.RectifiedAdam(learning_rate=scheduler)\n",
    "    # optimizer = AdamP(learning_rate=scheduler)\n",
    "    optimizer_name = type(optimizer).__name__\n",
    "\n",
    "    model.compile(\n",
    "        loss=loss_function,\n",
    "        optimizer=optimizer,\n",
    "        metrics=[tf.keras.metrics.Accuracy()],\n",
    "    )\n",
    "\n",
    "    model.build((1, 28, 28))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eo3jhwhHVOqu"
   },
   "outputs": [],
   "source": [
    "# define logging & callbacks\n",
    "log_interval = 100\n",
    "# run_name = f\"{datetime.now()}-{model_name}-{optimizer_name}-optim-{lr}_lr_with_{scheduler_name}\"\n",
    "efficient_net_weight_trainable = _efficient_finetune_cfg.efficient_net_weight_trainable\n",
    "run_name = f\"{datetime.now()}-{model_name}-{efficient_net_weight_trainable}\"\n",
    "run_dirname = \"dnn-tutorial-fashion-mnist-runs-tf\"\n",
    "log_dir = os.path.join(drive_project_root, \"runs\", run_dirname, run_name)\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir, update_freq=log_interval\n",
    ")\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=3, verbose=True)\n",
    "\n",
    "# wandb setup\n",
    "project_name = \"fastcampus_fashion_mnist_tutorials_tf\"\n",
    "run_tags = [project_name]\n",
    "\n",
    "wandb.init(\n",
    "    project=project_name,\n",
    "    name=run_name,\n",
    "    tags=run_tags,\n",
    "    config={\n",
    "        \"lr\": lr,\n",
    "        \"model_name\": model_name,\n",
    "        \"optimizer_name\": optimizer_name,\n",
    "        \"scheduler_name\": scheduler_name,\n",
    "    },\n",
    "    reinit=True,\n",
    "    sync_tensorboard=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIXfPYDeWqTH"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/drive/MyDrive/\\#fastcampus/runs/\n",
    "\n",
    "model.fit(\n",
    "    train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    epochs=max_epoch,\n",
    "    callbacks=[tb_callback, early_stop_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5SnIKTKVOab"
   },
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bt7d0YMxXd5w"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3NIlcw5YBJc"
   },
   "outputs": [],
   "source": [
    "test_labels_list = []\n",
    "test_preds_list = []\n",
    "test_outputs_list = []\n",
    "\n",
    "for i, (test_images, test_labels) in enumerate(tqdm(test_dataloader, position=0, leave=True, desc=\"testing\")):\n",
    "    with mirrored_strategy.scope():\n",
    "        test_outputs = model(test_images)\n",
    "    test_preds = tf.argmax(test_outputs, 1)\n",
    "\n",
    "    final_outs = test_outputs.numpy()\n",
    "    test_outputs_list.extend(final_outs)\n",
    "    test_preds_list.extend(test_preds.numpy())\n",
    "    test_labels_list.extend(test_labels.numpy())\n",
    "\n",
    "test_preds_list = np.array(test_preds_list)\n",
    "test_labels_list = np.array(test_labels_list)\n",
    "\n",
    "test_accuracy = np.mean(test_preds_list == test_labels_list)\n",
    "print(f\"\\nacc: {test_accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7TXScWnZAPR"
   },
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh = {}\n",
    "n_class = 10\n",
    "\n",
    "for i in range(n_class):\n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(test_labels_list, np.array(test_outputs_list)[:, i], pos_label=i)\n",
    "\n",
    "# plot\n",
    "for i in range(n_class):\n",
    "    plt.plot(fpr[i], tpr[i], linestyle=\"--\", label=f\"Class {i} vs Rest\")\n",
    "plt.title(\"Multi-class ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "auc_score = roc_auc_score(test_labels_list, test_outputs_list, multi_class=\"ovo\", average=\"macro\")\n",
    "print(f\"auc_score: {auc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b-2TiKdZdWd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qau4Vi0zZgm3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMmgZCzvntixEK8g73hUHCx",
   "collapsed_sections": [],
   "name": "05-2. Efficient_Net_Finetune_TF.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1CBRS5ajK2WVXMEN97owUmh6yhmljiILI",
     "timestamp": 1634015371559
    },
    {
     "file_id": "1bRDRTJmGm80o_sWIacmyt-pUnHfqOKqE",
     "timestamp": 1634012243448
    },
    {
     "file_id": "1apCHsYMGF0a1zGJeJe5WhAGyUTbehmi7",
     "timestamp": 1633794411943
    },
    {
     "file_id": "1Yq_LYjN0_deqoBFaD_ZJFfoes_VYtBVN",
     "timestamp": 1633790134053
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
