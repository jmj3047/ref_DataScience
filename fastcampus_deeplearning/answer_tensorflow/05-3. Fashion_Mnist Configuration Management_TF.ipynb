{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7CUNw8dNBwA4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/content/drive/MyDrive/#fastcampus\")\n",
    "drive_project_root = \"/content/drive/MyDrive/#fastcampus\"\n",
    "!pip install -r \"/content/drive/MyDrive/#fastcampus/requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBxZXWDEHbkh"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from omegaconf import OmegaConf\n",
    "from omegaconf import DictConfig\n",
    "import hydra\n",
    "from hydra.core.config_store import ConfigStore\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlfSWYBuJvXi"
   },
   "outputs": [],
   "source": [
    "from config_utils_tf import flatten_dict\n",
    "from config_utils_tf import register_config\n",
    "from config_utils_tf import get_optimizer_element\n",
    "from config_utils_tf import get_callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdeNqGVRIJzv"
   },
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mR6QoAFuIMY0"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aknbjV1NLAGD"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZ_NevzbKGBk"
   },
   "source": [
    "## 데이터 및 데이터로더 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LwF7hrxLzfP"
   },
   "source": [
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54FXO6o1J654"
   },
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.linear1 = tf.keras.layers.Dense(input_dim=cfg.input_dim, units=cfg.h1_dim)\n",
    "        self.linear2 = tf.keras.layers.Dense(units=cfg.h2_dim)\n",
    "        self.linear3 = tf.keras.layers.Dense(units=cfg.out_dim)\n",
    "        self.relu = tf.nn.relu\n",
    "    \n",
    "    def call(self, input, training=False):\n",
    "        x = self.flatten(input)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        out = self.linear3(x)\n",
    "        out = tf.nn.softmax(out)\n",
    "        return out\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        images, labels = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self(images, training=True)\n",
    "            preds = tf.argmax(outputs, 1)\n",
    "\n",
    "            loss = self.compiled_loss(\n",
    "                labels, outputs\n",
    "            )\n",
    "        \n",
    "        # compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # update the metrics\n",
    "        self.compiled_metrics.update_state(labels, preds)\n",
    "\n",
    "        # return a dict mapping metrics names to current values\n",
    "        logs = {m.name: m.result() for m in self.metrics}\n",
    "        logs.update({\"loss\": loss})\n",
    "        return logs\n",
    "\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        images, labels = data\n",
    "        outputs = self(images, training=False)\n",
    "        preds = tf.argmax(outputs, 1)\n",
    "        loss = self.compiled_loss(\n",
    "            labels, outputs\n",
    "        )\n",
    "\n",
    "        # update the metrics\n",
    "        self.compiled_metrics.update_state(labels, preds)\n",
    "\n",
    "        # return a dict mapping metrics names to current values\n",
    "        logs = {m.name: m.result() for m in self.metrics}       \n",
    "        logs.update({\"test_loss\": loss})\n",
    "        return logs      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJI8kHykffHy"
   },
   "outputs": [],
   "source": [
    "class MLPWithDropout(tf.keras.Model):\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.linear1 = tf.keras.layers.Dense(input_dim=cfg.input_dim, units=cfg.h1_dim)\n",
    "        self.linear2 = tf.keras.layers.Dense(units=cfg.h2_dim)\n",
    "        self.linear3 = tf.keras.layers.Dense(units=cfg.out_dim)\n",
    "        self.dropout = tf.keras.layers.Dropout(cfg.dropout_prob)\n",
    "        self.relu = tf.nn.relu\n",
    "    \n",
    "    def call(self, input, training=False):\n",
    "        x = self.flatten(input)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.dropout(x, training=training)\n",
    "        out = self.linear3(x)\n",
    "        out = tf.nn.softmax(out)\n",
    "        return out\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        images, labels = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self(images, training=True)\n",
    "            preds = tf.argmax(outputs, 1)\n",
    "\n",
    "            loss = self.compiled_loss(\n",
    "                labels, outputs\n",
    "            )\n",
    "        \n",
    "        # compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # update the metrics\n",
    "        self.compiled_metrics.update_state(labels, preds)\n",
    "\n",
    "        # return a dict mapping metrics names to current values\n",
    "        logs = {m.name: m.result() for m in self.metrics}\n",
    "        return logs\n",
    "\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        images, labels = data\n",
    "        outputs = self(images, training=False)\n",
    "        preds = tf.argmax(outputs, 1)\n",
    "        loss = self.compiled_loss(\n",
    "            labels, outputs\n",
    "        )\n",
    "\n",
    "        # update the metrics\n",
    "        self.compiled_metrics.update_state(labels, preds)\n",
    "\n",
    "        # return a dict mapping metrics names to current values\n",
    "        logs = {m.name: m.result() for m in self.metrics}\n",
    "        return logs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNXZzmTwvUll"
   },
   "outputs": [],
   "source": [
    "class ConvBatchNormMaxPool(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv2d_filters,\n",
    "        conv2d_kernel_size,\n",
    "        conv2d_strides,\n",
    "        conv2d_padding,\n",
    "        maxpool2d_pool_size,\n",
    "        maxpool2d_strides,\n",
    "        maxpool2d_padding,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv2d = tf.keras.layers.Conv2D(\n",
    "            filters=conv2d_filters,\n",
    "            kernel_size=conv2d_kernel_size,\n",
    "            strides=conv2d_strides,\n",
    "            padding=conv2d_padding,\n",
    "        )\n",
    "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "        self.maxpool2d = tf.keras.layers.MaxPool2D(\n",
    "            pool_size = maxpool2d_pool_size,\n",
    "            strides=maxpool2d_strides,\n",
    "            padding=maxpool2d_padding,\n",
    "        )\n",
    "    \n",
    "    def call(self, input):\n",
    "        \"\"\"Conv2D --> BatchNormalization --> Activation --> Maxpooling \"\"\"\n",
    "        x = self.conv2d(input)\n",
    "        x = self.batchnorm(x)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        out = self.maxpool2d(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CNN(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, cfg: DictConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layer1 = ConvBatchNormMaxPool(\n",
    "            cfg.layer_1.conv2d_filters,\n",
    "            cfg.layer_1.conv2d_kernel_size,\n",
    "            cfg.layer_1.conv2d_strides,\n",
    "            cfg.layer_1.conv2d_padding,\n",
    "            cfg.layer_1.maxpool2d_pool_size,\n",
    "            cfg.layer_1.maxpool2d_strides,\n",
    "            cfg.layer_1.maxpool2d_padding,\n",
    "        )\n",
    "        self.layer2 = ConvBatchNormMaxPool(\n",
    "            cfg.layer_2.conv2d_filters,\n",
    "            cfg.layer_2.conv2d_kernel_size,\n",
    "            cfg.layer_2.conv2d_strides,\n",
    "            cfg.layer_2.conv2d_padding,\n",
    "            cfg.layer_2.maxpool2d_pool_size,\n",
    "            cfg.layer_2.maxpool2d_strides,\n",
    "            cfg.layer_2.maxpool2d_padding,\n",
    "        )\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(cfg.fc_1.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(cfg.fc_2.units)\n",
    "        self.fc3 = tf.keras.layers.Dense(cfg.fc_3.units)\n",
    "        self.dropout = tf.keras.layers.Dropout(cfg.dropout_prob)\n",
    "        \n",
    "    def call(self, input, training=False):\n",
    "        input = tf.expand_dims(input, -1)  # [B X 28 X 28 X 1]\n",
    "        x = self.layer1(input)\n",
    "        x = self.layer2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.fc2(x)\n",
    "        out = self.fc3(x)\n",
    "        out = tf.nn.softmax(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "    def train_step(self, data):\n",
    "        images, labels = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # images = [B X 28 X 28]\n",
    "            outputs = self(images, training=True)\n",
    "            preds = tf.argmax(outputs, 1)\n",
    "\n",
    "            loss = self.compiled_loss(\n",
    "                labels, outputs\n",
    "            )\n",
    "        \n",
    "        # compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # update the metrics\n",
    "        self.compiled_metrics.update_state(labels, preds)\n",
    "\n",
    "        # return a dict mapping metrics names to current values\n",
    "        logs = {m.name: m.result() for m in self.metrics}\n",
    "        logs.update({\"loss\": loss})\n",
    "        return logs\n",
    "\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        images, labels = data\n",
    "        outputs = self(images, training=False)\n",
    "        preds = tf.argmax(outputs, 1)\n",
    "        loss = self.compiled_loss(\n",
    "            labels, outputs\n",
    "        )\n",
    "\n",
    "        # update the metrics\n",
    "        self.compiled_metrics.update_state(labels, preds)\n",
    "\n",
    "        # return a dict mapping metrics names to current values\n",
    "        logs = {m.name: m.result() for m in self.metrics}       \n",
    "        logs.update({\"test_loss\": loss})\n",
    "        return logs      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-c9u4e_6-kj"
   },
   "outputs": [],
   "source": [
    "class EfficientNetFinetune(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, cfg: DictConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.efficientnet = getattr(\n",
    "            tf.keras.applications.efficientnet,\n",
    "            cfg.efficient_net_model_name\n",
    "        )(**cfg.kwargs)\n",
    "        self.efficientnet.trainable = cfg.efficient_net_weight_trainable\n",
    "\n",
    "        self.resize = tf.keras.layers.Resizing(224, 224)\n",
    "        self.avgpool = tf.keras.layers.GlobalAveragePooling2D(name=\"avg_pool\")\n",
    "        self.out_dense = tf.keras.layers.Dense(units=cfg.classes)\n",
    "\n",
    "        \n",
    "    def call(self, input, training=False):\n",
    "        input = tf.expand_dims(input, -1) # [B, 28, 28, 1]\n",
    "        x = self.resize(input) # [B, 224, 224, 1]\n",
    "        x = tf.stack([x, x, x], axis=-2) # [B, 224, 224, 3]\n",
    "        x = self.efficientnet(x, training=training)\n",
    "\n",
    "        # build top\n",
    "        x = self.avgpool(x)\n",
    "        out = self.out_dense(x)\n",
    "        out = tf.nn.softmax(out)\n",
    "        return out\n",
    "\n",
    "    def train_step(self, data):\n",
    "        images, labels = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # images = [B X 28 X 28]\n",
    "            outputs = self(images, training=True)\n",
    "            preds = tf.argmax(outputs, 1)\n",
    "\n",
    "            loss = self.compiled_loss(\n",
    "                labels, outputs\n",
    "            )\n",
    "        \n",
    "        # compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # update the metrics\n",
    "        self.compiled_metrics.update_state(labels, preds)\n",
    "\n",
    "        # return a dict mapping metrics names to current values\n",
    "        logs = {m.name: m.result() for m in self.metrics}\n",
    "        logs.update({\"loss\": loss})\n",
    "        return logs\n",
    "\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        images, labels = data\n",
    "        outputs = self(images, training=False)\n",
    "        preds = tf.argmax(outputs, 1)\n",
    "        loss = self.compiled_loss(\n",
    "            labels, outputs\n",
    "        )\n",
    "\n",
    "        # update the metrics\n",
    "        self.compiled_metrics.update_state(labels, preds)\n",
    "\n",
    "        # return a dict mapping metrics names to current values\n",
    "        logs = {m.name: m.result() for m in self.metrics}       \n",
    "        logs.update({\"test_loss\": loss})\n",
    "        return logs    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SopEf-gWLq_d"
   },
   "source": [
    "## Configuration 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tmuu8gaLpwf"
   },
   "outputs": [],
   "source": [
    "# data configuration\n",
    "data_fashion_mnist_cfg: dict = {\n",
    "    \"n_class\": 10,\n",
    "    \"train_val_split\": [0.9, 0.1],\n",
    "    \"train_val_shuffle\": True,\n",
    "    \"train_val_shuffle_buffer_size\": 1024,\n",
    "    \"test_shuffle\": False,\n",
    "    \"test_shuffle_buffer_size\": 1024,\n",
    "}\n",
    "\n",
    "# model configuration\n",
    "model_mnist_mlp_cfg: dict = {\n",
    "    \"name\": \"MLP\",\n",
    "    \"data_normalize\": True,\n",
    "    \"input_dim\": 28*28*1,\n",
    "    \"h1_dim\": 128,\n",
    "    \"h2_dim\": 64,\n",
    "    \"out_dim\": 10,\n",
    "}\n",
    "\n",
    "model_mnist_mlp_with_dropout_cfg: dict = {\n",
    "    \"name\": \"MLPWithDropout\",\n",
    "    \"data_normalize\": True,\n",
    "    \"input_dim\": 28*28*1,\n",
    "    \"h1_dim\": 128,\n",
    "    \"h2_dim\": 64,\n",
    "    \"out_dim\": 10,\n",
    "    \"dropout_prob\": 0.3,\n",
    "}\n",
    "\n",
    "model_mnist_cnn_cfg: dict = {\n",
    "    \"name\": \"CNN\",\n",
    "    \"data_normalize\": True,\n",
    "    \"layer_1\": {\n",
    "        \"conv2d_filters\": 32,\n",
    "        \"conv2d_kernel_size\": [3, 3],\n",
    "        \"conv2d_strides\": [1, 1],\n",
    "        \"conv2d_padding\": \"same\",\n",
    "        \"maxpool2d_pool_size\": [2, 2],\n",
    "        \"maxpool2d_strides\": [2, 2],\n",
    "        \"maxpool2d_padding\": \"valid\",\n",
    "    },\n",
    "    \"layer_2\": {\n",
    "        \"conv2d_filters\": 64,\n",
    "        \"conv2d_kernel_size\": [3, 3],\n",
    "        \"conv2d_strides\": [1, 1],\n",
    "        \"conv2d_padding\": \"valid\",\n",
    "        \"maxpool2d_pool_size\": [2, 2],\n",
    "        \"maxpool2d_strides\": [1, 1],\n",
    "        \"maxpool2d_padding\": \"valid\",\n",
    "    },\n",
    "    \"fc_1\": {\"units\": 512},\n",
    "    \"fc_2\": {\"units\": 128},\n",
    "    \"fc_3\": {\"units\": 10},\n",
    "    \"dropout_prob\": 0.25,\n",
    "}\n",
    "\n",
    "model_mnist_efficient_finetune_cfg: dict = {\n",
    "    \"name\": \"EfficientNetFinetune\",\n",
    "    \"data_normalize\": False,\n",
    "    \"efficient_net_model_name\": \"EfficientNetB0\",\n",
    "    \"classes\": 10,\n",
    "    \"efficient_net_weight_trainable\": True,\n",
    "    \"kwargs\": {\n",
    "        \"include_top\": False,\n",
    "        \"weights\": 'imagenet',\n",
    "    }\n",
    "}\n",
    "\n",
    "# optimizer_configs\n",
    "adam_warmup_lr_sch_opt_cfg = {\n",
    "    \"optimizer\": {\n",
    "        \"name\": \"Adam\",\n",
    "        \"other_kwargs\": {},\n",
    "    },\n",
    "    \"lr_scheduler\": {\n",
    "        \"name\": \"LinearWarmupLRSchedule\",\n",
    "        \"kwargs\": {\n",
    "            \"lr_peak\": 1e-3,\n",
    "            \"warmup_end_steps\": 1500,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "radam_no_lr_sch_opt_cfg = {\n",
    "    \"optimizer\": {\n",
    "        \"name\": \"RectifiedAdam\",\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"other_kwargs\": {},\n",
    "    },\n",
    "    \"lr_scheduler\": None\n",
    "}\n",
    "\n",
    "# train_cfg\n",
    "train_cfg: dict = {\n",
    "    \"train_batch_size\": 128,\n",
    "    \"val_batch_size\": 32,\n",
    "    \"test_batch_size\": 32,\n",
    "    \"max_epochs\": 50,\n",
    "    \"distribute_strategy\": \"MirroredStrategy\",\n",
    "}\n",
    "\n",
    "_merged_cfg_presets = {\n",
    "    \"cnn_fashion_mnist_radam\": {\n",
    "        \"data\": data_fashion_mnist_cfg,\n",
    "        \"model\": model_mnist_cnn_cfg,\n",
    "        \"opt\": radam_no_lr_sch_opt_cfg,\n",
    "        \"train\": train_cfg\n",
    "    },\n",
    "    \"mlp_with_dropout_fashion_mnist_adam_with_warmup_lr_schedule\": {\n",
    "        \"data\": data_fashion_mnist_cfg,\n",
    "        \"model\": model_mnist_mlp_with_dropout_cfg,\n",
    "        \"opt\": adam_warmup_lr_sch_opt_cfg ,\n",
    "        \"train\": train_cfg\n",
    "    },\n",
    "}\n",
    "\n",
    "### hydra composition ###\n",
    "# clear hydra instance \n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "\n",
    "# register preset configs\n",
    "register_config(_merged_cfg_presets)\n",
    "\n",
    "# initialization\n",
    "hydra.initialize(config_path=None)\n",
    "\n",
    "using_config_key = \"mlp_with_dropout_fashion_mnist_adam_with_warmup_lr_schedule\"\n",
    "cfg = hydra.compose(using_config_key)\n",
    "\n",
    "# define & override log_cfg\n",
    "model_name = cfg.model.name\n",
    "run_dirname = \"dnn-tutorial-fashion-mnist-runs-tf\"\n",
    "run_name = f\"{datetime.now().isoformat(timespec='seconds')}-{using_config_key}-{model_name}\"\n",
    "log_dir = os.path.join(drive_project_root, \"runs\", run_dirname, run_name)\n",
    "\n",
    "log_cfg = {\n",
    "    \"run_name\": run_name,\n",
    "    \"callbacks\": {\n",
    "        \"TensorBoard\": {\n",
    "            \"log_dir\": log_dir,\n",
    "            \"update_freq\": 1,\n",
    "        },\n",
    "        \"EarlyStopping\": {\n",
    "            \"patience\": 3,\n",
    "            \"verbose\": True,\n",
    "        }\n",
    "    },\n",
    "    \"wandb\": {\n",
    "        \"project\": \"fastcampus_fashion_mnist_tutorials_tf\",\n",
    "        \"name\": run_name,\n",
    "        \"tags\": [\"fastcampus_fashion_mnist_tutorials_tf\"],\n",
    "        \"reinit\": True,\n",
    "        \"sync_tensorboard\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# unlock struct of config & set log config\n",
    "OmegaConf.set_struct(cfg, False)\n",
    "cfg.log = log_cfg\n",
    "\n",
    "# relock config\n",
    "OmegaConf.set_struct(cfg, True)\n",
    "print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "# save yaml\n",
    "# with open(os.path.join(log_dir, \"config.yaml\")) as f:\n",
    "# with open(\"config.yaml\", \"w\") as f:\n",
    "#     OmegaConf.save(cfg, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yARForQbUcbx"
   },
   "outputs": [],
   "source": [
    "def get_distribute_strategy(strategy_name: str, **kwargs):\n",
    "    return getattr(tf.distribute, strategy_name)(**kwargs)\n",
    "\n",
    "distribute_strategy = get_distribute_strategy(cfg.train.distribute_strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X89D1QCGJXTN"
   },
   "outputs": [],
   "source": [
    "with distribute_strategy.scope():\n",
    "    # 데이터 셋 정의 \n",
    "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    \n",
    "    # normalization\n",
    "    if cfg.model.data_normalize:\n",
    "        x_train = x_train / 255.0\n",
    "        x_test = x_test / 255.0\n",
    "\n",
    "    # train/val splits\n",
    "    assert sum(cfg.data.train_val_split) == 1.0\n",
    "    train_size = int(len(x_train) * cfg.data.train_val_split[0])\n",
    "    val_size = len(x_train) - train_size\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    if cfg.data.train_val_shuffle:\n",
    "        dataset = dataset.shuffle(\n",
    "            buffer_size=cfg.data.train_val_shuffle_buffer_size,\n",
    "        )\n",
    "    if cfg.data.test_shuffle:\n",
    "        test_dataset = test_dataset.shuffle(\n",
    "            buffer_size=cfg.data.test_shuffle_buffer_size,\n",
    "        )\n",
    "\n",
    "    train_dataset = dataset.take(train_size)\n",
    "    val_dataset = dataset.skip(train_size)\n",
    "    print(len(train_dataset), len(val_dataset), len(dataset), len(test_dataset))\n",
    "    \n",
    "    # dataloader 정의\n",
    "    train_batch_size = cfg.train.train_batch_size\n",
    "    val_batch_size = cfg.train.val_batch_size\n",
    "    test_batch_size = cfg.train.test_batch_size\n",
    "\n",
    "    train_dataloader = train_dataset.batch(train_batch_size, drop_remainder=True)\n",
    "    val_dataloader = val_dataset.batch(val_batch_size, drop_remainder=True)\n",
    "    test_dataloader = test_dataset.batch(test_batch_size, drop_remainder=True)\n",
    "\n",
    "sample_example = next(iter(train_dataloader))\n",
    "print(sample_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9KyfbS4RzQ7"
   },
   "outputs": [],
   "source": [
    "# 모델 정의 \n",
    "def get_model(cfg: DictConfig):\n",
    "    if cfg.model.name == \"CNN\":\n",
    "        model = CNN(cfg.model)\n",
    "    elif cfg.model.name == \"EfficientFinetune\":\n",
    "        model = EfficientFinetune(cfg.model)\n",
    "    elif cfg.model.name == \"MLP\":\n",
    "        model = MLP(cfg.model)\n",
    "    elif cfg.model.name == \"MLPWithDropout\":\n",
    "        model = MLPWithDropout(cfg.model)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "with distribute_strategy.scope():\n",
    "    model = get_model(cfg)\n",
    "    # define loss\n",
    "    loss_function = tf.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "    # define optimizer & scheduler\n",
    "    optimizer, scheduler = get_optimizer_element(\n",
    "        cfg.opt.optimizer, cfg.opt.lr_scheduler\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=loss_function,\n",
    "        optimizer=optimizer,\n",
    "        metrics=[tf.keras.metrics.Accuracy()],\n",
    "    )\n",
    "\n",
    "    model.build((1, 28, 28))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eo3jhwhHVOqu"
   },
   "outputs": [],
   "source": [
    "# get callbacks\n",
    "callbacks = get_callbacks(cfg.log)\n",
    "\n",
    "# wandb setup\n",
    "wandb.init(\n",
    "    config=flatten_dict(cfg),\n",
    "    **cfg.log.wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIXfPYDeWqTH"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/drive/MyDrive/\\#fastcampus/runs/\n",
    "\n",
    "model.fit(\n",
    "    train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    epochs=cfg.train.max_epochs,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5SnIKTKVOab"
   },
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bt7d0YMxXd5w"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3NIlcw5YBJc"
   },
   "outputs": [],
   "source": [
    "test_labels_list = []\n",
    "test_preds_list = []\n",
    "test_outputs_list = []\n",
    "\n",
    "for i, (test_images, test_labels) in enumerate(tqdm(test_dataloader, position=0, leave=True, desc=\"testing\")):\n",
    "    with mirrored_strategy.scope():\n",
    "        test_outputs = model(test_images)\n",
    "    test_preds = tf.argmax(test_outputs, 1)\n",
    "\n",
    "    final_outs = test_outputs.numpy()\n",
    "    test_outputs_list.extend(final_outs)\n",
    "    test_preds_list.extend(test_preds.numpy())\n",
    "    test_labels_list.extend(test_labels.numpy())\n",
    "\n",
    "test_preds_list = np.array(test_preds_list)\n",
    "test_labels_list = np.array(test_labels_list)\n",
    "\n",
    "test_accuracy = np.mean(test_preds_list == test_labels_list)\n",
    "print(f\"\\nacc: {test_accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7TXScWnZAPR"
   },
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh = {}\n",
    "n_class = 10\n",
    "\n",
    "for i in range(n_class):\n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(test_labels_list, np.array(test_outputs_list)[:, i], pos_label=i)\n",
    "\n",
    "# plot\n",
    "for i in range(n_class):\n",
    "    plt.plot(fpr[i], tpr[i], linestyle=\"--\", label=f\"Class {i} vs Rest\")\n",
    "plt.title(\"Multi-class ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "auc_score = roc_auc_score(test_labels_list, test_outputs_list, multi_class=\"ovo\", average=\"macro\")\n",
    "print(f\"auc_score: {auc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b-2TiKdZdWd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qau4Vi0zZgm3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOvNgo3u5dp3Pl5jbxpqDvE",
   "collapsed_sections": [],
   "name": "05-3. Fashion_Mnist Configuration Management_TF.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "14UhxSABuj-M683k4TilUDhQIo39HFK-t",
     "timestamp": 1634019312693
    },
    {
     "file_id": "1CBRS5ajK2WVXMEN97owUmh6yhmljiILI",
     "timestamp": 1634015371559
    },
    {
     "file_id": "1bRDRTJmGm80o_sWIacmyt-pUnHfqOKqE",
     "timestamp": 1634012243448
    },
    {
     "file_id": "1apCHsYMGF0a1zGJeJe5WhAGyUTbehmi7",
     "timestamp": 1633794411943
    },
    {
     "file_id": "1Yq_LYjN0_deqoBFaD_ZJFfoes_VYtBVN",
     "timestamp": 1633790134053
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
