{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part4_9_Transformer_based_Seq2Seq.ipynb","private_outputs":true,"provenance":[{"file_id":"16-sxYgu_FHRpWsTks0wngiC5ZRhfIcgU","timestamp":1654578309354}],"collapsed_sections":[],"authorship_tag":"ABX9TyN4yR5wdq3LoXMxieJh4Coe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["torch nn transformer\n","- https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html"],"metadata":{"id":"fQAqYq_NVToS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ME4t6L5OqxUU"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","import sys\n","from datetime import datetime\n","\n","drive_project_root = \"/content/drive/MyDrive/#fastcampus\"\n","sys.path.append(drive_project_root)"]},{"cell_type":"code","source":["!pip install -r \"/content/drive/MyDrive/#fastcampus/requirements.txt\""],"metadata":{"id":"GoBQrmra0b4k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","    print(\"Select the Runtime > 'Change runtime type' menu to enable a GPU accelerator, \")\n","    print('and then re-execute this cell.')\n","else:\n","    print(gpu_info)"],"metadata":{"id":"ApKyMlyprJ6y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["현재 torchmetrics 사용하면 안 되고 강의에 맞추려면 downgrading 필요"],"metadata":{"id":"WhBAxtxswJNy"}},{"cell_type":"code","source":["pip uninstall torchmetrics"],"metadata":{"id":"11-VQsFxukkA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install torchmetrics==0.5"],"metadata":{"id":"udB14lWhunPt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pip install pytorch_lightning"],"metadata":{"id":"tJttwE3BEBqP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for data loading\n","from typing import List, Dict, Union, Any, Optional, Iterable, Callable\n","from abc import abstractmethod, ABC\n","from datetime import datetime\n","from functools import partial\n","from collections import Counter, OrderedDict\n","import random\n","import math\n","import numpy as np\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import torch\n","from torch import nn, optim\n","import torch.nn.functional as F\n","\n","from torch.nn import Transformer\n","\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader\n","import pytorch_lightning as pl\n","from pprint import pprint  # pretty print\n","\n","# https://pytorch.org/text/stable/index.html\n","import torchtext\n","from torchtext import data\n","from torchtext import datasets\n","from torchtext.datasets import Multi30k\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.data.functional import to_map_style_dataset\n","from torchtext.vocab import Vocab, build_vocab_from_iterator, vocab\n","import spacy\n","\n","# For configuration\n","from omegaconf import DictConfig, OmegaConf\n","import hydra\n","from hydra.core.config_store import ConfigStore\n","\n","# for logger\n","from torch.utils.tensorboard import SummaryWriter\n","import wandb\n","os.environ['WANDB_START_METHOD'] = 'thread'"],"metadata":{"id":"L71IApmZreOS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(torchtext.__version__)"],"metadata":{"id":"WRk-0qON6t63"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from data_utils import dataset_split\n","from config_utils import flatten_dict, register_config, configure_optimizers_from_cfg, get_loggers, get_callbacks"],"metadata":{"id":"np4HB_ImtSjS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python -m spacy download en   # 영어\n","!python -m spacy download en_core_web_sm\n","!python -m spacy download de   # 독일어\n","!python -m spacy downlaod de_core_news_sm"],"metadata":{"id":"zgcdoVUKuyLu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["독일어를 영어로 번역할 예정"],"metadata":{"id":"BFB2wJcgu_Bn"}},{"cell_type":"code","source":["# !git clone --recursive https://github.com/multi30k/dataset.git multi30k-dataset"],"metadata":{"id":"QZJcwVNCfqg2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Multi30k를 미리 지정해 둔 data_root 경로에 download & 압축 해제\n","# train_data, valid_data, test_data = Multi30k(data_cfg.data_root)"],"metadata":{"id":"_SEMsSfYu_NN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["test_data 형태는 기본적으로 iterable 형태\n","- iterable 형태란 처음에 아래 코드 돌리면 이 결과가 나오는데 다시 한 번 돌리면 결과 안 나옴\n","```python\n","for i in test_data:\n","    print(i)\n","```\n","- pytorch에서는 이걸 map style로 바꾸면 우리가 원하는 대로 조절 가능\n","  - 계속 실행해도 똑같이 결과 나옴\n","  - 단, data가 엄청 큰 경우 특별하게 처리 안 해 놨으면 깨질 수도 있음 (ram이 안 좋은 경우)\n","  - 그래서 data 엄청 큰 경우에는 map을 사용하지 말거나 똑똑하게 처리해서 사용해야 함"],"metadata":{"id":"RGGvGHr9x8Lx"}},{"cell_type":"code","source":["test_data = to_map_style_dataset(test_data)"],"metadata":{"id":"xY_KU_NXu_Pa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. token_transform (token...)"],"metadata":{"id":"a3X2g9m60q3j"}},{"cell_type":"code","source":["def get_token_transform(data_cfg: DictConfig) -> dict:\n","    token_transform: dict = {}\n","    token_transform(data_cfg.src_lang) = get_tokenizer(data_cfg.tokenizer, language=data_cfg.src_lang)\n","    token_transform(data_cfg.tgt_lang) = get_tokenizer(data_cfg.tokenizer, language=data_cfg.tgt_lang)\n","    return toekn_transform\n","\n","token_transform = get_token_transform(data_cfg)"],"metadata":{"id":"wTKCeA1k0q_Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. vocab_transform"],"metadata":{"id":"jU3RXuq-0rJ2"}},{"cell_type":"code","source":["def yield_tokens(data_iter: Iterable, lang: str, lang2index: Dict[str, int]) -> List[str]:\n","    for data_sample in data_iter:\n","        # help function to yield list of tokens\n","        yield token_transform[lang](data_sample[lang2index[lang]])\n","\n","def get_vocab_transform(data_cfg: DictConfig) -> dict:\n","    vocab_transform: dict = {}\n","    for ln in [data_cfg.src_lang, data_cfg.tgt_lang]:\n","\n","        # build from train_data\n","        train_iter = Multi30k(\n","            split=\"train\", language_pair=(data_cfg.src_lang, data_cfg.trg_lang)\n","        )\n","\n","        # create torchtext's Vocab object\n","        vocab_transform[ln] = build_vocab_from_iterator(\n","            yield_tokens(\n","                train_iter,\n","                ln,\n","                {\n","                    data_cfg.src_lang: data_cfg.src_index,\n","                    data_cfg.tgt_lang: data_cfg.tgt_index\n","                }\n","            ),\n","            min_freq = data_cfg.vocab.min_freq,\n","            specials = list(data_cfg.vocab.special_symbol2index.keys()),\n","            special_first = True,   # 이 특수 문자들을 앞으로 보낼지, random 처리할 지, 뒤로 보낼지 -> 일반적으로 앞으로 보냄\n","        )\n","    \n","    # set UNKNOWN as the default index\n","    # --> toekn이 찾아지지 않는 경우 index를 unknown으로 return\n","    # = token이 not found가 되었을 때 error가 나지 않고 unknown으로 나타나도록 하기\n","    # ex) train data에 없었는데 test data에 있는 token\n","    # 만약 setting되지 않으면 runtime error 발생함\n","    for ln in (data_cfg.src_lang, data_cfg.tgt_lang):\n","        vocab_transform[ln].set_default_index(data_cfg.vocab.special_symbol2index['<unk>'])\n","\n","    return vocab_transform\n","\n","vocab_transform = get_vocab_transform(data_cfg)"],"metadata":{"id":"rhjBs7K50rUX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["작성 코드 테스트해 보기"],"metadata":{"id":"OoDuSFpSFCgO"}},{"cell_type":"code","source":["print(vocab_transform[\"de\"][\"<unk>\"])  # 0\n","print(vocab_transform[\"en\"][\"<unk>\"])  # 0\n","print(vocab_transform[\"de\"][\"<bos>\"])  # 2\n","print(vocab_transform[\"en\"][\"hello\"], vocab_transform[\"de\"][\"world\"])  # 5466, 107  -> return index number for each token"],"metadata":{"id":"vDIlWbHE03da"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. integrated transforms\n","- 이 과정에서 text_transform 필요\n","  - 각각에 대해 token transform -> vocab transform -> torch.tensor transform"],"metadata":{"id":"ksRTc2p6LffQ"}},{"cell_type":"code","source":["# helper function for callate_fn\n","\n","# *transforms : list 형태로 어떤 것이든 들어올 수 있음\n","def sequential_transforms(*transforms):\n","    def func(txt_input):\n","        for transform in transforms:\n","            txt_input = transform(txt_input)\n","        return txt_input\n","    return func\n","\n","# convert to torch.tensor with bos & eos\n","def tensor_transform(token_ids: List[int], bos_index: int, eos_index: int):\n","    return torch.cat(\n","        (torch.tensor([bos_index]), torch.tensor(token_ids), torch.tensor([eos_index]))\n","        )\n","\n","# src & tgt lang language text_transforms to convert rqaw strings --> tensor indices\n","def get_text_transform(data_cfg: DictConfig):\n","    text_transform = {}\n","    for ln in [data_cfg.src_lang, data_cfg.tgt_lang]:\n","        text_transform[ln] = sequential_transforms(\n","            token_transform[ln],\n","            vocab_transform[ln],\n","            partial(\n","                tensor_transform,\n","                bos_index = data_cfg.vocab.special_symbol2index[\"<bos>\"],\n","                eos_index = data_cfg.vocab.special_symbol2index[\"<eos>\"]\n","            )\n","        )\n","    return text_transform\n","\n","text_transform =get_text_transform(data_cfg)"],"metadata":{"id":"nxEfrci904n1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["잘 되었는지 test\n","- 2 : 맨 앞\n","- 3 : 맨 뒤\n","- 가운데 : 실제 내가 넣은 단어"],"metadata":{"id":"bTHIITYKKwdS"}},{"cell_type":"code","source":["print(text_transform['en']('hello'))\n","print(text_transform['en']('hello,'))\n","print(text_transform['en']('hello, how'))\n","print(text_transform['en']('hello, how are you?'))"],"metadata":{"id":"ZaqHAb5x04vU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. collate_fn -> batch를 어떻게 전처리 할까?"],"metadata":{"id":"YjOa2dX7LSSu"}},{"cell_type":"code","source":["def collate_fn(batch, data_cfg: DictConfig):\n","    src_batch, tgt_batch = [], []\n","\n","    for src_sample, tgt_sample in batch:\n","        # rstrip('\\n') : to remove '\\n'\n","        src_batch.append(text_transform[data_cfg.src_lang](src_sample.rstring(\"\\n\")))\n","        tgt_batch.append(text_transform[data_cfg.tgt_lang](tgt_sample.rstring(\"\\n\")))\n","    \n","    src_batch = pad_sequence(src_batch, padding_value = data_cfg.vocab.special_symbol2index[\"<pad>\"])  # use padding if token is small\n","    tgt_batch = pad_sequence(tgt_batch, padding_value = data_cfg.vocab.special_symbol2index[\"<pad>\"])  # use padding if token is small\n","    return src_batch, tgt_batch\n","\n","def get_collate_fn(cfg: DictConfig):\n","    return partial(collate_fn, data_cfg=cfg.data)"],"metadata":{"id":"Ht1-Ees4LSV4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. data loader"],"metadata":{"id":"BefRFyXZy1AV"}},{"cell_type":"code","source":["def get_multi30k_dataloader(split_mode: str, language_pair: tuple, batch_size: int, collate_fn: Callable):\n","    iter = Multi30k(split=split_mode, language_pair=language_pair)\n","    dataset = to_map_style_dataset(iter)  # map style로 바꿔야 error 없이 작업 가능 + 지금은 data 작기 때문에 map style 해주면 훨씬 빠름 (memory 거의 차지X)\n","    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\n","    return dataloader"],"metadata":{"id":"MHpo8kfNy1C8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["잘 되나 확인"],"metadata":{"id":"C-ba86K6bCKc"}},{"cell_type":"code","source":["# 3 : batch size\n","# test_dataloader = get_multi30k_dataloader('test', (data_cfg.src_lang, data_cfg.tgt_lang), 3, collate_fn=partial(collate_fn, data_cfg=data_cfg))"],"metadata":{"id":"y8RXVZexLSYr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for i in test_dataloader:\n","#     print(i)"],"metadata":{"id":"_IYYExnRLSbj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## seed 설정하기\n","\n","- pytorch lightning을 쓰는 경우 'pytorch lightning seed everyting' 쓰면 아래와 같이 하나하나 모두 SEED 지정할 필요 없음"],"metadata":{"id":"LSnxi_CTbbz2"}},{"cell_type":"code","source":["SEED = 1234\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"metadata":{"id":"W98L5tQiLSeH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _text_postprocessing(res: List[str]) -> str:\n","    if \"<eos>\" in res:\n","        res = res[:res.index('<eos>')]  # eos index를 찾아서 그 앞까지만 가져오겠다\n","    if \"<pad>\" in res:\n","        res = res[:res.index[\"<pad>\"]]\n","    res = \" \".join(res).replace(\"<bos>\", \"\")\n","    return res\n","\n","class BaseTranslateLightningModule(pl.LightningModule):\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__()\n","        self.cfg = cfg\n","        self.loss_function = torch.nn.CrossEntropyLoss(\n","            # 학습하지 않을 단어 지정\n","            ignore_index=cfg.data.vocab.special_symbol2index['<pad>']\n","        )\n","    \n","    def configure_optimizers(self):\n","        self._optimizers, self._schedulers = configure_optimizers_from_cfg(\n","            self.cfg, self\n","        )\n","        return self._optimizers, self._schedulers\n","    \n","    @abstractmethod\n","    def forward(self, src, tgt, teacher_forcing_ratio: float):\n","        raise NotImplementedError()\n","    \n","    # mode : train, validation, test 지정\n","    def _forward(self, src, tgt, mode: str, teacher_forcing_ratio: float=0.5):\n","        # teacher forcing\n","        #   seq2seq에서 많이 씀\n","        #   src -> tgt autoregressive 학습하면, 앞 부분은 빠르게 학습함\n","        #   but, 뒷 부분 학습은 언제? (앞 부분 학습하는 거 기다리기 너무 힘듦)\n","        #   랜덤으로 미래 정보도 조금 줘서 뒤에 있는 정보도 학습이 가능하게 하자!\n","        #   0.5 -> 0.5 확률로 teacher forcing 하겠다는 의미\n","\n","        assert mode in [\"train\", \"val\", \"test\"]\n","\n","        # get predictions\n","        # teacher forcing 용 input\n","        tgt_inputs = tgt[:-1, :]  # delete ends for teacher forcing inputs\n","        outputs = self(src, tgt_inputs, teacher_forcing_ratio=teacher_forcing_ratio)\n","        tgt_outputs = tgt[1:, :]  # delete start tokens\n","\n","        loss = self.loss_function(\n","            # -1 : 나머지를 한다\n","            # outputs.shape[-1] : hidden layer의 output\n","            outputs.reshape(-1, outputs.shape[-1]),  # [[batch * seq_size], other_output_shape]\n","            tgt_outputs.reshape(-1),\n","        )\n","\n","        logs_detail = {\n","            f\"{mode}_src\": src,\n","            f\"{mode}_tgt\": tgt,\n","            f\"{mode}_results\": outputs,\n","        }\n","\n","        if mode in [\"val\", \"test\"]:\n","            _, tgt_results = torch.max(outputs, dim=2)  # sequence * batch_size\n","\n","            src_texts = []  # input\n","            tgt_texts = []  # 정답\n","            res_texts = []  # 실제값\n","\n","            # convert [L * batch * others] --> [batch * L * others]\n","            for src_i in torch.transpose(src, 0, 1).detach().cpu().numpy().tolist():\n","                # lookup_tokens : token을 넣으면 text로 바꿔줌\n","                res = vocab_transform[self.cfg.data.src_lang].lookup_tokens(src_i)\n","                src_texts.append(_text_postprocessing(res))\n","\n","            for tgt_i in torch.transpose(tgt, 0, 1).detach().cpu().numpy().tolist():\n","                res = vocab_transform[self.cfg.data.tgt_lang].lookup_tokens(tgt_i)\n","                tgt_texts.append(_text_postprocessing(res))\n","            \n","            for tgt_res_i in torch.transpose(tgt_results, 0, 1).detach().cpu().numpy().tolist():\n","                res = vocab_transform[self.cfg.data.tgt_lang].lookup_tokens(tgt_res_i)\n","                res_texts.append(_text_postprocessing(res))           \n","\n","            text_result_summary = {\n","                f\"{mode}_src_text\": src_texts,\n","                f\"{mode}_tgt_text\": tgt_texts,\n","                f\"{mode}_results_text\": res_texts,\n","            }\n","            print(f\"{self.global_step} step: \\n src_text: {src_texts[0]}, \\n tgt_text: {tgt_texts[0]}, \\n result_text: {res_texts[0]}\")\n","            logs_detail.update(text_result_summary)\n","\n","        return {f\"{mode}_loss\": loss}, logs_detail\n","    \n","    def training_step(self, batch, batch_idx):\n","        src, tgt = batch[0], batch[1]\n","        logs, _ = self._forward(src, tgt, \"train\", self.cfg.model.teacher_forcing_ratio)\n","        self.log_dict(logs)\n","        logs[\"loss\"] = logs[\"train_loss\"]\n","        return logs\n","\n","    def validation_step(self, batch, batch_idx):\n","        src, tgt = batch[0], batch[1]\n","        logs, logs_detail = self._forward(src, tgt, \"val\", 0.0)\n","        self.log_dict(logs)\n","        logs[\"loss\"] = logs[\"val_loss\"]\n","        logs.update(logs_detail)\n","        return logs\n","\n","    def test_step(self, batch, batch_idx):\n","        src, tgt = batch[0], batch[1]\n","        logs, logs_detail = self._forward(src, tgt, \"test\", 0.0)\n","        self.log_dict(logs)\n","        logs[\"loss\"] = logs[\"test_loss\"]\n","        logs.update(logs_detail)\n","        return logs\n","\n","class TransformerTranslateLightningModule(BaseTranslateLightningModule):\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__(cfg)\n","\n","    @abstractmethod\n","    def forward(self, src, tgt):\n","        raise NotImplementedError()\n","    \n","    # mode : train, validation, test 지정\n","    def _forward(self, src, tgt, mode: str):\n","        # teacher forcing\n","        #   seq2seq에서 많이 씀\n","        #   src -> tgt autoregressive 학습하면, 앞 부분은 빠르게 학습함\n","        #   but, 뒷 부분 학습은 언제? (앞 부분 학습하는 거 기다리기 너무 힘듦)\n","        #   랜덤으로 미래 정보도 조금 줘서 뒤에 있는 정보도 학습이 가능하게 하자!\n","        #   0.5 -> 0.5 확률로 teacher forcing 하겠다는 의미\n","\n","        assert mode in [\"train\", \"val\", \"test\"]\n","\n","        # get predictions\n","        # teacher forcing 용 input\n","        tgt_inputs = tgt[:-1, :]  # delete ends\n","        outputs = self(src, tgt_inputs, teacher_forcing_ratio=teacher_forcing_ratio)\n","        tgt_outputs = tgt[1:, :]  # delete start tokens\n","\n","        loss = self.loss_function(\n","            # -1 : 나머지를 한다\n","            # outputs.shape[-1] : hidden layer의 output\n","            outputs.reshape(-1, outputs.shape[-1]),  # [[batch * seq_size], other_output_shape]\n","            tgt_outputs.reshape(-1),\n","        )\n","\n","        logs_detail = {\n","            f\"{mode}_src\": src,\n","            f\"{mode}_tgt\": tgt,\n","            f\"{mode}_results\": outputs,\n","        }\n","\n","        if mode in [\"val\", \"test\"]:\n","            _, tgt_results = torch.max(outputs, dim=2)  # sequence * batch_size\n","\n","            src_texts = []  # input\n","            tgt_texts = []  # 정답\n","            res_texts = []  # 실제값\n","\n","            # convert [L * batch * others] --> [batch * L * others]\n","            for src_i in torch.transpose(src, 0, 1).detach().cpu().numpy().tolist():\n","                # lookup_tokens : token을 넣으면 text로 바꿔줌\n","                res = vocab_transform[self.cfg.data.src_lang].lookup_tokens(src_i)\n","                src_texts.append(_text_postprocessing(res))\n","\n","            for tgt_i in torch.transpose(tgt, 0, 1).detach().cpu().numpy().tolist():\n","                res = vocab_transform[self.cfg.data.tgt_lang].lookup_tokens(tgt_i)\n","                tgt_texts.append(_text_postprocessing(res))\n","            \n","            for tgt_res_i in torch.transpose(tgt_results, 0, 1).detach().cpu().numpy().tolist():\n","                res = vocab_transform[self.cfg.data.tgt_lang].lookup_tokens(tgt_res_i)\n","                res_texts.append(_text_postprocessing(res))           \n","\n","            text_result_summary = {\n","                f\"{mode}_src_text\": src_texts,\n","                f\"{mode}_tgt_text\": tgt_texts,\n","                f\"{mode}_results_text\": res_texts,\n","            }\n","            print(f\"{self.global_step} step: \\n src_text: {src_texts[0]}, \\n tgt_text: {tgt_texts[0]}, \\n result_text: {res_texts[0]}\")\n","            logs_detail.update(text_result_summary)\n","\n","        return {f\"{mode}_loss\": loss}, logs_detail\n","    \n","    def training_step(self, batch, batch_idx):\n","        src, tgt = batch[0], batch[1]\n","        logs, _ = self._forward(src, tgt, \"train\")\n","        self.log_dict(logs)\n","        logs[\"loss\"] = logs[\"train_loss\"]\n","        return logs\n","\n","    def validation_step(self, batch, batch_idx):\n","        src, tgt = batch[0], batch[1]\n","        logs, logs_detail = self._forward(src, tgt, \"val\")\n","        self.log_dict(logs)\n","        logs[\"loss\"] = logs[\"val_loss\"]\n","        logs.update(logs_detail)\n","        return logs\n","\n","    def test_step(self, batch, batch_idx):\n","        src, tgt = batch[0], batch[1]\n","        logs, logs_detail = self._forward(src, tgt, \"test\")\n","        self.log_dict(logs)\n","        logs[\"loss\"] = logs[\"test_loss\"]\n","        logs.update(logs_detail)\n","        return logs"],"metadata":{"id":"RouL_nrdLShj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["utils for initialization"],"metadata":{"id":"mP_IoBW1A-er"}},{"cell_type":"code","source":["def init_weights(model: Union[nn.Module, pl.LightningModule]):\n","    for name, param in model.named_parameters():\n","        nn.init.uniform_(param.data, -0.08, 0.08)  # 이것도 원래 configuration 처리 해 주면 좋음"],"metadata":{"id":"KhDabAa0LSnz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## model definition"],"metadata":{"id":"8GA-siKgBIsK"}},{"cell_type":"markdown","source":["### 1. encoder"],"metadata":{"id":"cKBnV89VBJHq"}},{"cell_type":"code","source":["class LSTMEncoder(nn.Module):\n","    def __init__(\n","        self,\n","        input_dim: int,\n","        embed_dim: int,\n","        hidden_dim: int,\n","        n_layers: int,\n","        dropout: float,\n","    ):\n","        super().__init__()\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","        self.embedding = nn.Embedding(input_dim, embed_dim)\n","        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout) # embed_dim : input\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # initialization of weights\n","        self.apply(init_weights)\n","\n","    def forward(self, src):\n","        # src = [seq_len, batch_size]\n","        embedded = self.dropout(self.embedding(src))  # [seq_len, batch_size, emb_dim]\n","\n","        # LSTM은 output, hidden, cell state 모두 알 수 있음\n","        outputs, (hidden, cell) = self.rnn(embedded)\n","\n","        # outputs = [seq_len, batch_size, hidden_dim * n directional] # -> n directional : bidirectional인 경우 두 개\n","        # hidden, cell = [n layers * n directions, batch_size, hidden_dim] \n","\n","        # outputs will be used from top hidden layers\n","        return hidden, cell"],"metadata":{"id":"3eNKcmQ1BJOq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. decoder"],"metadata":{"id":"eOhnsRA4BJZT"}},{"cell_type":"code","source":["class LSTMDecoder(nn.Module):\n","    def __init__(\n","        self,\n","        output_dim: int,\n","        embed_dim: int,\n","        hidden_dim: int,\n","        n_layers: int,\n","        dropout: float\n","    ):\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","        self.output_dim = output_dim\n","        self.embedding = nn.Embedding(input_dim, embed_dim)\n","        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout) # embed_dim : input\n","        self.fc_out = nn.Linear(hidden_dim, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","    \n","    def forward(self, input, hidden, cell):\n","        # input: [batch size * ...]  --> decoder에서는 이게 start token\n","\n","        # outputs = [seq_len, batch_size, hidden_dim * n directional] # -> n directional : bidirectional인 경우 두 개\n","        # hidden, cell = [n layers * 1 direction, batch_size, hidden_dim] \n","        \n","        input = input.unsqueeze(0)  # <- [1, batch_size]\n","        embedded = self.dropout(self.embedding(input))\n","\n","        # embedding = [1, batch_size, embed_dim]\n","        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","\n","        # output = [1, batch_size, hidden_diim]\n","        # hidden, cell = [n layers * 1 direction, batch_size, hidden_dim]\n","\n","        prediction = self.fc_out(output.sequeeze(0))  # [batch_size, output_dim]\n","\n","        return prediction, hidden, cell"],"metadata":{"id":"nP37bt6iBJfK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Seq2Seq (cfg <-- encoder, decoder)"],"metadata":{"id":"hkgZjM4MBQ3M"}},{"cell_type":"code","source":["class LSTMSeq2Seq(BaseTranslateLightningModule):\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__(cfg)\n","\n","        self.encoder = LSTMEncoder(**cfg.model.enc)\n","        self.decoder = LSTMDecoder(**cfg.model.dec)\n","\n","        assert self.encoder.hidden_dim == self.decoder.hidden_dim\n","        assert self.encoder.n_layers == self.decoder.n_layers\n","\n","        # parameters init\n","        self.apply(init_weights)\n","    \n","    def forward(self, src, tgt, teacher_forcing_ratio: float = 0.5):\n","\n","        # src, tgt = [seq_len (can be different), batch_size]\n","        # for val, test teacher forcing should be 0.0\n","\n","        batch_size = tgt.shape[1]\n","        tgt_len = tgt.shape[0]\n","        tgt_vocab_size = self.decoder.output_dim\n","\n","        # tensor to store decoder outputs\n","        outputs = torch.zeros(tgt_len, batch_size, tgt_vocab_size).to(self.device)\n","\n","        hidden, cell = self.encoder(src)\n","\n","        # start token input (<sos> token)\n","        input = tgt[0, :]\n","\n","        for t in range(1, tgt_len):\n","\n","            # get 1 cell's output\n","            output, hidden, cell = self.decoder(input, hidden, cell)\n","\n","            # set to all outputs results\n","            outputs[t] = output\n","\n","            # decide whether going to use teacher forcing or not\n","            teacher_force = random.random() < teacher_forcing_ratio\n","\n","            top1 = output.argmax(1)  # 확률이 가장 높은 token을 뽑겠다\n","\n","            input = tgt[t] if teacher_force else top1   # 정답 넣기\n","        \n","        return outputs"],"metadata":{"id":"M9Bbm0PNBQ5w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Concat 기반 Additive Attention 기반의 모델 새로 정의\n","- 이번에는 위에 모델과 다르게 bidirection\n","- GRU encoder 사용 예정\n","- concat 기반 additive는 encoder, decoder rnn이 다를 수 있음\n","  - encoder, decoder 각각에 대한 hidden layer 생성 필요"],"metadata":{"id":"avei_HnCAZbY"}},{"cell_type":"code","source":["class BidirectionalGRUEncoder(nn.Module):\n","    def __init__(\n","        self,\n","        input_dim: int,\n","        enbed_dim : int,\n","        enc_hidden_dim: int,\n","        dec_hidden_dim: int,\n","        hidden_dim: int,\n","        n_layers: int,\n","        dropout: float\n","    ):\n","        super().__init__()\n","        self.input_dim = input_dim\n","        self.n_layers = n_layers\n","\n","        self.embedding = nn.Embedding(input_dim, embed_dim)\n","        self.rnn = nn.LSTM(embed_dim, enc_hidden_dim, n_layers, bidirectional=True, dropout=dropout) # embed_dim : input\n","        self.fc = nn.Linear(enc_hidden_dim * 2, dec_hidden_dim)   # fully connected : hidden layer 부분 좀 달라짐 -> * 2 = bidirectional\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # initialization of weights\n","        self.apply(init_weights)\n","\n","    def forward(self, src):\n","        \n","        embedded = self.dropout(self.embedding(src))\n","\n","        # GRU는 hidden, cell 둘 다 받지 않고 hidden state만 받는 게 LSTM과 차이점\n","        outputs , hidden = self.rnn(embedded)\n","\n","        # encoder RNNs fed through a linear layer to connect decoder\n","        # hidden 구성 : [forward_1, backward_1, forward_2, backward_2, ...]\n","        # 우리가 필요한 건 맨 마지막 레이어의 forward, backward 두 개 concat 하게 필요\n","        # hidden[-2, :, :] : forward\n","        # hidden[-1, :, :] : backward\n","        hidden = torch.tanh(self.fc(\n","            torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n","            ))\n","        \n","        return outputs, hidden\n","\n","class ConcatAttention(nn.Module):\n","    def __init__(self, enc_hidden_dim: int, dec_hidden_dim: int):\n","        super().__init__()\n","\n","        # attention score 값 계산\n","        self.attn = nn.Linear((enc_hidden_dim * 2) + dec_hidden_dim, dec_hidden_dim)  # * 2 : bidirectional\n","\n","        # weight를 곱해서 최종 attention 계산\n","        self.v = nn.Linear(dec_hidden_dim, 1, bias=False)   # 1 : 하나의 값 추출, attention에서는 bias 없어야 함\n","\n","    def forward(self, hidden, encoder_outputs):\n","\n","        # hidden = [batch_size, dec_hidden_dim] -> from decoder (key, query, value 중 query에 해당하는 값)\n","        # encoder_outputs = [src_len, batch_size, enc_hidden_dim * 2] --> key, query, value 중 key, value에 해당하는 값\n","\n","        batch_size = encoder_outputs.shape[1]\n","        src_len = encoder_outputs.shape[0]\n","\n","        # repeat decoder hidden state src_len times\n","        # unsqueeze를 통해 일단 shape 맞추기\n","        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","\n","        # hidden: [batch_size, src_len, dec_hidden_dim]\n","        # encoder_outputs = [batch_size, src_len, enc_hidden_dim * 2]\n","\n","        # concat attention이니까 concat 필요\n","        # 이 경우에는 sequence batch size, sequence length 유지하고 나머지를 concat 하니 dimension은 2\n","        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outpus), dim=2)))\n","\n","        # energy : [batch_size, src_len, dec_hidden_dim]\n","\n","        attention = self.v(energy).squeeze(2)\n","\n","        # attention : [batch_size, src_len]\n","\n","        return F.softmax(attention, dim=1)   # dim=1 : 0~1 사이 확률 값 return\n","    \n","class AttentionalRNNDecoder(nn.Module):\n","    def __init__(\n","        self,\n","        output_dim: int,\n","        embed_dim: int,\n","        enc_hidden_dim: int,\n","        dec_hidden_dim: int,\n","        n_layers: int,\n","        dropout: float,\n","        attention: nn.Module\n","    ):\n","        super().__init__()\n","\n","        self.output_dim = output_dim\n","        self.attention = attention\n","\n","        self.embedding = nn.Embedding(output_dim, embed_dim)\n","\n","        self.rnn = nn.GRU((enc_hidden_dim * 2) + embed_dim, dec_hidden_dim, n_layers, dropout=dropout)\n","\n","        self.fc_out = nn.Linear((enc_hidden_dim*2) + dec_hidden_dim + embed_dim, output_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, input, hidden, encoder_outputs):\n","\n","        # input : [batch_size]  -> start token\n","        # hidden : [batch_size, dec_hidden_dim]\n","        # encoder_outputs [src_len, batch_size, enc_hidden_dim * 2]\n","\n","        input = input.unsqueeze(0) # input = [1, batch_size]\n","\n","        embedded = self.dropout(self.embedding(input))  # 1, batch_size, embed_dim\n","\n","        a = self.attention(hidden, encoder_outputs)   # [batch_size, src_len]\n","        a = a.unsqueeze(1)   # [batch_size, 1, src_len]\n","\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # [batch_size, src_len, enc_hidden_dim * 2]\n","        weighted = torch.bmm(a, encoder_outputs)  # bmm : batch matrix-matrix product - [batch_size, 1, enc_hidden_dim * 2]\n","        weighted = weighted.permute(1, 0, 2)      # [1, batch_size, enc_hidden_dim * 2]\n","\n","        # cat : concat\n","        rnn_input = torch.cat((embedded, weighted), dim=2)  # [1, batch_size, (enc_hidden_dim * 2 + embed_dim)]\n","\n","        # hidden_unsqueeze(0) : [1, batch_size, dec_hidden_dim]\n","        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n","        # output = [seq_len, batch_size, dec_hidden_dim * n directions] -> [1, batch_size, dec_hidden_dim]\n","        # hidden = [n_layers * n_directions, batch_size, dec_hidden_dims] -> [1, batch_size, dec_hidden_dim]\n","        \n","        if not (output == hidden).all():\n","            raise AssertionError()\n","\n","        embedded = embedded.squeeze(0)\n","        output = output.squeeze(0)\n","        weighted = weighted.squeeze(0)\n","\n","        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))  # [batch_size, output_dim]\n","\n","        return prediction, hidden.squeeze(0)\n","    \n","class AttentionBasedSeq2Seq(BaseTranslateLightningModule):\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__(cfg)\n","\n","        self.encoder = BidirectionalGRUEncoder(**cfg.model.enc)\n","        self.attention = ConcatAttention(**cfg.model.attention)\n","        self.decoder = AttentionalRNNDecoder(\n","            attention=self.attention, **cfg.model.dec\n","        )\n","    \n","    def forward(self, src, tgt, teacher_forcing_ratio: float = 0.5):\n","\n","        # src, tgt = [seq_len (can be different), batch_size]\n","        # for val, test teacher forcing should be 0.0\n","\n","        batch_size = tgt.shape[1]\n","        tgt_len = tgt.shape[0]\n","        tgt_vocab_size = self.decoder.output_dim\n","\n","        # tensor to store decoder outputs\n","        outputs = torch.zeros(tgt_len, batch_size, tgt_vocab_size).to(self.device)\n","\n","        encoder_outputs, hidden = self.encoder(src)\n","\n","        # start token input (<sos> token)\n","        input = tgt[0, :]\n","\n","        for t in range(1, tgt_len):\n","\n","            # get 1 cell's output\n","            output, hidden = self.decoder(input, hidden, encoder_outputs)\n","\n","            # set to all outputs results\n","            outputs[t] = output\n","\n","            # decide whether going to use teacher forcing or not\n","            teacher_force = random.random() < teacher_forcing_ratio\n","\n","            top1 = output.argmax(1)  # 확률이 가장 높은 token을 뽑겠다\n","\n","            input = tgt[t] if teacher_force else top1   # 정답 넣기\n","        \n","        return outputs"],"metadata":{"id":"UCz1D4qjAZeZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. tokenembedding\n","# 2. positional encoding\n","# 3. nn.Transformer\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(\n","        self,\n","        emb_size: int,\n","        dropout: float,\n","        maxlen: int = 5000\n","    ):\n","        super().__init__()\n","        den = torch.exp(-torch.arange(0, embed_size, 2)*math.log(10000) / emb_size)\n","        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n","        pos_embedding = torch.zeros((maxlen, emb_size))   # 0으로 초기화\n","\n","        # sin: 2i\n","        # 0::2 : 0에서 시작해서 2개씩 뛰어넘기\n","        pos_embedding[:, 0::2] = torch.sin(pos * den)\n","\n","        # cos: 2i + 1\n","        pos_embedding[:, 1::2] = torch.cos(pos * den)\n","\n","        pos_embedding = pos_embedding.unsqueeze(-2) # 마지막 정보 없애기\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer(\"pos_embedding\", pos_embedding)  # \"pos_embedding\"을 call하면 pos_embedding 값을 가져옴\n","\n","    def forward(self, token_embedding: torch.Tensor):\n","        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n","\n","class TokenEmbedding(nn.Module):\n","    def __init__(\n","        self,\n","        vocab_size: int,\n","        embed_size: int\n","    ):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.embed_size = embed_size\n","    \n","    def forward(self, tokens: torch.Tensor):\n","        # * math.sqrt(self.embed_size) : scaling for embed size\n","        return self.embedding(token.long()) * math.sqrt(self.embed_size)\n","\n","class TransformerSeq2Seq(TransformerTranslateLightningModule):\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__(cfg)\n","        self.cfg = cfg\n","        num_encoder_layers = self.cfg.model.num_encoder_layers\n","        num_decoder_layers = self.cfg.model.num_decoder_layers\n","        embed_size = self.cfg.model.embed_size\n","        nhead = self.cfg.model.nhead\n","        src_vocab_size = self.cfg.model.src_vocab_size\n","        tgt_vocab_size = self.cfg.model.tgt_vocab_size\n","        dim_feedforward = self.cfg.model.dim_feedforward\n","        dropout = self.cfg.model.dropout\n","\n","        self.transformer = Transformer(\n","            d_model = embed_size,\n","            nhead = nhead,\n","            num_encoder_layers = num_encoder_layers,\n","            num_decoder_layers = num_decoder_layers,\n","            dim_feedforward = dim_feedforward,\n","            dropout = dropout\n","        )\n","\n","        self.generator = nn.Linear(embed_size, tgt_vocab_size)\n","        self.src_token_embed = TokenEmbedding(src_vocab_size, embed_size)\n","        self.tgt_token_embed = TokenEmbedding(tgt_vocab_size, embed_size)\n","        self.positional_encoding = PositionalEncoding(embed_size, dropout=dropout)\n","\n","    def generate_squre_subsequent_mask(self, sz: int):\n","        # device=self.device : 현재 device를 인식해서 GPU를 쓰는 중이면 GPU에 맞게 변환\n","        mask = (torch.triu(torch.ones((sz, sz), device=self.device)) == 1).transpose(0, 1)\n","        mask = mask.float().masked_fill(mask == 0, float(\"-inf\")).masked_fill(mask == 1, float(0.0))\n","        return mask\n","\n","    def create_mask(self, src, tgt):\n","        src_seq_len = src.shape[0]\n","        tgt_seq_len = tgt.shape[0]\n","\n","        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len)\n","        # device=self.device : 현재 device를 인식해서 GPU를 쓰는 중이면 GPU에 맞게 변환\n","        src_mask = torch.zeros((src_seq_len, src_seq_len), device=self.device).type(torch.bool)\n","\n","        src_padding_mask = (src == self.cfg.data.vocab.special_symbol2index[\"<pad>\"]).transpose(0, 1)\n","        tgt_padding_mask = (tgt == self.cfg.data.vocab.special_symbol2index[\"<pad>\"]).transpose(0, 1)\n","        return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n","    \n","    def forward(self, src: torch.Tensor, tgt: torch.Tensor):\n","\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.create_mask(src, tgt)\n","        memory_key_padding_mask = src_padding_mask\n","\n","        src_emb = self.positional_encoding(self.src_token_emb(src))\n","        tgt_emb = self.positional_encoding(self.tgt_token_emb(tgt))\n","\n","        outs = self.transformer(\n","            src_emb, tgt_emb, src_mask, tgt_mask, None, src_padding_mask, tgt_padding_mask, memory_key_padding_mask\n","        )\n","\n","        return self.generator(outs)\n","\n","    def encode(self, src: torch.Tensor, src_mask: torch.Tensor):\n","        return self.transformer.encoder(self.positional_encoding(self.src_token_emb(src)), src_mask)\n","    \n","    def decode(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: torch.Tensor):\n","        return self.transformer.decoder(self.positional_encoding(self.tgt_token_emb(tgt)), memory, tgt_mask)"],"metadata":{"id":"egcvZYtRsThQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_spacy_de_en_cfg = {\n","    \"name\": \"spacy_de_en\",\n","    \"data_root\": os.path.join(os.getcwd(), \"data\"),\n","    \"tokenizer\": \"spacy\",\n","    \"src_lang\": \"de\",  # source\n","    \"tgt_lang\": \"en\",  # target\n","    \"src_index\": 0,\n","    \"tgt_index\": 1,\n","    \"vocab\": {\n","        # special token을 index로 처리\n","        \"special_symbol2index\": {\n","            \"<unk>\": 0,   # unknown token\n","            \"<pad>\": 1,   # sequence 길이가 안 맞는 경우 뒷 부분 padding 처리\n","            \"<bos>\": 2,   # 문장의 시작\n","            \"<eos>\": 3,   # 문장의 끝\n","        },\n","        \"special_first\": True,\n","        \"min_freq\": 2\n","    }\n","}\n","\n","# 잘 만들어졌는지 중간 확인\n","data_cfg = OmegaConf.create(data_spacy_de_en_cfg)\n","# 예쁘게 프린트하는 방법1) to_yaml 사용\n","# print(OmegaConf.to_yaml(data_cfg))  # 문제 없이 잘 프린트 되면 된 것\n","# 방법2) pprint 사용 (pretty pring) - 단, pprint는 dictionary 형태여야 함\n","pprint(dict(data_cfg))\n","\n","# get_dataset\n","train_data, valid_data, test_data = Multi30k(data_cfg.data_root)\n","\n","token_transform = get_token_transform(data_cfg)\n","vocab_transform = get_vocab_transform(data_cfg)"],"metadata":{"id":"lEUp3HCgBQ7q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### model configs"],"metadata":{"id":"rZRfQ3BXvX4T"}},{"cell_type":"code","source":["model_translate_lstm_seq2seq_cfg = {\n","    \"name\": \"LSTMSeq2Seq\",\n","    # \"out_dim\": len(vocab_transform(data_cfg))\n","    \"enc\" : {\n","        \"input_dim\": len(vocab_transform(data_cfg.src_lang)),\n","        \"embed_dim\": 256,\n","        \"hidden_dim\": 256,\n","        \"n_layers\": 2,\n","        \"dropout\": 0.5,\n","    },\n","    \"dec\": {\n","        \"output_dim\": len(vocab_transform(data_cfg.tgt_lang)),\n","        \"embed_dim\": 256,\n","        \"hidden_dim\": 256,\n","        \"n_layers\": 2,\n","        \"dropout\": 0.5,\n","    },\n","    \"teacher_forcing_ratio\": 0.5\n","}\n","\n","model_translate_attention_based_seq2seq_cfg = {\n","    \"name\": \"AttentionBasedSeq2Seq\",\n","    # \"out_dim\": len(vocab_transform(data_cfg))\n","    \"enc\" : {\n","        \"input_dim\": len(vocab_transform(data_cfg.src_lang)),\n","        \"embed_dim\": 256,\n","        \"enc_hidden_dim\": 512,\n","        \"dec_hidden_dim\": 512,\n","        \"n_layers\": 1,\n","        \"dropout\": 0.5,\n","    },\n","    \"dec\": {\n","        \"output_dim\": len(vocab_transform(data_cfg.tgt_lang)),\n","        \"embed_dim\": 256,\n","        \"enc_hidden_dim\": 512,\n","        \"dec_hidden_dim\": 512,\n","        \"n_layers\": 1,\n","        \"dropout\": 0.5,\n","    },\n","    \"attention\": {\n","        \"enc_hidden_dim\": 512,\n","        \"dec_hidden_dim\": 512,\n","    },\n","    \"teacher_forcing_ratio\": 0.5\n","}\n","\n","model_translate_transformer_seq2seq_cfg = {\n","    \"name\": \"TransformerSeq2Seq\",\n","    \"num_encoder_layers\": 3,\n","    \"num_decoder_layers\": 3,\n","    \"embed_size\": 512,\n","    \"nhead\": 8,\n","    \"src_vocab_size\": len(vocab_transform(data_cfg.src_lang)),\n","    \"tgt_vocab_size\": len(vocab_transform(data_cfg.tgt_lang)),\n","    \"dim_feedforward\": 512,  # 너무 크면 모델 느려짐\n","    \"dropout\": 0.5,\n","}"],"metadata":{"id":"VBNXEgYXvX7W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### opt config"],"metadata":{"id":"Oed7h82FvX-E"}},{"cell_type":"code","source":["opt_cfg = {\n","    \"optimizers\": [\n","                   {\n","                    # config_utils.py 파일 참고\n","                    \"name\": \"RAdam\",\n","                    \"kwargs\": {\n","                        \"lr\": 1e-3,\n","                    }\n","                   }\n","    ],\n","    \"lr_schedulers\": [\n","                      {\n","                          \"name\": None,\n","                       \"kwargs\": {\n","                           \"warmup_end_steps\": 1000\n","                           }\n","                       }\n","    ]\n","}\n","\n","_merged_cfg_presets = {\n","    \"LSTM_seq2seq_de_en_translate\": {\n","        \"opt\": opt_cfg,\n","        \"data\": data_spacy_de_en_cfg,\n","        \"model\": model_translate_lstm_seq2seq_cfg,\n","    },\n","    \"attention_based_seq2seq_de_en_translate\": {\n","        \"opt\": opt_cfg,\n","        \"data\": data_spacy_de_en_cfg,\n","        \"model\": model_translate_attention_based_seq2seq_cfg,\n","    },  \n","    \"transformer_seq2seq_de_en_translate\": {\n","        \"opt\": opt_cfg,\n","        \"data\": data_spacy_de_en_cfg,\n","        \"model\": model_translate_transformer_seq2seq_cfg,\n","    },     \n","}\n","\n","# clear config hydra instance first\n","hydra.core.global_hydra.GlobalHydra.instance().clear()\n","\n","# register preset configs\n","register_config(_merged_cfg_presets)\n","\n","# initialization & compose configs\n","hydra.initialize(config_path=None)\n","cfg = hydra.compose(\"transformer_seq2seq_de_en_translate\")\n","\n","# override some cfg\n","run_name = f\"{datetime.now().isoformat(timespec='seconds')}-{cfg.model.name}-{cfg.data.name}\"\n","\n","project_root_dir = os.path.join(drive_project_root, \"runs\", \"de_en_translate_tutorials\")"],"metadata":{"id":"WagHQwf0vYA1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_dir = os.path.join(project_root_dir, run_name)\n","run_root_dir = os.path.join(project_root_dir, run_name)\n","\n","# train configs\n","train_cfg = {\n","    \"train_batch_size\": 128,\n","    \"val_batch_size\": 32,\n","    \"test_batch_size\": 32,\n","    \"train_val_split\": [0.9, 0.1],\n","    \"run_root_dir\": run_root_dir,\n","    \"trainer_kwargs\": {\n","        \"accelerator\": \"dp\", # 하나의 gpu로 할 때는 dp로 하지만 multiple gpu인 경우 ddp 등 설정 가능\n","        \"gpus\": \"0\",         # \"gpus\": \"0\",  # 0번 gpu 사용하기\n","        \"max_epochs\": 50,    \n","        # 1.0 : train epoch가 끝날 때 validation check을 하겠다\n","        # 0.5 : train epoch가 절반 돌았을 때 validation check 하겠다\n","        # integer인 경우 : 몇 step마다 돌 지 설정하는 것\n","        \"val_check_interval\": 1.0,\n","        \"log_every_n_steps\": 100,   # 100번 step마다 한다\n","        \"flush_logs_every_n_steps\": 100,\n","    },    \n","}\n","\n","\n","# logger configs\n","log_cfg = {\n","    \"loggers\": {\n","        \"WandbLogger\": { \n","            \"project\": \"fastcampus_de_en_translate_tutorials\",\n","            \"name\": run_name,\n","            \"tags\": [\"fastcampus_de_en_translate_tutorials\"],\n","            \"save_dir\": run_root_dir,\n","        },\n","        \"TensorBoardLogger\": {\n","            \"save_dir\": project_root_dir,\n","            \"name\": run_name,\n","            },\n","    },\n","    \"callbacks\": {\n","        \"ModelCheckpoint\": {\n","            \"save_top_k\": 3,\n","            \"monitor\": \"val_loss\",\n","            \"mode\": \"min\",\n","            \"verbose\": True,\n","            \"dirpath\": os.path.join(run_root_dir, \"weights\"),\n","            \"filename\": \"{epoch}-{val_loss:.3f}\"\n","            },\n","        \"EarlyStopping\": {\n","            \"monitor\": \"val_loss\",\n","            \"mode\": \"min\",\n","            \"patience\": 3,\n","            \"verbose\": True,\n","            }\n","    }\n","}\n","\n","OmegaConf.set_struct(cfg, False)\n","cfg.train = train_cfg\n","cfg.log = log_cfg\n","\n","# lock config\n","OmegaConf.set_struct(cfg, True)\n","print(OmegaConf.to_yaml(cfg))"],"metadata":{"id":"TewwKaRfwpJd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataloader def\n","\n","train_dataloader = get_multi30k_dataloader(\n","    \"train\",\n","    (data_cfg.src_lang, data_cfg.tgt_lang),\n","    cfg.train.train_batch_size,\n","    collate_fn=get_collate_fn(cfg)\n",")\n","\n","val_dataloader = get_multi30k_dataloader(\n","    \"valid\",\n","    (data_cfg.src_lang, data_cfg.tgt_lang),\n","    cfg.train.val_batch_size,\n","    collate_fn=get_collate_fn(cfg)\n",")\n","\n","test_dataloader = get_multi30k_dataloader(\n","    \"test\",\n","    (data_cfg.src_lang, data_cfg.tgt_lang),\n","    cfg.train.test_batch_size,\n","    collate_fn=get_collate_fn(cfg)\n",")"],"metadata":{"id":"kUXKl7ITwpeN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### pl translater def & get model"],"metadata":{"id":"tO-Ip5bCwpj9"}},{"cell_type":"code","source":["def get_pl_model(cfg: DictConfig, checkpoint_path: Optional[str] = None):\n","\n","    if cfg.model.name == \"LSTMSeq2Seq\":\n","        model = LSTMSeq2Seq(cfg)\n","    elif cfg.model.name == \"AttentionBasedSeq2Seq\":\n","        model = AttentionBasedSeq2Seq(cfg)\n","    elif cfg.model.name == \"TransformerSeq2Seq\":\n","        model = TransformerSeq2Seq(cfg)\n","    else:\n","        raise NotImplementedError(\"Not Implemented model\")\n","    \n","    if checkpoint_path is not None:\n","        model = model.load_from_checkpoint(cfg, checkpoint_path = checkpoint_path)\n","    return model\n","\n","model = None\n","model = get_pl_model(cfg)\n","print(model)"],"metadata":{"id":"UgY8Hqspwpof"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### pytorch lightning trainer def"],"metadata":{"id":"EUErdpnJwps2"}},{"cell_type":"code","source":["logger = get_loggers(cfg)\n","callbacks = get_callbacks(cfg)\n","\n","trainer = pl.Trainer(\n","    callbacks = callbacks,\n","    logger = logger,\n","    default_root_dir = cfg.train.run_root_dir,\n","    num_sanity_val_steps = 3,\n","    **cfg.train.trainer_kwargs\n",")"],"metadata":{"id":"TtBuyEcmwpxY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.fit(model, train_dataloader, val_dataloader)"],"metadata":{"id":"IqSnTXDIwp1t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"gTCy2UTbwp9V"},"execution_count":null,"outputs":[]}]}
