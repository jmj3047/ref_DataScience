{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part4_5_EfficientNet_Finetune.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"mount_file_id":"1Yp0yH2RjfScp9VM5ATexx5ttY00Ns4cs","authorship_tag":"ABX9TyPNVROIwLqolhRUwGGDl2C6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Fashion Mnist DNN Tutorial [CNN & Multi-layer Perceptron (MLP)]\n","\n","Efficient Network\n","- https://github.com/lukemelas/EfficientNet-PyTorch\n","- image 시리즈 중 auto ML을 통해 효율적으로 구현한 것\n","\n","외부 파일 가져오기 & requirements 설치"],"metadata":{"id":"M-WPUCURrOHF"}},{"cell_type":"code","source":["!pwd\n","\n","# mount\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import sys\n","\n","drive_project_root = '/content/drive/MyDrive/#fastcampus'\n","sys.path.append(drive_project_root)\n","\n","!ls"],"metadata":{"id":"nlOJHTqlrXqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["!pip install -r '/content/drive/MyDrive/#fastcampus/requirements.txt'"],"metadata":{"id":"4zXqMRJx_4sI"}},{"cell_type":"code","source":["pip install torch-optimizer"],"metadata":{"id":"htTM48WtVYBZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["wandb 오류 있을 때 : `wandb.flush`"],"metadata":{"id":"mVflxznMvrp3"}},{"cell_type":"code","source":["pip install wandb"],"metadata":{"id":"PLYu27ZkVasl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install omegaconf"],"metadata":{"id":"p_8AzO0Zkrbf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install efficientnet_pytorch"],"metadata":{"id":"YdNWgLlmYbHX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["https://pytorch.org/vision/stable/transforms.html\n","\n","★ OmegaConf\n","- https://omegaconf.readthedocs.io/en/2.1_branch/\n","- hyperparameter configuration을 관리하기 위한 open source library\n","- DictConfig : Dictionary 형태의 configuration\n","- Hydra도 omegaconf를 기반으로 만들어짐\n","  - Hydra는 무겁기 때문에 omegaconf를 먼저 거치고, hydra 사용"],"metadata":{"id":"eX4npp3cxT4p"}},{"cell_type":"markdown","source":["gpu 확인"],"metadata":{"id":"qWQbtu32WqXA"}},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","print(gpu_info)"],"metadata":{"id":"QBp8qPTCWuFN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datetime import datetime\n","\n","import numpy as np\n","from tqdm import tqdm   # 진행율\n","import matplotlib.pyplot as plt\n","\n","from omegaconf import OmegaConf, DictConfig\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F     # relu 등 함수 모음\n","from torch import optim\n","from torch_optimizer import RAdam, AdamP\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from torchvision.datasets import FashionMNIST\n","from torchvision import transforms    # feature engineering 전처리 작업 효율적으로 할 수 있게 도와줌\n","from torch.utils.data import random_split  # dataset split\n","\n","import wandb\n","\n","from efficientnet_pytorch import EfficientNet"],"metadata":{"id":"WC1nxqJKs72F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["왼쪽 파일 부분 보면 'data' 라는 폴더가 새로 생겼고 FahionMNIST data download 된 것 볼 수 있음</br>\n","'raw' 파일 안에 압축 파일들 생성됨<br>\n","https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html\n","- 링크 들어가서 보면 getitem으로 정의되어 있는 데이터라는 것 알 수 있음\n","- 이런 dataset은 iterator가 아니기 때문에 list 같이 index를 통해 특정 value 바로 접근 가능\n","- iterator dataset은 next 함수 이용해서 접근 가능"],"metadata":{"id":"GycvOEvm37Un"}},{"cell_type":"code","source":["data_root = os.path.join(os.getcwd(), 'data')\n","\n","# 전처리 부분 (preprocessing) & 데이터 셀 출력\n","# transform = transforms.Compose(\n","#     [\n","#      transforms.ToTensor(),\n","#      transforms.Normalize([0.5], [0.5]), # mean, std\n","#     ]\n","# )\n","\n","# This 'transform' is for EfficientNet\n","transform = transforms.Compose(\n","    [\n","     transforms.Resize(224),\n","     transforms.ToTensor(),\n","     # 원래 input 이미지 channel이 지금 1개(회색)인데 3개(color)로 확대 필요\n","     transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n","     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # efficientNet에 맞는 normalize 방식 (검색)\n","    ]\n",")\n","\n","fashion_mnist_dataset = FashionMNIST(data_root, download=True, train=True, transform=transform)"],"metadata":{"id":"TYBdBoCiuWC2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# list처럼 특정 이미지 바로 접근하기\n","# normalize해서 다운받았기 때문에 어느 정도 normalize된 데이터\n","fashion_mnist_dataset[0][0]\n","\n","# 첫 번째 데이터의 label\n","fashion_mnist_dataset[0][1]\n","\n","# dataset split\n","dset = random_split(fashion_mnist_dataset, [int(len(fashion_mnist_dataset)*0.7), int(len(fashion_mnist_dataset)*0.3)])\n","\n","dset[0]"],"metadata":{"id":"1ptvG-uSw9rI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["data_utils : 강사가 미리 만들어놓은 코드"],"metadata":{"id":"JBbfROJWyvbG"}},{"cell_type":"code","source":["from data_utils import dataset_split"],"metadata":{"id":"2PDva3Bvw9_R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datasets = dataset_split(fashion_mnist_dataset, split=[0.9, 0.1])\n","print(datasets)"],"metadata":{"id":"6mJ5o4cVyOuE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["dataloader 정의"],"metadata":{"id":"9WOvaXA62p3x"}},{"cell_type":"code","source":["train_dataset = datasets[\"train\"]\n","val_dataset = datasets[\"val\"]\n","\n","# batch 단위로 데이터를 묶을 예정\n","train_batch_size = 100\n","val_batch_size = 10\n","\n","# num_workers : 병렬 processing\n","train_dataloader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=1\n",")\n","\n","val_dataloader = torch.utils.data.DataLoader(\n","    val_dataset, batch_size=val_batch_size, shuffle=False, num_workers=1\n",")"],"metadata":{"id":"XLr9B81AyYQz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for sample_batch in train_dataloader:\n","    print(sample_batch)\n","    # 그림과 label 각각에 대한 shape 확인\n","    print(sample_batch[0].shape, sample_batch[1].shape)\n","    break"],"metadata":{"id":"NuxnwfgXzDjY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 모델 정의\n","- MLP (Multi-layer Perceptron)\n","- MLPWithDropout\n","- CNN\n","  - https://pytorch.org/docs/stable/nn.html#convolution-layers\n","    - conv2d : https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d\n","  - https://pytorch.org/docs/stable/nn.html#pooling-layers\n","    - maxpool2d : https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d"],"metadata":{"id":"ThJn_qDv2tST"}},{"cell_type":"markdown","source":["pytorch model은 보통 class로 정의\n","- nn과 같은 module을 꼭 import 해야 함 ★ : GPU, CPU 간 꼬이는 것 방지"],"metadata":{"id":"m3jeaflV2tT_"}},{"cell_type":"code","source":["# v1 : basic\n","class MLP(nn.Module):\n","    # input, hidden layer1/2, output의 차원을 먼저 알아야 함\n","    def __init__(self, in_dim: int, h1_dim: int, h2_dim: int, out_dim: int):\n","        # nn.Module 가져오기\n","        super().__init__()\n","        # MLP layer를 각각 정의할 때 제일 처음 linear layer를 지나야 함\n","        self.linear1 = nn.Linear(in_dim, h1_dim)\n","        self.linear2 = nn.Linear(h1_dim, h2_dim)\n","        self.linear3 = nn.Linear(h2_dim, out_dim)\n","        self.relu = F.relu\n","    \n","    def forward(self, input):\n","        # 위에서 input shape : torch.Size([100, 1, 28, 28]) → 1×28×28\n","        x = torch.flatten(input, start_dim=1)\n","        x = self.relu(self.linear1(x))\n","        x = self.relu(self.linear2(x))\n","        out = self.linear3(x)\n","        # output에 sigmoid(binary)나 softmax(multi) 씌워서 출력해도 되고 출력 후 나중에 작업해도 됨\n","        # out = F.softmax(out)\n","        return out\n","\n","# v2 : regularizers (dropout, early stoping)\n","class MLPWithDropout(MLP):\n","    def __init__(self, in_dim: int, h1_dim: int, h2_dim: int, out_dim: int, dropout_prob: float):\n","        super().__init__(in_dim, h1_dim, h2_dim, out_dim)\n","        # Dropout에 2d, 3d 등도 있는데 linear에 적용할 땐 1d 사용\n","        self.dropout1 = nn.Dropout(dropout_prob)\n","        self.dropout2 = nn.Dropout(dropout_prob)\n","\n","    def forward(self, input):\n","        x = torch.flatten(input, start_dim=1)\n","        x = self.relu(self.linear1(x))\n","        x = self.dropout1(x)\n","        x = self.relu(self.linear2(x))\n","        x = self.dropout2(x)\n","        out = self.linear3(x)\n","        return out"],"metadata":{"id":"tafPABGv2tW6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["CNN"],"metadata":{"id":"ViGxVRn6YsYV"}},{"cell_type":"code","source":["# cnn hyperparameter configuration\n","_cnn_cfg_dict: dict = {\n","    # layer 하나에 convolution, maxpooling 모두 만들 예정\n","    'layer_1': {\n","        # conv2d 사이트 보면 in_channels, out_channels, kernel_size는 반드시 정의해야 됨\n","        'conv2d_in_channels': 1, # 흑백 이미지니까 input channel : 1\n","        'conv2d_out_channels': 32,\n","        'conv2d_kernel_size': 3,\n","        'conv2d_padding': 1,\n","        # maxpool2d 사이트 보면 kernel_size 반드시 정의해야 됨\n","        'maxpool2d_kernel_size': 2,\n","        'maxpool2d_stride': 2,\n","    },\n","    'layer_2': {\n","        'conv2d_in_channels': 32, # layer1의 out channel이 32였으니까\n","        'conv2d_out_channels': 64,\n","        'conv2d_kernel_size': 3,\n","        'conv2d_padding': 0,\n","        'maxpool2d_kernel_size': 2,\n","        'maxpool2d_stride': 1,\n","\n","    },\n","    # fully connected layer\n","    'fc_1': {\n","        # input features -> 일단 임의의 값 '7011' 넣기 (에러 메시지 보고 고치기)\n","        # 나중에 model build 해 보면 multiply 안 된다는 에러 발생 : 7744로 변경\n","        'in_features': 7744,\n","        'out_features': 512\n","    },\n","    'fc_2': {\n","        'in_features': 512,   # fc_1의 out이 512였으니까\n","        'out_features': 128\n","    },\n","    'fc_3': {\n","        'in_features': 128,   # fc_2의 out이 128였으니까\n","        'out_features': 10    # 최종 class 수\n","    },\n","    'dropout_prob': 0.25,\n","}\n","\n","_cnn_cfg = OmegaConf.create(_cnn_cfg_dict)\n","# print(_cnn_cfg)                                # {'layer_1': {}, 'layer_2': {}, 'fc_1': {}, 'fc_2': {}, 'fc_3': {}}\n","# print(_cnn_cfg.layer_1, _cnn_cfg['layer_1'])   # {} {}\n","print(OmegaConf.to_yaml(_cnn_cfg))               # to_yaml : 위 두 개 코드보다 좀 더 보기 좋게 프린트\n","\n","# 결과 저장\n","# with open('cnn_test.ymal', 'w') as f:\n","#     OmegaConf.save(_cnn_cfg, f)\n","\n","# 불러오기\n","# OmegaConf.load\n","\n","class CNN(nn.Module):\n","    def __init__(self, cfg: DictConfig = _cnn_cfg):\n","        super().__init__()\n","        # convolutional layer와 maxpooling layer를 동시에 쓰려면 nn.Sequential로 같이 묶어줘야 함\n","        self.layer1 = nn.Sequential(\n","            # convolutional layer -> batch normalization -> relu -> maxpooling layer\n","            nn.Conv2d(\n","                in_channels=cfg.layer_1.conv2d_in_channels,\n","                out_channels=cfg.layer_1.conv2d_out_channels,\n","                kernel_size=cfg.layer_1.conv2d_kernel_size,\n","                padding=cfg.layer_1.conv2d_padding\n","            ),\n","            nn.BatchNorm2d(cfg.layer_1.conv2d_out_channels),  # num_features\n","            nn.ReLU(),\n","            nn.MaxPool2d(\n","                kernel_size=cfg.layer_1.maxpool2d_kernel_size,\n","                stride=cfg.layer_1.maxpool2d_stride\n","            )\n","        )\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(\n","                in_channels=cfg.layer_2.conv2d_in_channels,\n","                out_channels=cfg.layer_2.conv2d_out_channels,\n","                kernel_size=cfg.layer_2.conv2d_kernel_size,\n","                padding=cfg.layer_2.conv2d_padding\n","            ),\n","            nn.BatchNorm2d(cfg.layer_2.conv2d_out_channels),  # num_features\n","            nn.ReLU(),\n","            nn.MaxPool2d(\n","                kernel_size=cfg.layer_2.maxpool2d_kernel_size,\n","                stride=cfg.layer_2.maxpool2d_stride\n","            )\n","        )\n","\n","        self.fc1 = nn.Linear(\n","            in_features=cfg.fc_1.in_features,\n","            out_features=cfg.fc_1.out_features,\n","        )\n","        self.fc2 = nn.Linear(\n","            in_features=cfg.fc_2.in_features,\n","            out_features=cfg.fc_2.out_features,\n","        )\n","        self.fc3 = nn.Linear(\n","            in_features=cfg.fc_3.in_features,\n","            out_features=cfg.fc_3.out_features,\n","        )\n","\n","        # conv, maxpool 모두 2d를 계속 사용했으니 dropout도 2d로 함\n","        self.dropout = nn.Dropout2d(cfg.dropout_prob)\n","    \n","    # output은 이미지 형태인데 fully connected는 2d 이미지가 아닌 걸로 가정하기 때문에 중간에 한 번 flatten 해줘야 함\n","    def forward(self, x):\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        # flatten\n","        out = out.view(out.size(0), -1)  # size(0) : batch size, -1 : 앞에 있는 size(batch)만 남기고 나머지 flatten 해라\n","        out = self.fc1(out)\n","        out = self.dropout(out)\n","        out = self.fc2(out)\n","        out = self.fc3(out)\n","        return out\n","\n","CNN()"],"metadata":{"id":"5IriKAkkYnwg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Efficient Net\n","- 현재 우리가 GPU 1개만 쓰고 있으니\n","  - 깊은 모델은 어렵고 efficientnet-b0, b1 정도 가능\n","  - pretrain 안 하고 이미 pretrained 된 모델로 사용"],"metadata":{"id":"7a7po7yeYvlc"}},{"cell_type":"code","source":["_efficient_finetune_cfg_dict: dict = {\n","    \"efficient_net_model_name\": \"efficientnet-b1\",\n","    \"num_classes\": 10\n","}\n","\n","_efficient_finetune_cfg_dict = OmegaConf.create(_efficient_finetune_cfg_dict)\n","print(OmegaConf.to_yaml(_efficient_finetune_cfg_dict))\n","\n","class EfficientNetFinetune(nn.Module):\n","    def __init__(self, cfg: DictConfig = _efficient_finetune_cfg_dict):\n","        super().__init__()\n","        self.efficientnet = EfficientNet.from_pretrained(\n","            cfg.efficient_net_model_name,\n","            cfg.num_classes   # default : 1000개\n","\n","        )\n","    \n","    def forward(self, x):\n","        out = self.efficientnet(x)\n","        return out"],"metadata":{"id":"z34VIKc7YwCt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Learning Rate Scheduler\n","- https://pytorch.org/docs/stable/optim.html\n","- https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html"],"metadata":{"id":"S4lfS577fweA"}},{"cell_type":"code","source":["# Warmup Scheduler\n","class WarmupLR(optim.lr_scheduler.LambdaLR):\n","\n","    def __init__(\n","        self,\n","        optimizer: optim.Optimizer,\n","        warmup_end_steps: int,\n","        last_epoch: int = -1,\n","        ):\n","\n","        # warmup lr scheduler\n","        # warmup은 Adam Optimizer의 단점을 보완하기 위해 나옴 - Adam Optimizer와 같이 사용해보기\n","        def warmup_fn(step: int):\n","            if step < warmup_end_steps:\n","                # max(warmup_end_steps, 1) : 1 이상인 값으로 인지하도록\n","                return float(step) / float(max(warmup_end_steps, 1))\n","            return 1.0\n","\n","        # warmup_fn : lr_lambda 값으로 입력\n","        super().__init__(optimizer, warmup_fn, last_epoch)"],"metadata":{"id":"3G0KebztfzBX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 모델 선언 및 손실 함수, 최적화(optimizer) 정의, Tensorboard Logger 정의\n","\n","- 아래 셀 값들 조절해서 다른 모델 만들어 볼 수 있음"],"metadata":{"id":"F90LNgLP2ta5"}},{"cell_type":"markdown","source":["### GPU setting"],"metadata":{"id":"TjwwBXD-uAFB"}},{"cell_type":"code","source":["# gpu = None  # 코드 에러 날 때 cpu는 잘 작동하는지 확인하기 위해 gpu=None으로 설정\n","gpu = 0   # gpu를 0번 쓰겠다는 의미"],"metadata":{"id":"2LIELBSBuBJ4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### define model\n","- input shape : 28*28\n","- hidden layer : 임의로 일단 128, 64로 정함 -> 수정 가능 (0.5배, 2배, 3배 등)\n","- output : label 0~9까지 있으니 10"],"metadata":{"id":"16u_coLyhzc8"}},{"cell_type":"code","source":["# model = MLP(28*28, 128, 64, 10)\n","# model = MLPWithDropout(28*28, 128, 64, 10, dropout_prob=0.3)\n","# model = CNN(cfg=_cnn_cfg)\n","model = EfficientNetFinetune(cfg=_efficient_finetune_cfg_dict)\n","\n","# ★★ model을 GPU에 태우겠다는 의미\n","# 안 쓰면 아래와 같은 error 발생\n","# RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n","if gpu is not None:\n","    model.cuda(gpu)\n","\n","model_name = type(model).__name__\n","print(model_name)\n","\n","# define loss\n","loss_function = nn.CrossEntropyLoss()"],"metadata":{"id":"4CHBOnbp2teA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### define optimizer\n","- 공식문서 : https://pytorch.org/docs/stable/optim.html\n","- open source : https://github.com/jettify/pytorch-optimizer\n","  - 유명하거나 많이 쓰이는 것을 모아 놓은 곳이라 다른 open source 대비 신뢰성↑\n","\n","Adam vs SGD\n","- Adam이 tuning 등 손이 덜 가고 사용하기 편리\n","- SGD tuning을 굉장히 잘 하면 Adam보다 성능 좋을 수 있음\n","- 단, SGD는 Adam보다 학습 속도가 훨씬 느림\n","- learning rate 값 조절에 따른 효과 보기 위해서는 기본 optimizer인 SGD 사용하는 게 좋음\n","\n","RAdam : Rectified Adam\n","- 뭘 써야할 지 잘 모르겠으면 RAdam 쓰는 게 제일 좋음\n","\n","AdamP\n","- Adam과 거의 비슷하지만 Adam이 툭 튀는 부분을 잡아주는 optimizer = Adam보다 조금 나음"],"metadata":{"id":"1l_EcZgz-ZUA"}},{"cell_type":"code","source":["# finetune에서는 보통 learing rate(1ㄷ-3)보다 조금 더 작은 값 쓰면 모델 결과 더 좋게 나오는 경우 많음\n","lr = 1e-3\n","\n","# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n","optimizer = RAdam(model.parameters(), lr=lr)\n","# optimizer = AdamP(model.parameters(), lr=lr)\n","optimizer_name = type(optimizer).__name__\n","\n","# define scheduler\n","scheduler = None\n","# scheduler = WarmupLR(optimizer, 1500)\n","scheduler_name = type(scheduler).__name__ if scheduler is not None else 'no'\n","\n","max_epoch = 50"],"metadata":{"id":"vHpI0EKY-ZWx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### define tensorboard logger\n","- .isoformat(timespec='seconds') : 초 단위까지만 시간 나타나게 하기"],"metadata":{"id":"ul7xjq2JApbu"}},{"cell_type":"code","source":["# 현재 위치를 예전 모델들 결과물이 있는 폴더로 설정해 놓아서\n","# 모든 결과물이 같은 폴더 내에 있으면 tensorboard에서 모델 간 비교 가능\n","run_name = f\"{datetime.now().isoformat(timespec='seconds')}-{model_name}-{optimizer_name}_optim_{lr}_lr_with_{scheduler_name}_scheduler\"\n","run_dirname= 'dnn_tutorial-fashion-mnist-runs'\n","# log_dir = f'runs/{run_name}'\n","log_dir = os.path.join(drive_project_root, \"runs\", run_dirname, run_name)\n","writer = SummaryWriter(log_dir=log_dir)\n","log_interval = 100"],"metadata":{"id":"Fs_GN__t-ZdE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### define wandb\n","- wandb 사용하기\n","  - 가입\n","  - browser here 링크 클릭하면 코드 나옴\n","  - 해당 코드 입력하면 로그인 됨\n","  - project page 링크 클릭\n","- wandb는 tensorboard에서 보여주지 않는 것들 보여줌\n","  - hardware : GPU 쓰다 보면 처음 보는 에러 발생해서 죽는 경우 있는데 그때 보통 hardware 에러 - wandb를 통해 문제 파악 가능"],"metadata":{"id":"efGe8HGIBqRg"}},{"cell_type":"code","source":["project_name = 'fastcampus_fashion_mnist_tutorials'\n","run_tags = [project_name]\n","wandb.init(\n","    project=project_name,\n","    name=run_name,\n","    tags=run_tags,\n","    config={\"lr\": lr, \"model_name\": model_name, \"optimizer_name\": optimizer_name, \"scheduler_name\": scheduler_name},\n","    reinit=True,\n",")\n","\n","# 모델 그래프 보기\n","wandb.watch(model)"],"metadata":{"id":"ddBKc0JqBlCr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### set save model path\n","- 이렇게 서로 다른 파일에 모델 결과들을 저장해 놓아야 여러 모델을 돌릴 때 비교 가능"],"metadata":{"id":"73EPS1G0BnWs"}},{"cell_type":"code","source":["log_model_path = os.path.join(log_dir, 'models')\n","os.makedirs(log_model_path, exist_ok=True)"],"metadata":{"id":"OQD-lAM5BlMT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### scheduler 확인\n","\n","- linear하게 증가하는 것 볼 수 있음\n","- step 0 : 6.666666666666667e-07 -> 0.001/1500\n","  - 0.001 : learning rate 값\n","  - 1500 : scheduler = WarmupLR(optimizer, 1500)에서 사용된 값\n","- optimizer에 warmup_fn에서 return한 '1.0'이 곱해짐"],"metadata":{"id":"wgdlbrRQlXoJ"}},{"cell_type":"code","source":["# for i in range(250):\n","#     print(\"step\", i)\n","#     optimizer.step()\n","#     scheduler.step()\n","#     print(scheduler.get_last_lr())"],"metadata":{"id":"n_X_IGwelH0z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## early stopping callback object class 정의\n","- with some modification, source is from [here](https://github.com/bjarten/early-stopping-pytorch)\n","\n","- patience (int): How long to wait after last time validation loss improved (default 7)\n","- verbose (bool): If True, print a message for each validation loss improvement (default False)\n","- delta (float) : Minimum change in the monitored quantity to qualify as an improvement (default 0)\n","- path (str): Path for the checkpoint to be saved to (default 'checkpoint.ckpt') - epoch가 돌 때마다 모델 저장\n","- trace_func (function): trace print function (default print)            "],"metadata":{"id":"hWHLkFraqK0l"}},{"cell_type":"code","source":["class EarlyStopping:\n","    # Early stops the training if validation loss doesn't improve after a given atience\n","    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.ckpt', trace_func=print):\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","        self.path = path\n","        self.trace_func = trace_func\n","    \n","    def __call__(self, val_loss, model):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            # score가 최대가 되었을 때 = loss가 최소가 되었을 때 저장\n","            self.save_checkpoint(val_loss, model)\n","        # best_score보다 score 값이 적을 때 = 모델 성능이 더이상 개선되지 않을 때\n","        # 이론과 달리 실전에서는 model이 계속 개선되다가 특정 구간에서 다시 계속 증가하지 않고\n","        # 약간 울렁울렁 올라갔다 내려갔다 변동하는 게 있음\n","        # -> 그래서 patience 사용하는 것 (n번정도까지 기다려 봄)\n","        #    그래도 성능 개선이 없으면 early stop 'True'\n","        #    위에서 model save_checkpoint를 해줬기 때문에\n","        #    좀 더 학습하더라도 저장된 최종 모델을 사용하면 되니 안전함\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","    \n","    def save_checkpoint(self, val_loss, model):\n","        # Svaes model when validation loss decrease\n","        if self.verbose:\n","            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n","        \n","        filename = self.path.split('/')[-1]\n","        save_dir = os.path.dirname(self.path)\n","        torch.save(model, os.path.join(save_dir, f'val_loss-{val_loss}-{filename}'))\n","        self.val_loss_min = val_loss"],"metadata":{"id":"M4LDrVl-qLA8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%load_ext tensorboard\n","# runs라는 폴더를 만들고 그 안에 logging을 쌓을 예정\n","# %tensorboard --logdir runs/\n","\n","# 현재 위치를 예전 모델들 결과물이 있는 폴더로 설정해 놓아서\n","# 모든 결과물이 같은 폴더 내에 있으면 tensorboard에서 모델 간 비교 가능\n","# 경로 지정 : terminal이기 때문에 '#'를 '#'으로 인식하려면 앞에 '\\' 입력 필요\n","%tensorboard --logdir /content/drive/MyDrive/\\#fastcampus/runs/dnn-tutorial-fashion-mnist-runs/\n","\n","# define EarlyStopping\n","early_stopper = EarlyStopping(\n","    patience=3, verbose=True, path=os.path.join(log_model_path, 'model.ckpt')\n",")\n","\n","# Do train with validation\n","train_step = 0\n","for epoch in range(1, max_epoch+1):\n","    \n","    # valid step\n","    # ★TIP : train step보다 먼저 해 보면 랜덤 input에 대해 얼마나 막장인지 미리 결과 확인해 볼 수 있으니 좋음\n","    # 단, validation data에 대해서는 optimizer를 하면 안 됨 -> cheating이니까\n","    # pytorch에서는 'torch.no_grad()'만 해도 optimizer 안 한 것으로 바꿔줌\n","    with torch.no_grad():\n","        val_loss = 0.0\n","        val_corrects = 0\n","        model.eval()   # 지금은 model evaluation 하는 time이라는 것 알려줌 (model.train()과 세트)\n","\n","        # enumerate 안에 tqdm 해 주면 결과를 더 예쁘게 볼 수 있음\n","        # https://tqdm.github.io/docs/tqdm/\n","        # - position : Specify the line offset to print this bar (starting from 0) -> Useful to manage multiple bars at once\n","        #              ipynb은 자꾸 밑으로 내려가는데(?) 'position=0'으로 해야 가만히 있음 \n","        # - leave=True : keeps all traces of the progressbar upon termination of iteration\n","        # - desc : 진행바 앞에 text 출력\n","        for val_batch_idx, (val_images, val_labels) in enumerate(\n","            tqdm(val_dataloader, position=0, leave=True, desc=\"validation\")\n","            ):\n","\n","            # ★ cpu가 아닌 gpu 사용할 때는 이 코드 넣어줘야 함\n","            if gpu is not None:\n","                val_images = val_images.cuda(gpu, non_blocking=True)\n","                val_labels = val_labels.cuda(gpu, non_blocking=True)\n","\n","            # forward\n","            val_outputs = model(val_images)\n","            _, val_preds = torch.max(val_outputs, 1)\n","\n","            # loss & acc\n","            # val_outputs.shape[0] : batch size\n","            val_loss += loss_function(val_outputs, val_labels) / val_outputs.shape[0]\n","            val_corrects += torch.sum(val_preds == val_labels.data) / val_outputs.shape[0]\n","            \n","    # valid step logging\n","    val_epoch_loss = val_loss / len(val_dataloader)\n","    val_epoch_acc = val_corrects / len(val_dataloader)\n","\n","    print(\n","        f\"{epoch} epoch, {train_step} step: val_loss: {val_epoch_loss}, val_acc: {val_epoch_acc}\"\n","    )\n","\n","    # tensorboard log : tensorboard에 write\n","    # val_step 말고 train_step으로 해야 서로 같은 시점에서 비교하기 좋음\n","    writer.add_scalar('Loss/val', val_epoch_loss, train_step)\n","    writer.add_scalar('Acc/val', val_epoch_acc, train_step)\n","    writer.add_images('Images/val', val_images, train_step)\n","\n","    # wandb log\n","    wandb.log({\n","        'Loss/val': val_epoch_loss,\n","        'Acc/val': val_epoch_acc,\n","        # image는 그냥 쓸 수 없고 wandb.Image 써줘야 함\n","        'Images/val': wandb.Image(val_images),\n","        # histogram\n","        # prediction 하고 있는 게 한 가지 class로 overfitting 되고 있지 않은 지 확인 가능\n","        # detach() : gpu를 쓰고 있다면 detach\n","        # ★ .cpu() : gpu 쓰던 중이었으면 detach 하고 cpu로 보내주는 것까지 해야 numpy로 변환 가능\n","        'Outputs/val': wandb.Histogram(val_outputs.detach().cpu().numpy()),\n","        'Preds/val': wandb.Histogram(val_preds.detach().cpu().numpy()),\n","        'Labels/val': wandb.Histogram(val_labels.data.detach().cpu().numpy())\n","    }, step=train_step)\n","\n","    # check model early stopping print & save model if model reached the best perormance\n","    early_stopper(val_epoch_loss, model)   # call 부분\n","    if early_stopper.early_stop:           # early_stop = True면 학습 중단\n","        break\n","\n","    # ===============================================================\n","\n","    # train step\n","    current_loss = 0\n","    current_corrects = 0\n","    model.train()     # 지금은 model training 하는 time이라는 것 알려줌 (model.eval()과 세트)\n","\n","    for batch_idx, (images, labels) in enumerate(\n","        tqdm(train_dataloader, position=0, leave=True, desc='training')\n","        ):\n","\n","        # ★ cpu가 아닌 gpu 사용할 때는 이 코드 넣어줘야 함\n","        if gpu is not None:\n","            images =  images.cuda(gpu)\n","            labels = labels.cuda(gpu)\n","\n","        current_loss = 0.0\n","        current_corrects = 0  # 몇 개나 맞췄는지\n","        \n","        # Forward ===================================================\n","        # get predictions\n","        outputs = model(images)\n","        _, preds = torch.max(outputs, 1)  # 1 : dimension\n","        # print(outputs)\n","        # print(preds)\n","\n","        # get loss\n","        # Cross Entropy : loss_function(input, target)\n","        loss = loss_function(outputs, labels)\n","        \n","        # Backpropagation ===========================================\n","        # optimizer 초기화 (zero화) : garbage 값이 있을 수도 있으니 처리해주기\n","        optimizer.zero_grad()\n","\n","        # Perform backward pass\n","        # Chain rule 자동 계산해 줌\n","        loss.backward()\n","\n","        # Perform Optimization\n","        optimizer.step()\n","\n","        # Perform LR scheduler work\n","        if scheduler is not None:\n","            scheduler.step()  # 이거 안 하면 learning rate 제대로 안 나옴\n","\n","        current_loss += loss.item()\n","        current_corrects += torch.sum(preds == labels.data)\n","\n","        # 일정 이상 돌면 = 100번(log_interval)마다 정확도 평균 계산\n","        if train_step % log_interval == 0:\n","            train_loss = current_loss / log_interval\n","            train_acc = current_corrects / log_interval\n","\n","            print(\n","                f'{train_step}: train_loss: {train_loss}, train_acc: {train_acc}'\n","            )\n","\n","            # current learning rate\n","            # scheduler를 사용하지 않는 경우 대비\n","            # 아래 코드 없이 돌리면 Error 발생 : 'NoneType' object has no attribute 'get_last_lr'\n","            cur_lr = optimizer.param_groups[0]['lr'] if scheduler is None else scheduler.get_last_lr()[0]\n","\n","            # tensorboard log : tensorboard에 write\n","            writer.add_scalar('Loss/train', train_loss, train_step)\n","            writer.add_scalar('Acc/train', train_acc, train_step)\n","            writer.add_images('Images/train', images, train_step)\n","            # scheduler : list형태 return -> [0] 값 가져오기\n","            writer.add_scalar(\"Learning Rate\", cur_lr, train_step)\n","            writer.add_graph(model, images)\n","\n","            # wandb log\n","            wandb.log({\n","                'Loss/train': train_loss,\n","                'Acc/train': train_acc,\n","                'Images/train': wandb.Image(images),\n","                # ★ .cpu() : gpu 쓰던 중이었으면 detach 하고 cpu로 보내주는 것까지 해야 numpy로 변환 가능\n","                'Outputs/train': wandb.Histogram(outputs.detach().cpu().numpy()),\n","                'Preds/train': wandb.Histogram(preds.detach().cpu().numpy()),\n","                'Labels/train': wandb.Histogram(labels.data.detach().cpu().numpy()),\n","                'Learning Rate': cur_lr,\n","            }, step=train_step)\n","\n","            # loss 초기화\n","            current_loss = 0\n","            current_corrects= 0\n","\n","        train_step += 1\n","        # break\n","    # break"],"metadata":{"id":"2Ferpwec7a-F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## save model\n","\n","- ckpt : checkpoint"],"metadata":{"id":"0wLLsasjiVx7"}},{"cell_type":"code","source":["# torch.save(model, os.path.join(log_model_path, \"model.ckpt\"))"],"metadata":{"id":"VLw2mu7a8Mze"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["log_model_path"],"metadata":{"id":"jhHIjgBF4aEl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["load model\n","\n","- eval(expression) : expression을 문자열로 받아 실행"],"metadata":{"id":"CrGdVCUda1aH"}},{"cell_type":"code","source":["# loaded_model = torch.load(os.path.join(log_model_path, \"model.ckpt\"))\n","loaded_model = torch.load(os.path.join(log_model_path, 'val_loss-0.029985815286636353-model.ckpt'))\n","loaded_model.eval()\n","loaded_model.cpu()   # gpu 사용했다면 cpu로 바꿔주기 -> 아래 test 코드는 cpu용으로 되어 있으니 error 발생\n","print(loaded_model)  # 잘 load 되었는지 확인"],"metadata":{"id":"YxhfY-u8a1dn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## softmax"],"metadata":{"id":"71UCgFtffPRF"}},{"cell_type":"code","source":["def softmax(x, axis=0):\n","    'numpy softmax'\n","    max = np.max(x, axis=axis, keepdims=True)\n","    e_x = np.exp(x - max)\n","    sum = np.sum(e_x, axis=axis, keepdims=True)\n","    f_x = e_x / sum\n","    return f_x"],"metadata":{"id":"RmiXm9w7bABH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## test model"],"metadata":{"id":"liGHGIJWfSOK"}},{"cell_type":"markdown","source":["torch.tensor\n","- It has an additional \"computational graph\" layer\n","- To convert a `torch.tensor` to `np.ndarray`, you must explicitly remove the computational graph of the tensor using the ★ `detach()` command\n","\n","★ 분석을 할 때는 마지막에 다 numpy로 바꿔줘야 scikit learn 등 여러 가지 tool에 쉽고 유연하게 적용이 가능하기 때문에 아래와 같이 대부분 마지막에는 numpy로 바꿔주고 끝냄"],"metadata":{"id":"pd3tepWWdTgx"}},{"cell_type":"code","source":["test_batch_size = 100\n","test_dataset = FashionMNIST(data_root, download=True, train=False, transform=transforms.ToTensor())\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=1)\n","\n","test_labels_list = []\n","test_preds_list = []\n","test_outputs_list = []\n","\n","for i, (test_images, test_labels) in enumerate(tqdm(test_dataloader, position=0, leave=True, desc='testing')):\n","    # forward\n","    test_outputs = loaded_model(test_images)\n","    _, test_preds = torch.max(test_outputs, 1)\n","\n","    final_outs = softmax(test_outputs.detach().numpy(), axis=1)\n","    test_outputs_list.extend(final_outs)\n","    test_preds_list.extend(test_preds.detach().numpy())\n","    test_labels_list.extend(test_labels.detach().numpy())\n","\n","# accuracy를 구하기 위해 numpy array로 변경\n","test_preds_list = np.array(test_preds_list)\n","test_labels_list = np.array(test_labels_list)\n","\n","print(f'\\nacc: {np.mean(test_preds_list == test_labels_list)*100}%')"],"metadata":{"id":"jZEUnSE1cGVV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ROC curve\n","\n","- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html"],"metadata":{"id":"ZmfIQKhWfD_o"}},{"cell_type":"code","source":["from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score\n","\n","fpr = {}\n","tpr = {}\n","thresh = {}  # threshold\n","n_class = 10\n","\n","for i in range(n_class):\n","    # [:, i] : batch size는 유지하고 각 class마다 확인\n","    fpr[i], tpr[i], thresh[i] = roc_curve(test_labels_list, np.array(test_outputs_list)[:, i], pos_label=i)"],"metadata":{"id":"91RPOZE_fXZm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["전체 test case에 대한 값"],"metadata":{"id":"uFxCXLUDisQF"}},{"cell_type":"code","source":["print(fpr)"],"metadata":{"id":"8H8phF44igCg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(n_class):\n","    plt.plot(fpr[i], tpr[i], linestyle='--', label=f'Class {i} vs Rest')\n","plt.title('Multi-class ROC Curve')\n","plt.xlabel('False Position Rate')\n","plt.ylabel('True Positive Rate')\n","plt.legend(loc='best')\n","plt.show()"],"metadata":{"id":"_ZjbBLVuilIQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n","\n","- ovo : Stands for One-vs-one. Computes the average AUC of all possible pairwise combinations of classes.\n","- macro : Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account."],"metadata":{"id":"qXg4j3kRjhSO"}},{"cell_type":"code","source":["print('auc_score ', roc_auc_score(test_labels_list, test_outputs_list, multi_class='ovo', average='macro'))"],"metadata":{"id":"ikzNIkLBivPx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"WuhuVp-SjaPp"},"execution_count":null,"outputs":[]}]}
